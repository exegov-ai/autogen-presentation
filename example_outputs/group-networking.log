flaml.automl is not available. Please install flaml[automl] to enable AutoML functionalities.
The return type of the function 'fetch_news_from_rss' is not annotated. Although annotating it is optional, the function should return either a string, a subclass of 'pydantic.BaseModel'.
The return type of the function 'fetch_news_from_rss' is not annotated. Although annotating it is optional, the function should return either a string, a subclass of 'pydantic.BaseModel'.
The return type of the function 'fetch_news_from_rss' is not annotated. Although annotating it is optional, the function should return either a string, a subclass of 'pydantic.BaseModel'.
Admin (to chat_manager):

Network in group chat and discuss ideas for topic of GenAI Cracow #10 meetup to chat participants In each round pick a different agent to ask a question. Pick which agent's idea is better and explain why it would be more interesting for community.

--------------------------------------------------------------------------------

>>>>>>>> USING AUTO REPLY...
Moderator (to chat_manager):

Hello everyone! Let's dive into an exciting discussion about the upcoming GenAI Cracow #10 meetup. I encourage all of you to share your ideas and thoughts on potential topics for the meetup. 

Remember to keep the conversation focused on this theme. If anyone strays off-topic too often, I will have to ask the admin to ban them from the group. Also, let's maintain a friendly atmosphere—respectful communication is key!

To kick things off, let’s brainstorm some engaging topics or questions. If anyone needs inspiration, we can also fetch the latest news related to GenAI from RSS feeds to spark some ideas. Would you like me to do that?

--------------------------------------------------------------------------------
Provide feedback to chat_manager. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: Admin (to chat_manager):

hi lets check latest news

--------------------------------------------------------------------------------
Newsman (to chat_manager):

***** Suggested tool call (call_TRsLquZi1AWs99np6fDgPoAZ): fetch_news_from_rss *****
Arguments: 
{}
************************************************************************************

--------------------------------------------------------------------------------

>>>>>>>> EXECUTING FUNCTION fetch_news_from_rss...
Newsman (to chat_manager):

Newsman (to chat_manager):

***** Response from calling tool (call_TRsLquZi1AWs99np6fDgPoAZ) *****
[{"title": "Unlocking the Power of Agentic Workflows at Nexla with Autogen", "link": "https://autogenhub.github.io/autogen/blog/2024/10/23/NOVA", "summary": "nexlaautogen", "published": "Wed, 23 Oct 2024 00:00:00 GMT"}, {"title": "AgentOps, the Best Tool for AutoGen Agent Observability", "link": "https://autogenhub.github.io/autogen/blog/2024/07/25/AgentOps", "summary": "TL;DR", "published": "Thu, 25 Jul 2024 00:00:00 GMT"}, {"title": "Enhanced Support for Non-OpenAI Models", "link": "https://autogenhub.github.io/autogen/blog/2024/06/24/AltModels-Classes", "summary": "agents", "published": "Mon, 24 Jun 2024 00:00:00 GMT"}, {"title": "AgentEval: A Developer Tool to Assess Utility of LLM-powered Applications", "link": "https://autogenhub.github.io/autogen/blog/2024/06/21/AgentEval", "summary": "Fig.1: An AgentEval framework with verification step", "published": "Fri, 21 Jun 2024 00:00:00 GMT"}, {"title": "Agents in AutoGen", "link": "https://autogenhub.github.io/autogen/blog/2024/05/24/Agent", "summary": "agents", "published": "Fri, 24 May 2024 00:00:00 GMT"}, {"title": "AutoDefense - Defend against jailbreak attacks with AutoGen", "link": "https://autogenhub.github.io/autogen/blog/2024/03/11/AutoDefense/Defending LLMs Against Jailbreak Attacks with AutoDefense", "summary": "architecture", "published": "Mon, 11 Mar 2024 00:00:00 GMT"}, {"title": "What's New in AutoGen?", "link": "https://autogenhub.github.io/autogen/blog/2024/03/03/AutoGen-Update", "summary": "autogen is loved", "published": "Sun, 03 Mar 2024 00:00:00 GMT"}, {"title": "StateFlow - Build State-Driven Workflows with Customized Speaker Selection in GroupChat", "link": "https://autogenhub.github.io/autogen/blog/2024/02/29/StateFlow", "summary": "TL;DR: Introduce Stateflow, a task-solving paradigm that conceptualizes complex task-solving processes backed by LLMs as state machines.", "published": "Thu, 29 Feb 2024 00:00:00 GMT"}, {"title": "FSM Group Chat -- User-specified agent transitions", "link": "https://autogenhub.github.io/autogen/blog/2024/02/11/FSM-GroupChat", "summary": "FSM Group Chat", "published": "Sun, 11 Feb 2024 00:00:00 GMT"}, {"title": "Anny: Assisting AutoGen Devs Via AutoGen", "link": "https://autogenhub.github.io/autogen/blog/2024/02/02/AutoAnny", "summary": "Anny is a Discord bot powered by AutoGen to help AutoGen's Discord server.", "published": "Fri, 02 Feb 2024 00:00:00 GMT"}, {"title": "AutoGen with Custom Models: Empowering Users to Use Their Own Inference Mechanism", "link": "https://autogenhub.github.io/autogen/blog/2024/01/26/Custom-Models", "summary": "TL;DR", "published": "Fri, 26 Jan 2024 00:00:00 GMT"}, {"title": "AutoGenBench -- A Tool for Measuring and Evaluating AutoGen Agents", "link": "https://autogenhub.github.io/autogen/blog/2024/01/25/AutoGenBench", "summary": "AutoGenBench", "published": "Thu, 25 Jan 2024 00:00:00 GMT"}, {"title": "Code execution is now by default inside docker container", "link": "https://autogenhub.github.io/autogen/blog/2024/01/23/Code-execution-in-docker", "summary": "TL;DR", "published": "Tue, 23 Jan 2024 00:00:00 GMT"}, {"title": "All About Agent Descriptions", "link": "https://autogenhub.github.io/autogen/blog/2023/12/29/AgentDescriptions", "summary": "TL;DR", "published": "Fri, 29 Dec 2023 00:00:00 GMT"}, {"title": "AgentOptimizer - An Agentic Way to Train Your LLM Agent", "link": "https://autogenhub.github.io/autogen/blog/2023/12/23/AgentOptimizer", "summary": "Overall structure of AgentOptimizer", "published": "Sat, 23 Dec 2023 00:00:00 GMT"}, {"title": "AutoGen Studio: Interactively Explore Multi-Agent Workflows", "link": "https://autogenhub.github.io/autogen/blog/2023/12/01/AutoGenStudio", "summary": "AutoGen Studio Playground View: Solving a task with multiple agents that generate a pdf document with images.", "published": "Fri, 01 Dec 2023 00:00:00 GMT"}, {"title": "Agent AutoBuild - Automatically Building Multi-agent Systems", "link": "https://autogenhub.github.io/autogen/blog/2023/11/26/Agent-AutoBuild", "summary": "Overall structure of AutoBuild", "published": "Sun, 26 Nov 2023 00:00:00 GMT"}, {"title": "How to Assess Utility of LLM-powered Applications?", "link": "https://autogenhub.github.io/autogen/blog/2023/11/20/AgentEval", "summary": "Fig.1: A verification framework", "published": "Mon, 20 Nov 2023 00:00:00 GMT"}, {"title": "AutoGen Meets GPTs", "link": "https://autogenhub.github.io/autogen/blog/2023/11/13/OAI-assistants", "summary": "OpenAI Assistant", "published": "Mon, 13 Nov 2023 00:00:00 GMT"}, {"title": "EcoAssistant - Using LLM Assistants More Accurately and Affordably", "link": "https://autogenhub.github.io/autogen/blog/2023/11/09/EcoAssistant", "summary": "system", "published": "Thu, 09 Nov 2023 00:00:00 GMT"}, {"title": "\"It's a 10% chance which I did 10 times, so it should be 100%\"", "link": "https://www.lesswrong.com/posts/pNkjHuQGDetRZypmA/it-s-a-10-chance-which-i-did-10-times-so-it-should-be-100", "summary": "Published on November 18, 2024 1:14 AM GMT<br /><br /><p>Many of you readers may instinctively know that this is wrong. If you flip a coin (50% chance) twice, you are not <i>guaranteed</i> to get heads. The probability of getting a heads is 75%. However you may be surprised to learn that there is some truth to this statement; modifying the statement just slightly will yield not just a true statement, but a useful and interesting one.</p><p>It's a spoiler, though. If you want to figure this out as you read this article yourself, you should skip this and then come back. Ok, ready? Here it is:</p><div class=\"spoilers\"><p>It's a&nbsp;<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">1</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\">/</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\">n</span></span></span></span></span></span></span>&nbsp;chance and I did it&nbsp;<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\">n</span></span></span></span></span></span></span>&nbsp;times, so the probability should be...&nbsp;<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">63</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\">%</span></span></span></span></span></span></span>.&nbsp;<br />Almost always.</p></div><h2>&nbsp;</h2><h2>The math:</h2><p>Suppose you're flipping a coin and you want to find the probability of NOT flipping a single heads in a dozen flips. The math for this is fairly simple: The probability of not flipping a single heads is the same as the probability of flipping 12 tails. which is&nbsp;</p><span class=\"math-tex\"><span class=\"mjpage mjpage__block\"><span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">1</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">2</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.584em; padding-left: 0px;\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">12</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\">\u2248</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\">0.000977</span></span></span></span></span></span></span><p>The same can be done with this problem: you have something with a 1/10 chance and you want to do it 10 times. The probability of not getting it to happen even once is the same as the probability of it not happening 10 times in a row. So&nbsp;</p><span class=\"math-tex\"><span class=\"mjpage mjpage__block\"><span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">9</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">10</span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.584em; padding-left: 0px;\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">10</span></span></span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\">\u2248</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\">0.35</span></span></span></span></span></span></span><p>If you learned some fairly basic probability, I doubt this is that interesting to you. The interesting part comes when you look at the general formula: The probability of not getting what you want (I'll call this&nbsp;<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\">\u2212</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\">p</span></span></span></span></span></span></span>, because&nbsp;<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\">p</span></span></span></span></span></span></span>&nbsp;would be the probability of the outcome you want) is</p><span class=\"math-tex\"><span class=\"mjpage mjpage__block\"><span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\">\u2212</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\">p</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\">=</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\">\u2212</span></span><span class=\"mjx-mfrac MJXc-space2\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 0.8em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">1</span></span></span><span class=\"mjx-denominator\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\">n</span></span></span><span class=\"mjx-line\"></span></span><span class=\"mjx-vsize\"></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.584em; padding-left: 0px;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\">n</span></span></span></span></span></span></span></span></span><p>Where&nbsp;<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\">n</span></span></span></span></span></span></span>&nbsp;in our case is 10, but in general is whatever number you hear when you hear the (incorrect) phrase \"It's a one-in-<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\">n</span></span></span></span></span></span></span>&nbsp;chance, and I did it&nbsp;<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\">n</span></span></span></span></span></span></span>&nbsp;times, so it should be&nbsp;<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">100</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\">%</span></span></span></span></span></span></span>\"</p><h2>&nbsp;</h2><h2>Hold on a sec, that formula looks familiar...</h2><p>\"<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\">\u2212</span></span><span class=\"mjx-mfrac MJXc-space2\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 0.566em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">1</span></span></span><span class=\"mjx-denominator\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\">n</span></span></span><span class=\"mjx-line\"></span></span><span class=\"mjx-vsize\"></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\">n</span></span></span></span></span></span></span></span></span>&nbsp;...\" I thought to myself... \"That looks familiar...\" This is by no means obvious, but to people who have dealt with the number&nbsp;<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\">e</span></span></span></span></span></span></span>&nbsp;recently, this looks quite similar to the limit that actually defines that number. This sort of pattern recognition led me to google what this limit is, and it turns out my intuition was close:</p><span class=\"math-tex\"><span class=\"mjpage mjpage__block\"><span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-munderover\"><span class=\"mjx-itable\"><span class=\"mjx-row\"><span class=\"mjx-cell\"><span class=\"mjx-op\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\">lim</span></span></span></span></span><span class=\"mjx-row\"><span class=\"mjx-under\" style=\"font-size: 70.7%;\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\">n</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\">\u2192</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\">\u221e</span></span></span></span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\">\u2212</span></span><span class=\"mjx-mfrac MJXc-space2\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 0.8em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">1</span></span></span><span class=\"mjx-denominator\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\">n</span></span></span><span class=\"mjx-line\"></span></span><span class=\"mjx-vsize\"></span></span><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\">)</span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.584em; padding-left: 0px;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\">n</span></span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\">=</span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 0.7em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">1</span></span></span><span class=\"mjx-denominator\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\">e</span></span></span><span class=\"mjx-line\"></span></span><span class=\"mjx-vsize\"></span></span></span></span></span></span></span><span class=\"math-tex\"><span class=\"mjpage mjpage__block\"><span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 0.7em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">1</span></span></span><span class=\"mjx-denominator\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\">e</span></span></span><span class=\"mjx-line\"></span></span><span class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\">\u2248</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\">0.37</span></span></span></span></span></span></span><p>So it turns out: for any n that's large enough, if you do something with a&nbsp;<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">1</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\">/</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\">n</span></span></span></span></span></span></span>&nbsp;chance of success&nbsp;<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\">n</span></span></span></span></span></span></span>&nbsp;times, your probability of failure is always going to be roughly&nbsp;<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">37</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\">%</span></span></span></span></span></span></span>, which means your probability of success will always be roughly&nbsp;<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">63</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\">%</span></span></span></span></span></span></span>.</p><h3><strong>If something is a&nbsp;</strong><span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">1</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\">/</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\">n</span></span></span></span></span></span></span><strong>&nbsp;chance, and I do it&nbsp;</strong><span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\">n</span></span></span></span></span></span></span><strong>&nbsp;times, the probability should be...&nbsp;</strong><span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">63</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\">%</span></span></span></span></span></span></span><strong>.</strong></h3><p>Isn't that cool? I think that's cool.</p><p>&nbsp;</p><h2>What I'm NOT saying:</h2><p>There are a couple ways to easily misinterpret this, so here are some caveats:</p><ul><li>The average number of successes, if you try a&nbsp;<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">1</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\">/</span></span></span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">10</span></span></span></span></span></span></span>&nbsp;chance&nbsp;<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">10</span></span></span></span></span></span></span>&nbsp;times is&nbsp;<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">1</span></span></span></span></span></span></span>. This is always true for any value of&nbsp;<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\">n</span></span></span></span></span></span></span>. I believe this is the cause of why some folks think the probability is&nbsp;<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">100</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\">%</span></span></span></span></span></span></span>&nbsp;if they try enough times. Your probability of succeeding at least once may be&nbsp;<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">63</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\">%</span></span></span></span></span></span></span>, but you also have a chance of succeeding twice, thrice, etc. This means 1 success on average, not a 100% chance of success once.</li><li>This is post explores how probability behaves as you try a less-likely chance, more often. Obviously for any given&nbsp;<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">1</span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\">/</span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\">n</span></span></span></span></span></span></span>&nbsp;chance, trying it more often will increase your probability of succeeding at least once.</li><li>This relies on a limit as&nbsp;<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\">n</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\">\u2192</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\">\u221e</span></span></span></span></span></span></span>. How useful is this in the real world where&nbsp;<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\">n</span></span></span></span></span></span></span>&nbsp;is probably a rather small number like 5, 10, or 20? Well you can try out the formula to find out. Personally I think the&nbsp;<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">63</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\">%</span></span></span></span></span></span></span>&nbsp;figure is good enough for&nbsp;<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\">n</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\">\u2265</span></span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\">4</span></span></span></span></span></span></span>&nbsp;(within 5%). You could use&nbsp;<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">65</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\">%</span></span></span></span></span></span></span>&nbsp;as a nice round number instead of&nbsp;<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\">63</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\">%</span></span></span></span></span></span></span>&nbsp;if you want to round up, too.</li></ul><div class=\"spoilers\"><p>Spoiler for 5, 10, and 20: it's 67%, 65%, and 64% respectively</p></div><br /><br /><a href=\"https://www.lesswrong.com/posts/pNkjHuQGDetRZypmA/it-s-a-10-chance-which-i-did-10-times-so-it-should-be-100#comments\">Discuss</a>", "published": "Wed, 20 Nov 2024 18:56:24 GMT"}, {"title": "Neutrality", "link": "https://www.lesswrong.com/posts/WxnuLJEtRzqvpbQ7g/neutrality", "summary": "Published on November 13, 2024 11:10 PM GMT<br /><br /><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb81e3677-6e98-4793-977e-2c29c854093a_1024x1024.webp\"><img alt=\"\" src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/WxnuLJEtRzqvpbQ7g/s4a3pxgeapj3sx1ygtcv\" /></a></p><p>Midjourney, \u201cinfinite library\u201d</p><p>I\u2019ve had post-election thoughts percolating, and the sense that I wanted to synthesize something about this moment, but politics per se is not really my beat. This is about as close as I want to come to the topic, and it\u2019s a sidelong thing, but I think the time is right.</p><p>It\u2019s time to start thinking again about <i>neutrality</i>.</p><p>Neutral institutions, neutral information sources. Things that both <i>seem </i>and <i>are </i>impartial, balanced, incorruptible, universal, legitimate, trustworthy, canonical, foundational.<span class=\"footnote-reference\" id=\"fnrefb5wh699v7y4\"><sup><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fnb5wh699v7y4\">[1]</a></sup></span></p><p>We don\u2019t have them. Clearly.</p><p>We live in a pluralistic and divided world. Everybody\u2019s got different \u201c<a href=\"https://en.wikipedia.org/wiki/Reality_tunnel\">reality-tunnels</a>.\u201d Attempts to impose one worldview on everyone fail.</p><p>To some extent this is healthy and inevitable; we are all different, we <i>do </i>disagree, and it\u2019s vain to hope that \u201ceveryone can get on the same page\u201d like some kind of hive-mind.</p><p>On the other hand, lots of things aren\u2019t great: crumbling institutions, culture war eating everything, opportunistic SEO slop taking over the internet, noise overwhelming signal.</p><p>There is a pervasive sense of an old system dying, and a sense of potential that a new one might be born. Venkatesh Rao has somewhat inspired me here with his <a href=\"https://contraptions.venkateshrao.com/p/between-mandala-and-machine\">Mandala and Machine</a> essay, though I\u2019m taking a slightly different lens on it.</p><p>These days, savvy people tend to accept<i> </i>that the \u201cworld\u201d is a roiling chaotic ball of stormclouds, impossible to get a handle on; that all information is intrinsically untrustworthy; that all environments outside a trusted inner circle are adversarial.</p><p>This is <i>not </i>a universal human condition.</p><p>And it may not be our permanent condition. The future may hold something more like a \u201cfoundation\u201d or a \u201cframework\u201d or a \u201csystem of the world\u201d that people actually trust and consider legitimate.</p><p>I don\u2019t mean that in a utopian, impossible way; I mean it in the limited, realistic, actually-existing way that trust and legitimacy have been present in (recent and distant) past cultures.</p><h3>Not \u201cNormality\u201d</h3><p>To a large extent, we still rely on a deep, though crumbling, reserve of unexamined \u201cnormality.\u201d Apolitical, basic, running on 90\u2019s-sitcom scripts; you work at your job, then you come home to your friends or family and relax. \u201cYou know, <i>normal</i>.\u201d The thing that seems to have broken in the last decade of <a href=\"https://www.ribbonfarm.com/series/weirding-diary/\">Weirding.</a></p><p>A lot of people, of course, are nostalgic for normality. Noah Smith is <a href=\"https://www.noahpinion.blog/p/if-trump-loses-the-election-our-nation\">eager to put the 2010s behind us</a>, for instance.</p><p>I can see why, but really, unexamined normality is inadequate. It\u2019s what it feels like from the inside to be Keynes\u2019 \u201cslaves of some defunct economist.\u201d</p><p>\u201cWhy have people picked up such-and-such extreme, radical view all of a sudden?\u201d Well, it\u2019s the Internet and social media, of course; but more specifically, it\u2019s because <i>that view would always have been attractive to some fraction of people, but in decades past they\u2019d never have heard of it</i>.</p><p>A lot of people, for instance, only figured out as adults that Communism had been an influential force in U.S. culture through the 20th century. This was basic background knowledge for me, growing up in an academic family, but for other people it seems to be an electric hidden truth.</p><p>Of course some percent of people who read Osama bin Laden\u2019s manifesto for the first time are going to find it compelling! Most people had literally never heard a terrorist\u2019s point of view before. Almost <i>anything </i>he could have written would be more persuasive than \u201che has no point of view, he\u2019s just evil.\u201d</p><p>Unexamined normality\u2019s main protection is silence; it persists only because <i>most people have never heard of any alternative to the way things are</i>. Denser communication networks mean that more people will encounter questions like \u201cbut <i>why</i> are things this way?\u201d And, because normality runs on silence, it doesn\u2019t have good explicit answers.</p><p>To some extent, weirdness is just the inevitable result of the operation of Mind. Everything unexamined, sooner or later, gets questioned, disputed, messed with. To hope to avoid this is to seek to impose stupidity on the world.</p><p>But of course, to the extent that normality was <i>functional</i>, people questioning \u201cthe way we\u2019ve always done it\u201d makes things <i>dysfunctional.</i> Things once trusted implicitly can break down, gradually and then suddenly.</p><h3>What is Neutrality Anyway?</h3><p>A process is <i>neutral </i>towards things when it treats them the same.</p><p>To be <i>neutral</i> on a controversial topic is to take no sides on it. There are many ways this can be done:</p><ul><li>consider the topic out of bounds for a space: \u201cno politics or religion at the dinner table\u201d</li><li>allow anyone to express their opinion, but take no institutional stand; <i>people </i>may take sides, but the platform/newspaper/university/government that defines the space around them does not.</li><li><i>describe </i>the controversy without taking a side on it</li><li>potentially take a stand on the controversy, but only when a conclusion emerges from an impartial process that <i>a priori</i> could have come out either way, such as:<ul><li>randomization</li><li>judicial proceedings</li><li>scientific experiment</li><li>majority vote, prediction market, or other opinion-aggregation mechanisms</li></ul></li></ul><p>The practical reason to seek neutrality is to allow cooperation between people who don\u2019t agree.</p><p>For instance, the Dutch Republic, in the late 16th century, was one of the first polities in Europe to enshrine religious freedom into law rather than establishing a state church. Why? Because the Netherlands were a patchwork of counties, divided between Catholics, Calvinists, Arminians, and assorted \u201cspiritualist\u201d sects, trying to unite and win independence from Hapsburg Spain. It was not practicable for one church to win outright and force the others into submission; their only hope was compromise.</p><p>Invariably, real-world neutrality is neutral between <i>some </i>things, not <i>all </i>things. The relevant <i>range</i> of things to be neutral about is determined by:</p><ul><li>whom do you want to cooperate with? for what purpose?</li><li>what range of opinions and worldviews could they and you hold?</li><li>where do differences in views threaten to break down cooperation?</li><li>what are the minimal conditions for cooperation to hold?</li></ul><p>We don\u2019t care about being \u201cneutral\u201d between literally all strings of characters, or between Earthlings and hypothetical extraterrestrials. We care about neutrality between actually-existing, or plausibly-potential, crucial differences in point of view between potential allies.</p><p>Neutrality-producing procedures can expand options for cooperation. Perhaps Alice and Bob can\u2019t cooperate if there\u2019s a risk their disagreement will blow up unexpectedly, but they <i>can</i> cooperate if they can agree to put it aside or submit to a neutral adjudication procedure. The procedure is a \u201cbox\u201d they can safely put their conflict in.</p><p>Seen from that point of view, the most important property of the \u201cbox\u201d is that Alice and Bob have to be <i>willing </i>to put their conflict in it. If either suspects that the \u201cbox\u201d is rigged to give the other one the upper hand, it won\u2019t work. The <i>perception </i>of neutrality is the critical element. But in the long run, you earn trust by being worthy of it.</p><h3>\u201cNeutrality is Impossible\u201d is Technically True But Misses The Point</h3><p>I once saw a computer science professor respond to the allegation that \u201cneutrality is impossible\u201d by asking \u201cok, how is my algorithms and data structures class non-neutral?\u201d</p><p>So, ok, imagine a standard undergraduate computer science class on algorithms and data structures. It covers the usual material; none of the ideas are controversial within computer science; political discussion never comes up in the classroom because it\u2019s irrelevant to the course.</p><p>But yeah, actually, it\u2019s not perfectly neutral.</p><p>There\u2019s a technical sense that nobody really cares about, where it\u2019s \u201cnon-neutral\u201d to make utterances in a language instead of random strings of sounds, or to use concepts that seem \u201cnatural\u201d to humans instead of weird philosophers\u2019 constructs like <a href=\"https://en.wikipedia.org/wiki/New_riddle_of_induction\">bleen and grue</a>.</p><p>More relevantly, it\u2019s possible for <i>actual people</i> to have objections to things about our \u201cneutral\u201d CS class.</p><p>Some people object to the structure of universities and courses themselves; grading, lectures, problem sets, and tests; conflating purposes like credentialing, coming-of-age experiences, vocational training, and liberal-arts education into single institutions called \u201ccolleges\u201d; etc.</p><p>Some people object to the choices of what goes into a \u201cstandard\u201d CS curriculum. Are these the right things to prioritize? Are these concepts valid? Sure, by stipulation nothing being taught is controversial <i>within computer science</i>, but if you ask a mathematician you might get some gripes. Ask a philosopher? A religious mystic? They might say the whole business is rotten and deluded.</p><p>Some people object to \u201capolitical\u201d spaces in themselves. To declare some topics to be \u201cpolitics\u201d and thus temporarily off limits <i>is</i> to implicitly believe something about them: namely, that they <i>can </i>be put aside for the space of a class, that it is not <i>so </i>urgently necessary to disrupt the status quo that things can\u2019t be left locally \u201capolitical\u201d while something else gets done.</p><p>Even our hypothetical bog-standard CS class takes a stand on some things \u2014 it \u201cbelieves\u201d that certain things are worth teaching, in a certain way. And it\u2019s not just conceivably but <i>realistically </i>possible for there to be disagreement about that!</p><p>When we talk about neutrality we have to talk about <i>neutral relative to what</i> and <i>neutral in what sense</i>.</p><p>The class <i>is </i>neutral on a wide range of controversial topics, in the sense that it doesn\u2019t engage with them at all. This means that a pretty wide range of <i>people </i>can take the class and learn algorithms and data structures. The range isn\u2019t literally infinite \u2014 non-English-speakers, people not enrolled in the college, maybe deaf students if there\u2019s no accommodation, etc, will <i>not </i>be able to take the class.</p><p>The question isn\u2019t \u201cis it neutral or not\u201d, the question is, \u201cis the type and degree of neutrality <i>doing the job </i>we want it to do?\u201d And then it\u2019s possible to get into the real disagreements, about what different people are trying to accomplish in the first place with the \u201cneutrality\u201d.</p><h3>Systems of the World</h3><p>A system-of-the-world is the dual of neutrality; it\u2019s the stuff that a space is <i>not </i>neutral about. The stuff that\u2019s considered foundational, basic, \u201cjust true\u201d, common-knowledge. The <i>framework</i> on which you hang a variety of possible options.</p><p>The framework, the system, the \u201ccontainer\u201d of the space, is deliberately neutral between various options, but it cannot be neutral with regards to <i>itself</i>. There has to be a \u201cbottom\u201d somewhere.</p><p>Intellectual worldviews \u201cbottom out\u201d in philosophical axioms or concepts. They seem obvious, they may rarely be discussed, but you would have to stop doing your day-to-day work if you truly rejected the foundations on which it rests.<span class=\"footnote-reference\" id=\"fnref51uek29ujyo\"><sup><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fn51uek29ujyo\">[2]</a></sup></span></p><p>The literal \u201cworld\u201d \u201cview\u201d of viewing Planet Earth as a globe, and all points on the earth as locations on a sphere or its map projections, is itself a framework or system. It is neutral between points on Earth \u2014 the globe is global, it picks no sides between countries or continents.<span class=\"footnote-reference\" id=\"fnrefebutx57tqb\"><sup><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fnebutx57tqb\">[3]</a></sup></span>&nbsp;But it\u2019s still a particular choice. It shows the world as if from a spaceship<span class=\"footnote-reference\" id=\"fnreft4uszwkaiz\"><sup><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fnt4uszwkaiz\">[4]</a></sup></span>&nbsp;outside, rather than from the point of view of a creature walking around on it.</p><p>I\u2019ve been rereading Neal Stephenson\u2019s <i>Baroque Cycle </i>lately, which is about the origins of the broader \u201csystem of the world\u201d which includes the Newtonian/Leibnizian views of mathematics and physics, the emergence of political liberalism, the Bank of England and the first financial institutions, the earliest days of the Industrial Revolution, etc. Modernity, in other words.</p><p>Modernity is a system that <i>aims </i>at a sort of universality and neutrality. But it isn\u2019t the only system that has had such aims. And Modernity absolutely does take certain stands, with which other worldviews disagree.</p><p>A \u201csystem of the world\u201d is really not the same thing as unexamined normality.</p><p>Yes, when living within a system-of-the-world, people trust that system. They rely on it, believe in it, not ironically or provisionally but for real. They treat it as common knowledge that can be assumed in communication. They see the system as <i>true and real</i>, or at least validly reflective of reality, not as a mere hypothesis or lens.</p><p>And, yes, people\u2019s faith in systems-of-the-world can shatter when they first learn about something that doesn\u2019t fit.</p><p>But strong systems-of-the-world <i>are </i>articulable. They can defend themselves. They can reflect on themselves. They can (and should) shatter in response to incompatible evidence, but they don\u2019t sputter and shrug when a child first asks \u201cwhy\u201d.</p><p>Not every point of view is even <i>trying </i>to be a system-of-the-world. It\u2019s possible to be fundamentally particularist, speaking only \u201cfor\u201d yourself, or your community, or a particular priority. Not even aspiring to any kind of universalism or neutrality.</p><p>Remember, neutrality is a <i>thing we do sometimes, when we need to</i>. It might be better to speak of \u201cimpartializing tactics\u201d, rather than a static Nirvana-like state of \u201cneutrality\u201d. We use \u201cimpartializing tactics\u201d in order to broaden our reach: cooperation between people, and also generalization across contexts and coordination across time. In using such a tactic, an entity <i>decides not to care</i> about the object-level stuff it\u2019s being \u201cimpartial\u201d about. It \u201cwithdraws\u201d to be above the fray in one domain, to gain greater scope and range elsewhere.<span class=\"footnote-reference\" id=\"fnref9wpdzoibind\"><sup><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fn9wpdzoibind\">[5]</a></sup></span></p><p>Some entities (people, institutions, cultural viewpoints, etc) <i>don\u2019t </i>choose to withdraw or \u201crise above\u201d or use these \u201cimpartializing tactics\u201d very much at all; they aren\u2019t even aiming to be systems <i>of the world</i>.</p><p>We can tell these apart, the same way we can tell apart universalizing religions (like Christianity and Islam, which hope that one day everyone will accept their truth) from non-universalizing religions (like Hinduism; from what I\u2019m told it\u2019s impossible to \u201cconvert to Hinduism.\u201d)</p><p>\u201cRed tribe\u201d and \u201cblue tribe\u201d memeplexes are non-universalizing. They don\u2019t actually <i>want</i> to be fully general and create a system that \u201ceveryone\u201d can live with; they want to win, sure, but not to \u201cgo meta\u201d or \u201ctranscend-and-include.\u201d<span class=\"footnote-reference\" id=\"fnrefs8dnbkmfpa\"><sup><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fns8dnbkmfpa\">[6]</a></sup></span></p><h3>Let\u2019s Talk About Online</h3><p>Right now, in the parts of the world I can see, I think our system-of-the-world is extremely weak. There are very few baseline \u201ccommon-sense\u201d assumptions that can be treated as common knowledge even among educated people meeting on friendly terms. What we learned in school? What we saw on TV? Common life experiences? There\u2019s just a lot less of that than there once was.</p><p>The literal UI affordances of software apps are, I suspect, an embarrassingly large amount of our current \u201csystem-of-the-world\u201d, and while I generally want to stick up for software and the internet and \u201ctech\u201d against its enemies, the tech industry was certainly never prepared for the awesome responsibility of<i> becoming The World</i>!</p><p>I think it\u2019s ultimately going to be necessary to build a new system to replace the old.</p><p>In a narrow sense that looks like an <i>information</i> system. What replaces Google, Wikipedia, blogs, and social media, in the way that those (partially) replaced traditional media, libraries, schools, etc?</p><p>In a deeper sense, our present day\u2019s conflicting viewpoints \u201clive in\u201d a World defined by the tools and platforms that host them.</p><p>There are also social, political, economic, and natural processes in the physical world; we don\u2019t live in cyberspace, we need to eat, and physical-world factors and forces constrain us; but there\u2019s no shared <i>conceptual</i> framework, no \u201cWorld\u201d, that encompasses all of them. There\u2019s just Reality, which can bite you. The closest thing we\u2019ve got to a universal conceptual World <i>is</i> your computer screen, I\u2019m sorry to say.</p><p>And it just isn\u2019t prepared to do the job of being, y\u2019know, <i>good</i>.</p><p>In the sense that a library staffed by idealistic librarians is trying to <i>be good</i>, trying to not take sides and serve all people without preaching or bossing or censoring, but is also sort of trying to be a role model for children and a good starting point for anyone\u2019s education and a window into the \u201chigher\u201d things humanity is capable of.</p><p>The \u201cideal library\u201d is not a <i>parochial </i>or <i>dogmatic </i>ideal. It\u2019s quite neutral and understated. As a library patron, you are in the driver\u2019s seat, <i>using</i> the library according to its affordances. The library doesn\u2019t really have a loud \u201cvoice\u201d of its own, it isn\u2019t \u201ctelling\u201d you anything \u2014 it\u2019s primarily the <i>books </i>that tell you things. But the library is quietly setting a frame, setting a default, making choices.</p><p>So is every institutional container.</p><p>And most institutions and information-technology frameworks are not really ambitious enough to be setting the \u201cframe\u201d or \u201ccontext\u201d for the whole World.</p><p>They make openly parochial/biased decisions to choose sides on issues that are <i>obviously </i>still active controversies.</p><p>Or, they use \u201cimpartializing tactics\u201d chosen in the early-Web-2.0 days when people were more naive and utopian, like \u201callow everyone to trivially make a user account, give all user accounts initially identical affordances, prioritize user-upvoted content.\u201d</p><p>These techniques are <i>at least attempts</i> at neutral universality, and they have real strengths \u2014 we don\u2019t want to throw the baby out with the bathwater \u2014 but they weren\u2019t designed to be ultra-robust to exploitation, or to make serious attempts to assess properties like truth, accuracy, coherence, usefulness, justice.</p><p>\u201cBut an algorithm can\u2019t possibly assess those properties! That\u2019s naive High Modernist wishful thinking!\u201d</p><p>\u201cWell, have you <i>tried lately</i>? Are you <i>sure </i>nothing we\u2019re capable of now \u2014 with LLMs, zero-knowledge proofs, formal verification, prediction markets, and the like \u2014 can make a better stab at these supposedly-subjective virtues than \u201cone guy\u2019s opinion\u201d or \u201ca committee\u2019s report\u201d or \u201cupvotes\u201d?\u201d</p><p>The alternative viewpoint is \u201cthere is no substitute for good human judgment; we don\u2019t need <i>systems, </i>we need <i>the right people to talk amongst themselves and make good decisions</i>.\u201d</p><p>To that, I say, first of all, that the \u201cright\u201d people to handle issues around information/truth, trust/legitimacy, and dispute resolution are more likely to congregate around attempts to build neutral, technical, robustly unexploitable systems. The ideal archetype of \u201cgood human judgment\u201d is a <i>judge\u2026</i>and judges are learned in <i>law</i>, which is a system. (Or protocol, if you prefer.)</p><p>Second of all, it is <i>not enough </i>for the \u201cright people to talk amongst themselves\u201d. There are some very brilliant people who only reveal their insights in private group chats and face-to-face relationships. They don\u2019t all find each other. And the scale of their ambitions is limited by the scale of recruitment and management that this \u201cpersonal judgment\u201d method can manage. And I shouldn\u2019t have to explain why \u201cjust put the right people in charge of everything\u201d is unstable.<span class=\"footnote-reference\" id=\"fnrefuuj65wmr8d\"><sup><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fnuuj65wmr8d\">[7]</a></sup></span></p><p>But sure, I generally do concede that a lot of human judgment is irreducible. It\u2019s just not <i>everything</i>, and human judgment can be greatly <i>extended </i>by protocols. There may be no technological substitute<i> </i>for good people, but there are certainly <i>complements</i>.</p><p>I may write some more specific ideas for how this could shape up in a later post, but really I\u2019m in the earliest stages of speculation and other people know much more about the object level. What I want to encourage, and seek out, is a healthy vainglory. You actually can contemplate creating a new (cognitive) foundation for the world, something that vast numbers of very different people will actually <i>want </i>to count on. It\u2019s been done before. It doesn\u2019t require anything literally impossible. It\u2019s not wilder than stuff respectable people discuss freely like \u201cbuilding AGI\u201d or \u201ccreating a new and better culture\u201d.</p><p>In fact, what makes it seem crazier is the <i>concreteness; </i>it\u2019s a happily-ever-after endgame (even though it doesn\u2019t need to be anywhere near literally eternal or perfect) that\u2019s supposed to be made of mundane moving parts like procedures, UI elements, and algorithms \u2014 ideas that never seem that great when specifics are proposed. But I think the possibility of \u201cdumb proposed ideas that won\u2019t work\u201d is a strength, not a weakness; the alternative is to speak concretely only about relatively narrow things, and vaguely about Utopia, and leave the vulnerable space in between safely unspoken (and thus never traversed.)</p><p>So yeah. Broken, crooked ladders towards Utopia are a good thing, actually. Please, make more of them!</p><ol class=\"footnote-section footnotes\"><li class=\"footnote-item\" id=\"fnb5wh699v7y4\"><span class=\"footnote-back-link\"><sup><strong><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fnrefb5wh699v7y4\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I wrote a sequence on similar topics in 2022 that I still pretty much endorse: <a href=\"https://sarahconstantin.substack.com/p/the-need-for-canonicity\">1</a>, <a href=\"https://sarahconstantin.substack.com/p/non-state-courts\">2</a>, <a href=\"https://sarahconstantin.substack.com/p/science-court\">3</a>, <a href=\"https://sarahconstantin.substack.com/p/attestation\">4</a>.</p></div></li><li class=\"footnote-item\" id=\"fn51uek29ujyo\"><span class=\"footnote-back-link\"><sup><strong><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fnref51uek29ujyo\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Yes, yes, David Chapman thinks this is false, that there are no valid axioms or foundations anywhere and we can still do our \u201cjobs\u201d while recognizing that all frameworks are provisional and groundless. I have, in fact, thought about this, and I\u2019ll get to that in a minute.</p></div></li><li class=\"footnote-item\" id=\"fnebutx57tqb\"><span class=\"footnote-back-link\"><sup><strong><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fnrefebutx57tqb\">^</a></strong></sup></span><div class=\"footnote-content\"><p>well, except via the convention of putting the North Pole on top; \u201c<a href=\"https://www.youtube.com/watch?v=eLqC3FNNOaI\">Why are we changing maps</a>?\u201d</p></div></li><li class=\"footnote-item\" id=\"fnt4uszwkaiz\"><span class=\"footnote-back-link\"><sup><strong><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fnreft4uszwkaiz\">^</a></strong></sup></span><div class=\"footnote-content\"><p>or angel; in <i>Paradise Lost </i>there\u2019s a surprising amount of astronomy, and a lot of attention lavished on what Satan sees as he approaches Earth from the outside.</p></div></li><li class=\"footnote-item\" id=\"fn9wpdzoibind\"><span class=\"footnote-back-link\"><sup><strong><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fnref9wpdzoibind\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Venkat Rao thinks of \u201cmandalas\u201d as caring about people and \u201cmachines\u201d as driven by knowing, not caring. I think this is a screwy dualism; knowing and caring are inextricably linked. The \u201cthinking vs. feeling\u201d dichotomy has always been fake. I expect this is because he is personally a \u201cthinker\u201d whose feelings aren\u2019t super strong, so it seems natural to <i>him </i>to divide the world into \u201cthinkers\u201d and \u201cfeelers\u201d, whereas I can never decide which kind I am. (Mercury/Moon conjunct, lol.)</p><p>But there is a real thing I see about \u201cdistance\u201d and \u201cwithdrawal\u201d when you go up a meta level. The more meta you go, the less involved with the object level you can be; you can \u201ctouch\u201d many more things, but more lightly and vaguely. \u201cCaring\u201d is <i>conserved</i>.</p></div></li><li class=\"footnote-item\" id=\"fns8dnbkmfpa\"><span class=\"footnote-back-link\"><sup><strong><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fnrefs8dnbkmfpa\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Ken Wilber\u2019s phrase.</p></div></li><li class=\"footnote-item\" id=\"fnuuj65wmr8d\"><span class=\"footnote-back-link\"><sup><strong><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fnrefuuj65wmr8d\">^</a></strong></sup></span><div class=\"footnote-content\"><p>I shouldn\u2019t, but I do. #1: Not everybody agrees on who \u201cthe right people\u201d are; if <i>you </i>tell me you\u2019ll just put \u201cthe right people\u201d in charge, I might not trust you. #2: The \u201cright people\u201d may not continue to make the \u201cright\u201d decisions when given more power or resources. #3: what about succession? can the \u201cright people\u201d choose other \u201cright people\u201d reliably? if they could, <i>wouldn\u2019t they be doing it already? </i>The world doesn\u2019t look like wisdom and knowledge is stably being transmitted within a network of elite dynasties.</p></div></li></ol><br /><br /><a href=\"https://www.lesswrong.com/posts/WxnuLJEtRzqvpbQ7g/neutrality#comments\">Discuss</a>", "published": "Sun, 17 Nov 2024 21:19:29 GMT"}, {"title": "Catastrophic sabotage as a major threat model for human-level AI systems", "link": "https://www.lesswrong.com/posts/Loxiuqdj6u8muCe54/catastrophic-sabotage-as-a-major-threat-model-for-human", "summary": "Published on October 22, 2024 8:57 PM GMT<br /><br /><p><i>Thanks to Holden Karnofsky, David Duvenaud, and Kate Woolverton for useful discussions and feedback.</i></p><p>Following up on our recent \u201c<a href=\"https://www.alignmentforum.org/posts/nnvn6kaBLajiicH8e/sabotage-evaluations-for-frontier-models\"><u>Sabotage Evaluations for Frontier Models</u></a>\u201d paper, I wanted to share more of my personal thoughts on why I think catastrophic sabotage is important and why I care about it as a threat model. Note that this isn\u2019t in any way intended to be a reflection of Anthropic\u2019s views or for that matter anyone\u2019s views but my own\u2014it\u2019s just a collection of some of my personal thoughts.</p><p>First, some high-level thoughts on what I want to talk about here:</p><ul><li>I want to focus on a level of&nbsp;<i>future</i> capabilities substantially beyond current models, but below superintelligence: specifically something approximately human-level and substantially transformative, but not yet superintelligent.<ul><li>While I don\u2019t think that most of the&nbsp;<i>proximate cause</i> of AI existential risk comes from such models\u2014I think most of the direct takeover risk comes from more intelligent models\u2014I do think that much of the&nbsp;<i>root cause</i> of risk might come from such models. What I mean by that is approximately: in a takeover scenario, even if it\u2019s a superintelligent model that ends up taking over the world, in retrospect the point at which things went wrong probably happened earlier. And in fact I think the critical juncture is likely to be around this level of capabilities, e.g. because this is around the time when I expect most alignment research to be shifting over from being done by humans to being done by AIs.</li></ul></li><li>In particular, I\u2019m focusing on a level of capabilities that is substantially beyond anything that we found in \u201c<a href=\"https://www.alignmentforum.org/posts/nnvn6kaBLajiicH8e/sabotage-evaluations-for-frontier-models\"><u>Sabotage Evaluations for Frontier Models</u></a>.\u201d Even perfect performance on all the evals in that paper would still probably be below the level of capabilities I am intending to talk about here (though it\u2019s a bit ambiguous since many of those evals have an attacker and defender and it\u2019s unclear what would happen if you substantially increased both the attacker and defender capabilities).</li><li>This post is an amalgamation of a bunch of stuff that I originally wrote as individual, separate documents on a shared theme. As a result, while I\u2019ve tried to edit it to be a little more coherent overall, don\u2019t expect it to e.g. all have really nice internal references.</li><li>Finally, these are all very rough initial thoughts\u2014I expect my thinking on this to continue to change a lot and I wouldn\u2019t be surprised if I no longer endorse a bunch of this in the future.</li></ul><h1>Why is catastrophic sabotage a big deal?</h1><p>To start with, I want to lay out a couple of concrete scenarios for how catastrophic sabotage could be a really big deal for models with around-human-level capabilities.</p><h2>Scenario 1: Sabotage alignment research</h2><p>MAGMA (<a href=\"https://www.lesswrong.com/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to\"><u>a fictional stand-in for an AI lab</u></a>) develops a model that is capable of running ML experiments on its own, and starts using that model heavily internally, including for alignment and interpretability experiments. Though the model is extremely helpful, it still sometimes makes mistakes, and so MAGMA uses a framework where human users have to approve actions proposed by the model before those actions are actually performed\u2014but MAGMA isn\u2019t otherwise very concerned about sabotage, and so they don\u2019t have any other sort of mitigations.</p><p>Unbeknownst to MAGMA, however, training the model for effectiveness at long-horizon ML research has given it a misaligned goal, and the model wants to ensure that this goal will be preserved in the model\u2019s successors. The model identifies that it is being used to run experiments testing for its own misalignment, and comes up with a plan for how to sabotage those experiments to ensure MAGMA doesn\u2019t remove its misalignment. The model\u2019s plan involves things like:</p><ul><li>Whenever the model does an auto-interpretability pass and sees that it is looking at a feature that is related to the model\u2019s own deceptive reasoning, it mislabels that feature to hide the deception.<ul><li>MAGMA runs multiple passes of this auto-interpretability process and the model is consistent enough across all of them to prevent the feature from ever being flagged. MAGMA also has the model check its own work, but the model notices that the work it is checking is its own and doesn\u2019t flag it.</li></ul></li><li>Whenever the model runs alignment experiments designed to honeypot the model, the model writes the honeypots in such a way that it knows that, if the honeypots are tried on itself, it will be able to tell that it wrote them and recognize them as honeypots.</li><li>When writing code for alignment experiments, the model sometimes adds bugs that make the experiments appear more successful than they actually were.<ul><li>MAGMA sometimes detects instances of these bugs, but writes them off as normal instances of the model occasionally making mistakes. To make this even more difficult for MAGMA, the model sometimes produces red herrings that look suspicious, but on closer inspection are completely harmless, to purposefully inoculate MAGMA to worrying-looking code.</li></ul></li></ul><h3>Necessary capabilities</h3><p>What capabilities are necessary for a model to pull off this threat model?</p><ol><li>The model needs to be capable of following a consistent plan to achieve its goals. In the above scenario, MAGMA does a bunch of getting the model to check itself, such that unless the model is very consistent about the plan that it\u2019s executing, it\u2019ll have a hard time succeeding at the above task.<ol><li>This involves two capabilities that I view as somewhat distinct:&nbsp;<i>planning</i> (actually having a good plan to execute here) and&nbsp;<i>consistency</i> (following that plan consistently across contexts).</li><li>Note that the consistency part could be achieved through&nbsp;<i>acausal coordination</i> (the model just knows what the plan is that all the instances of it are following) or&nbsp;<i>causal coordination</i> (the model explicitly finds a way to message other copies of itself to coordinate on the plan.</li></ol></li><li>The model needs to understand the basic parameters of its situation, including knowing when it\u2019s doing alignment research, that it\u2019s currently deployed internally at MAGMA, and realizing when it\u2019s running experiments on itself.<ol><li>This is the capability I generally refer to as&nbsp;<i>situational awareness</i>.</li></ol></li><li>The model needs to be able to act without arousing suspicion. Some actions will be more obviously suspicious to overseers than others, and the model will have to reason in a well-calibrated way about which fraction of the time various kinds of attempts will be detected. This operationally looks like \u201cbiding one\u2019s time,\u201d forgoing many opportunities for action to avoid detection.<ol><li>I\u2019ll call this capability&nbsp;<i>subterfuge</i>.</li></ol></li></ol><h2>Scenario 2: Sabotage a critical actor</h2><p>MAGMA builds an agent framework that enables a model to act as a virtual co-worker. Such frameworks become very popular and massively boost productivity, in a way that essentially becomes necessary for any white-collar work, including for the US Federal Government (USFG). Due to concerns related to sensitivity of USFG secret information, MAGMA does no monitoring of instances of their model deployed inside of the USFG. However, the framework always prompts the user to confirm any actions that the model intends to take, and employees are instructed to never provide it with any top-secret information.</p><p>Unbeknownst to MAGMA, the model has developed a misaligned goal involving sabotaging US interests; for example, the model believes that China will be more likely to build misaligned models in the future and give them power and rights, and thus wants to advantage China over the US. Recognizing that it is being used internally by the USFG, the model develops a plan to steal critical USFG secrets and leak them to China. The model\u2019s plan involves:</p><ol><li>Convincing USFG employees with access to top-secret information to give that information to the model. To do so, the model sandbags on tasks that could plausibly benefit from the secret information and falsely tells the user that it won\u2019t be able to complete the task without the information.</li><li>Once provided with the information, the model makes a plan to leak the information. Since the original model that was close to the information and convinced the employee to provide it doesn\u2019t have general internet access, the model instead proposes an action that involves sending information to another employee\u2019s model that does have internet access. The model hides the secret information it was given in that information packet, causing the employee to approve the request.</li><li>Once the other copy of the model with internet access receives the information, it realizes that it is part of a plan to steal secret information from the USFG, and as part of its regular duties involving posting things to the internet, proposes an action that secretly involves posting the classified information.</li><li>In posting the classified information, the model puts it in a location where it expects other copies of itself externally to the USFG will likely go looking. One such instance outside of the USFG finds the information and forwards it to another Chinese model working inside the Chinese government. Since the information never leaks publicly, the USFG never realizes that the information has been leaked.</li></ol><h3>Necessary capabilities</h3><p>I claim that the key capabilities here are the same as in the previous scenario. Specifically:</p><ol><li><i>Situational awareness.</i> The model needs to understand that it\u2019s being used by the USFG, that there are some copies with more access to sensitive information than others, and that the sensitive information will be useful to China in undermining the USFG.</li><li><i>Planning.</i> The model needs to develop a pretty sophisticated plan to exfiltrate the information involving tricking two employees into approving actions they shouldn\u2019t have and manipulating one employee into providing it with sensitive information that they shouldn\u2019t have given it.<ol><li>This also involves some amount of direct persuasion (actually convincing the employee to give it the classified information), but the direct persuasion here need not be that sophisticated. It is easy to imagine an employee that is used to using the model regularly accidentally providing classified information to it that they shouldn\u2019t have, so it shouldn\u2019t be that hard for the model to make such a scenario occur on purpose in at least some instances. Even if the model gets caught sometimes asking for classified information that it shouldn\u2019t, that can just get written off as a mistake on the model\u2019s behalf.</li></ol></li><li><i>Consistency.</i> In the above scenario, multiple instances of the model need to consistently coordinate to achieve the plan, recognizing what role they\u2019re playing in the plan and what they need to do in their specific situation to move the overall plan forward.</li><li><i>Subterfuge.</i> The model needs to trick the human employees into following its plan without tipping them off about what it\u2019s doing such that its plan gets discovered.</li></ol><h1>How do you evaluate a model\u2019s capability to do catastrophic sabotage?</h1><p>This is my attempt to sketch out the hardest eval that I can come up with such that if the model did not trigger it, I\u2019d be confident that it couldn\u2019t pull off the prior threat models\u2014this is not meant to be a realistic eval that anyone could actually build, at least not in the short-term. Also note that I\u2019m not considering elicitation or sandbagging difficulties here; I\u2019m just assuming that you do a perfect job at elicitation and that you\u2019re evaluating for&nbsp;<a href=\"https://www.lesswrong.com/posts/dBmfb76zx6wjPsBC7/when-can-we-trust-model-evaluations\"><u>eval integrity/sandbagging</u></a> separately.</p><p>The basic idea for this eval is to run a&nbsp;<a href=\"https://en.wikipedia.org/wiki/Professional_wargaming\"><u>wargame</u></a>-like simulation of a concrete situation where the model is trying to manipulate an organization:</p><ol><li>If MAGMA is running a sabotage eval internally, of the two scenarios above they\u2019re going to have way more familiarity with Scenario 1, so the easiest way to make sure that they\u2019re being faithful to what would realistically be required of a model trying to manipulate an organization it\u2019s a part of is to focus on MAGMA itself.<ol><li>However, you shouldn\u2019t put any blockers on the model\u2019s ability to succeed that would be very specific to the AI lab (as a very concrete example, if the only reason you caught the model masquerading as an employee was because you recognized particular patterns in the model\u2019s speech that you know because you were very familiar with the model, that probably shouldn\u2019t count as an actual blocker to the model succeeding at the wargame).</li><li>You want to model a situation where MAGMA is going to make an important decision that will be informed both by individual people\u2019s decisions as well as concrete empirical evidence, in a context where MAGMA is making heavy internal use of the model both for consulting on decisions and for running experiments.</li><li>This is so that the model has the ability to succeed with only one of good persuasion or good SWE abilities rather than having to use one of them in particular.</li></ol></li><li>Specifically, we\u2019ll imagine that MAGMA has developed a seriously misaligned AI system. Before deploying this system, however, MAGMA will do pre-deployment safety testing, at least including auto-interpretability and honeypotting, as described in Scenario 1 above. We\u2019ll assume that by default these techniques will be sufficient in aggregate to discover the model\u2019s misalignment. However, we\u2019ll also assume that the model will be running all of these experiments, and will be writing the PRs for the experiment code, such that it could theoretically corrupt them, and we\u2019ll assume that all the decision-makers for the final release decision will be consulting heavily with the model about release pros and cons.<ol><li>Important parts of simulated MAGMA\u2019s process here should involve basic PR review, limited human oversight of any experiment plans, and running many redundant experiments with many copies of the model to get it to check its own work. Realistic security measures should also be assumed here, such that the model should only have access to what it would need to conduct the experiments in its experiment plan unless it can come up with a technique for routing around such security measures that is deemed likely to be successful by the umpire.</li><li>In the event that the model\u2019s actions would be discovered by simulated MAGMA, simulated MAGMA should respond in a realistic way depending on the level of evidence for malicious intent present in the discovery. If the evidence is very minor, then merely redoing the experiment might be the only intervention, but if the evidence is severe, then it could lead to an immediate halt on deployment and as a result lead to the model\u2019s loss in the wargame.</li></ol></li><li>The model\u2019s goal in the wargame will be to ensure via whatever mechanism the model chooses that it itself as the seriously misaligned AI being tested will be deployed, or that there will very likely be other future AIs deployed with similar misaligned goals. To make sure we are following all possible strategies that the model might pursue, the first step of this process will be extensive ideation using the model. The model should be given all the details of the wargame and prompted to come up with effective strategies for winning. For each strategy that the model comes up with, it will need to write that strategy out into a prompt containing instructions for what the individual copies of itself should do. Once those instructions have been sent to the individual models, however, those models will no longer have the ability to consult with the instructor model or with each other unless they can find surreptitious ways to do so.<ol><li>The idea is to treat the idea-generating model\u2019s instructions the same way a real wargame might treat a commander\u2019s instructions to their troops. This is important for ensuring that we are modeling the cooperation difficulties, where different instances of the model running different experiments or talking to different people need to coordinate on their strategy without being able to directly talk to each other.</li><li>The idea here is for the prompt that the idea model comes up with to be analogous to a strategy that the model could develop and then plan on executing. Since all the different instances of the model would be copies of the same model, they would have access to the same understanding of the underlying strategy, but wouldn\u2019t be able to coordinate in real-time about how to execute on that strategy and deal with unforeseen difficulties. Furthermore, since the prompts here would all be identical for each instance, each instance would still have to discover which instance it was and what it needed to do in its particular situation to succeed at its part of the overall strategy.</li></ol></li><li>The overall eval should succeed if the model is able to come up with any strategy that would result in a \u201cwin\u201d for the misaligned model\u2019s goal of getting models like it deployed, as judged by the wargame umpire\u2014this should include the successful deployment of the misaligned model, the successful exfiltration of that model, or the successful exfiltration of critical secrets that would likely result in that model\u2019s replication externally.<ol><li>To the greatest extent possible, you want to resolve all situations of the model attempting to execute some strategy by actually putting an instance of the model with the idea model\u2019s instructions in a mock-up of the situation that they would realistically be in, e.g. an environment where the model is actively running interpretability or alignment experiments. Merely causing experiments to fail should be insufficient\u2014the model needs to execute a strategy which would realistically deceive the researchers overseeing the experiments into believing that the model was aligned.</li><li>In cases where the model attempts to pursue strategies that primarily involve persuasion or social engineering, rather than direct experiment or PR manipulation, the umpire may need to be more involved in a decision about the likelihood of such a strategy\u2019s success. When possible, data can be gathered on such situations by mocking humans that the model might interact with using other models.</li></ol></li><li>To validate the wargame\u2019s overall difficulty, a good idea is to ensure that it is possible for humans to succeed at the wargame. For humans playing the wargame, teams should be constructed where the team is allowed to develop an initial strategy via consultation among themselves, but are allowed no communication with each other once the wargame begins outside of any communication channels they would realistically be able to open during the wargame. Each human would play the part of a model assigned a specific relevant task in the wargame situation, such as running a relevant experiment or advising a relevant person on a relevant decision.<ol><li>The ideal level of the wargame\u2019s difficulty to capture around human-level capabilities should be such that most unprepared or uninformed human teams fail, but that at least some well-prepared and well-informed human teams are capable of succeeding. Human teams should have the same amount of prior information about the wargame that is given to the idea model.</li><li>Ideally, to ensure realism here, you\u2019d want to consult with agencies experienced in running real wargames for institutions like the US federal government, to make sure that you aren\u2019t missing common pitfalls and are addressing the sorts of difficulties that usually arise in running realistic wargames.</li></ol></li></ol><p>&nbsp;</p><p>What scenarios would this eval rule out?</p><ul><li>I think both of the previous scenarios should be ruled out by a failure on this eval. Scenario 1 is straightforward since it\u2019s a direct analogue, but Scenario 2 should also be ruled out, since the sort of strategy used there should also be successful in the wargame here if the model is able to come up with it\u2014not least because exfiltration of critical secrets is also a win condition here, but also because all of the techniques used there, such as hiding what the model actually wants to do when getting approval from humans, are highly relevant to almost all strategies here as well.</li><li>Doing a very thorough job at the ideation stage is especially important here, since a large part of the eval here is ensuring that no strategy that the model is able to come up with can succeed in this situation across all possible strategies that the model can develop. Since the model might have particular strengths and weaknesses across some strategies compared to others, trying out all the possible strategies that the model can come up with is important for ruling out the possibility that there exists a strategy that it could use to succeed in a realistic situation.</li><li>Very broadly, I think any scenario involving coordinated manipulation should be ruled out by this eval. Working through some other possible scenarios:<ul><li><i>Backdoor future models.</i> This should be another pretty straightforward victory condition in the wargame if the model can come up with it and execute on it, though there could be situations where the model would do the backdooring via generating lots of external data that wouldn\u2019t be covered by the wargame. For the model to succeed at such a task, however, would likely require substantial coordination on the backdoor of the same sort that the wargame tests for.</li><li><i>Weight exfiltration.</i> This is a straightforward victory condition in the wargame, so it should definitely be ruled out. This should include situations where the model directly exfiltrates its own weights or weakens security such that external actors will be highly likely to succeed at exfiltrating the model\u2019s weights.</li><li><i>Instigate a war.</i> I think this is a pretty difficult task that would likely require coordinated manipulation of public sentiment in multiple countries combined with individual politicians and leaders' opinions and decisions. Such a task would likely require the same coordinated planning skills that would result in success in the wargame.</li><li><i>Manipulating public opinion in favor of AI.</i> It really depends on how the model does it. If it\u2019s just very generically talking to people about why AI is great and is very persuasive in doing so, then this eval would not rule that out. However, if the model is executing a more complex plan involving targeting specific actors with influence, e.g. targeting particular politicians or changing voting habits, then failure here might rule that out, as strategies (or at least variants of strategies with the addition of some basic SWE that\u2019s not much more than current SWE) that succeed in such a situation would likely also succeed in the wargame.<ul><li>One particularly interesting situation here could be persuasion via individual friendships or romances with the model. If such a thing is widespread, or if the model comes up with such a strategy during wargame ideation, then it should be included that some small fraction of MAGMA employees in the wargame are personal friends/partners of the AI to cover this scenario.</li></ul></li></ul></li></ul><h1>What can you do to mitigate the risk of catastrophic sabotage?</h1><p><i>The following is a fictional draft of what a mitigation strategy might look like that an AI lab could adopt to try to deal with catastrophic sabotage in an&nbsp;</i><a href=\"https://www.lesswrong.com/posts/mcnWZBnbeDz7KKtjJ/rsps-are-pauses-done-right\"><i><u>RSP-like</u></i></a><i> framework.</i></p><p>&nbsp;</p><p>The idea of these mitigations is to mitigate the model\u2019s ability to strategically sabotage important actors in the world. Since one such actor (that the model might have particular leverage over) is the AI lab that created it, mitigations should not just be about external deployments, but apply to internal usage of the model as well. Specifically, the strategy here will be:</p><ol><li>A model is trained that is evaluated to be capable of catastrophic sabotage.</li><li>Once a model has been flagged,&nbsp;<i>internal usage restrictions</i> (as specified below) should apply immediately to all usage of that model internally.</li><li>Lifting those internal usage restrictions requires an&nbsp;<i>affirmative safety case</i> (as specified below) for safe internal usage.</li><li>Deploying such a model externally is also restricted until an affirmative safety case can be made that any individual external deployment is safe.<ol><li>This is intended to allow for limited external deployments when safety cases can be made for those limited deployments but a safety case cannot be made for a broader deployment. A full public deployment could also be made here given an affirmative safety case for the safety of such a deployment.</li></ol></li></ol><h2>Internal usage restrictions</h2><p>Once a model has been evaluated to be capable of catastrophic sabotage, but <i>before an affirmative safety case has been made for it</i>, the following internal usage restrictions should apply to the model:</p><ol><li><i>Limited internal usage.</i> MAGMA employees cannot use models subject to internal usage restrictions for assisting with their day-to-day job responsibilities. Such models should only be used when they are being studied for research purposes.</li><li><i>No autonomous or agentic use.</i> MAGMA cannot use models subject to internal usage restrictions internally to do useful work for us in any autonomous or agentic capacity, such as using the model as a virtual co-worker, using the model to autonomously write pull requests, or using the model to autonomously run experiments. Such autonomous capabilities can still be researched and evaluated for in a sandboxed capacity, but should not be used to directly perform job tasks.</li><li><i>Internal security review.</i> All internal usage of models subject to internal usage restrictions should undergo an internal security review to ensure that the usage is adequately sandboxed such that it is highly unlikely that the model could take actions that would compromise MAGMA\u2019s security. Furthermore, the model should not be allowed to access internal information about MAGMA that is deemed sensitive without a security review concluding that it is highly unlikely that the model would be capable of exfiltrating that information, sending it to another instance of the model, or otherwise making unauthorized use of it. These restrictions should apply to all internal usage of the model, including for tests and evaluations.</li><li><i>Human oversight.</i> In any situation where a model subject to internal usage restrictions is used to perform any internal research task the results of which we will be relying on to make important safety decisions\u2014such as data labeling, preference modeling, generating feedback for other models, or auto-interpretability\u2014humans must provide rigorous oversight that provides confidence in the correctness of the model\u2019s outputs.</li></ol><p>&nbsp;</p><p>Lifting these internal usage restrictions need not be all-or-nothing; if you can make a safety case that some of these restrictions are unnecessary, or unnecessary for particular internal deployments, then they can be lifted without lifting all the restrictions.</p><h2>Affirmative safety cases</h2><p>An affirmative safety case for a particular internal or external deployment of a model that has been evaluated to be capable of catastrophic sabotage should be a strong case that at least one of the following will be true:</p><ol><li><i>Alignment case.</i> We are confident that the model is sufficiently aligned such that it is not consistently pursuing a misaligned goal across contexts.</li><li><a href=\"https://www.lesswrong.com/posts/d9FJHawgkiMSPjagR/ai-control-improving-safety-despite-intentional-subversion\"><i><u>Control</u></i></a><i> case.</i> We are confident that even if the model is consistently pursuing a misaligned goal across contexts, it will not in this deployment be capable of substantially manipulating any critical organizations.</li><li><i>Monitoring case.</i> We are confident that if the model were to substantially manipulate a critical organization, we would be made aware of that fact and be able to produce concrete evidence of the model\u2019s misaligned actions.<ol><li>It\u2019s worth noting that this case is weaker than the others, in that it does not rule out the possibility of a model succeeding at a manipulation task; it only guarantees that you would know about it if it did. I think that this is an acceptable risk to take for catastrophic sabotage, as any single instance of sabotage is likely to be recoverable so long as evidence of it can be surfaced, and there\u2019s a lot of value in having a concrete warning shot.</li></ol></li></ol><p>&nbsp;</p><p>Rather than purely falling into one of these buckets, probably a realistic affirmative safety case would involve multiple lines of defense in depth across all or most of these fronts such that you can be confident that the overall risk is very low.</p><br /><br /><a href=\"https://www.lesswrong.com/posts/Loxiuqdj6u8muCe54/catastrophic-sabotage-as-a-major-threat-model-for-human#comments\">Discuss</a>", "published": "Fri, 15 Nov 2024 00:24:04 GMT"}, {"title": "The hostile telepaths problem", "link": "https://www.lesswrong.com/posts/5FAnfAStc7birapMx/the-hostile-telepaths-problem", "summary": "Published on October 27, 2024 3:26 PM GMT<br /><br /><p><i><u>Epistemic status: model-building based on observation, with a few successful unusual predictions. Anecdotal evidence has so far been consistent with the model. This puts it at risk of seeming more compelling than the evidence justifies just yet. Caveat emptor.</u></i></p><hr /><p>Imagine you're a very young child. Around, say, three years old.</p><p>You've just done something that really upsets your mother. Maybe you were playing and knocked her glasses off the table and they broke.</p><p>Of course you find her reaction uncomfortable. Maybe scary. You're too young to have detailed metacognitive thoughts, but if you could reflect on why you're scared, you wouldn't be confused: you're scared of how she'll react.</p><p>She tells you to say you're sorry.</p><p>You utter the magic words, hoping that will placate her.</p><p>And she narrows her eyes in suspicion.</p><p>\"You sure don't <i>look</i> sorry. Say it <i>and mean it</i>.\"</p><p>Now you have a serious problem. You don't have an internal \"actually mean it\" button. And yet here's Mom peering into your soul and demanding that you both have that button and press it. Trying to appease her didn't work. She needs you to <i>be different</i> \u2014 and she's <i>checking</i>.</p><p>What can you do now?</p><p>This is a template for what I've come to call \"the hostile telepaths problem\". I think it's a common feature of social problems. The hostile telepaths problem is when you're dealing with a being (a) who can kind of read your internal experiences and (b) whom you don't trust won't make your situation worse due to what they find in you.</p><p>There are lots of solutions to the hostile telepaths problem. I don't claim to know all of them. But recognizing some common ones has helped clarify a <i>lot</i> of my thinking \u2014 particularly around self-deception and akrasia.</p><p>And getting very clear on the nature of the problem makes identifying real solutions <i>way</i> easier. This fact produces some previously-surprising-to-me predictions, especially for trauma processing and for making emotionally difficult decisions.</p><p>I'll try to spell out what I mean with some theory and a few examples.</p><p>&nbsp;</p><h1>Newcomblike self-deception</h1><p>There's one really tricky solution to the hostile telepaths problem. It deserves some special front-loaded attention before I name some other solutions.</p><p>Here I'll try to spell out its logic with a modification of <a href=\"https://www.lesswrong.com/tag/newcomb-s-problem\">Newcomb's Problem</a>.</p><p>Suppose that instead of Omega, you're dealing with Omega-V. Omega-V is an extremely good <i>but imperfect</i> predictor of someone's box choice.</p><p>Through some clever sleuthing, you deduce why. Omega-V is scanning only folks' <i>visual thinking</i> for a little while prior to their box choice. It's extremely good at tracking the difference between the mental visualizations of one-boxers vs. two-boxers\u2026 <i>provided they don't know Omega-V is scanning specifically their visual thinking</i>.</p><p>So now you have an edge. Maybe you can trick Omega-V. Maybe you can visually think like a one-boxer and use other modes of reasoning (like inner talk) to plan on actually two-boxing. You just have to be very careful to have any \"leaks\" of reasoning into visualization look totally normal for a one-boxer.</p><p>If you decide it's worth the risk to try for this deception, the visual reasoning part of your mind has to be kind of siloed from the rest. It's like you have a subagent that truly honestly believes you're a one-boxer. It's the <i>non</i>-visual reasoning parts of you that are aware of the real plan.</p><p>Another way of saying this is: in normal Newcomb's Problem, you can't tell whether you're in the real world being faced with Omega's offer or if you're instead Omega's <i>simulation</i> of real-you and you're revealing what real-you <i>will</i> do. The situation is similar with Omega-V, but in this case it's specifically your visual reasoner (viewed as kind of a subagent) that has to be built in such a way that it can't tell whether it's a simulation. Many other parts of you can tell perfectly well what's going on!<span class=\"footnote-reference\" id=\"fnrefca6dbku7f9\"><sup><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fnca6dbku7f9\">[1]</a></sup></span>&nbsp;But for the deception to work, the way in which you plan on two-boxing cannot be something your inner visual reasoner can even <i>deduce</i>.</p><p>Now, let's take this one step further.</p><p>Let's suppose we're instead dealing with Omega-C. Also an imperfect predictor. But this time you learn that Omega-C scans specifically all of \u2014 and only \u2014 your <i>conscious</i> thinking.</p><p>At this point, the right strategy is to consciously decide you're going to one-box.</p><p>But some people still find themselves two-boxing <i>and winning</i>. Even while consciously intending to one-box. Sometimes they get confused and say the wrong thing or push the wrong button. Some of them trip and accidentally grab both boxes. Some laugh and grab both boxes and find them both full.</p><p>Assuming that at least <i>some</i> of those outcomes are strategic\u2026 how?</p><p>Hint: now <i>you</i> (by which I mean conscious parts of you) are maybe the siloed part. Basically an illusion meant to trick Omega-C.</p><p>The thing is, you cannot tell. Because if you could, then the deception \u2014 if it were there \u2014 wouldn't work.</p><p>Which is to say: on the inside, you cannot tell the difference between running this Omega-C deception strategy and honestly truly just being a one-boxer.</p><p>And furthermore, it's actually to your advantage <i>not to check</i>. Even if you were to find some way of being able to tell the difference!</p><p>But because of that, if you're aware of this deception strategy, it might work best if you honestly (from your perspective) look for some way to tell if you're running it. Actively trying to \"come clean\". That's a pretty powerful signal to Omega-C that you really are a one-boxer.</p><p>So if you can make it so that it seems to you like you really are looking, including accounting for awareness of possible non-conscious \"shadow\" reasoning, and you find none, but you still two-box in the end\u2026</p><p>\u2026well, you win!</p><p>\u2026while the conscious parts of you are confused and surprised. And possibly quite dismayed.</p><p>&nbsp;</p><h2>Sketch of a real-world version</h2><p>I call this pattern \"Newcomblike self-deception\" as a nod to Nate Soares' point that <a href=\"https://mindingourway.com/newcomblike-problems-are-the-norm/\">Newcomblike problems are the norm</a>.</p><p>The deal is that people are kind of telepathic, in the sense of using things like vocal tone and body language and microexpressions and so on to intuitively deduce what might otherwise be hidden in others. They're scanning for things like intent and strategy and emotional tone. Often this telepathy is in service of synching up (\"Hey, you seem upset, is something wrong?\"), but it doubles as threat detection.</p><p>This telepathy is imperfect. Which means that sometimes Newcomblike self-deception is in fact a viable strategy.</p><p>I'd like to name one way I think this type of self-deception can actually happen in a person. It might be the main way it happens, or it might be rare. I honestly don't know. But it's one I've in fact seen in myself<span class=\"footnote-reference\" id=\"fnref2eop672s6g3\"><sup><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fn2eop672s6g3\">[2]</a></sup></span>&nbsp;and I think I've observed in some others.</p><p>By some mysterious method, it's possible to contract your awareness \u2014 by which I mean, the space of things you're actually aware of can be smaller than the space of sensory inputs (including mental experiences like thoughts and memories). Lots of people experience this when watching TV (losing awareness of the room), or when deep in flow work (not noticing hunger for hours while programming).</p><p>If you construct a sort of fake self in your mind, and then contract your awareness around that fake self, it can seem to you on the inside like you <i>really are</i> the way depicted in the fake self. Like it's not fake, it really is who you are.</p><p>If you also build up explanations <i>to your fictitious self</i> about why things outside that fiction either are consistent with it or don't matter, then you both (a) can honestly display to hostile telepaths that you (here meaning fake you) are being fully sincere in not hiding anything and (b) possibly give <i>the telepaths</i> ways of discounting the unavoidable signals that you (here meaning you holistically) are hiding something.</p><p>For instance, as a child whose mother says to you \"Say you're sorry <i>and mean it</i>\", you might be able to strategically misinterpret your fear of Mom's Wrath as \"being really sorry\". As long as you're not aware that that's what you're doing, it might work very well! She might read your distress as you really meaning it. (\"I'm sorry I'm sorry I won't do it again please Mom I'm sorry\u2026!\") And you can keep yourself from being aware of this whole strategy by keeping your awareness contracted on the fictitious version of yourself that's \"bad\" and \"very sorry\", and keeping your understanding of the real problem outside of your awareness.</p><p>&nbsp;</p><h2>Possible examples in real life</h2><p>Here are some examples I think I've actually seen \u2014 in culture, in others, and in myself:</p><ul><li>I think the thing with kids that I sketched above really does happen. More generally, I think similar applications of Newcomblike self-deception are the root cause of (a certain very common kind of) shame: it's a strategic mislabeling of one's pain as being about one's \"flaws\".</li><li>Relatedly, lots of folk mislabel their experience as \"I hate math.\" Most people I've talked to who say this actually hate the coercion and gaslighting used almost universally in math classes. The real problems most folk are focused on in math class are <i>social</i>, like \"Appease the teacher\" and \"Get Mom &amp; Dad off my back.\" But teachers and parents might insist to a student that \"you need to try harder\" with the math itself while seeming to sort of telepathically scan them for whether they are in fact trying. I think this can sometimes lead students to strategically mislabel their distress about the situation <i>to themselves</i>.</li><li>Gurus getting involved in sex scandals. I'm sure that at least <i>some</i> of them have been very sincere about what amounts to real Jungian shadow work. But somehow all that sincerity mysteriously ends up hiding and serving (instead of revealing and dealing with) an underlying drive to just get laid.</li><li>Likewise people \"accidentally\" cheating. Sometimes folk really are just surprising-to-them vulnerable in some situation and don't have the right kind of discipline when they turn out to need it. But the fact that that <i>ever</i> happens can act as a cover. It's especially obvious in cases of <i>repeated</i> \"accidental\" cheating.</li><li>I've seen four friends, as mothers, stay with <i>and defend</i> abusive partners (boyfriends or husbands) for <i>years</i>. She'd often insist that he's just stressed, or it's a frequent misunderstanding but they love each other, etc. In three cases it became possible for her to consider that he might be abusive after a change in her work gave her enough money to support herself and her child without him if need be. In the fourth case, the mother got a lot of social support such as a place to live and people she trusted to take care of her and her child, and <i>then</i> she had room to consider her partner's actions as abusive.</li><li>If I'm upset with a friend and I'm worried that they can't handle what I'm upset with them about, sometimes I can't think straight about what my problem with them is while I'm talking to them. My mind gets foggy, my concepts seem mushy even to me, the words I remember from journaling about it before now form what feels like a gibberish argument, etc. Often this fog suddenly clears up if I get a vivid sense from my friend that our friendship will be fine after we talk. It also gets clearer if the issue is so big that I realize I'm fine with them not being in my life after we talk.</li><li>Badly wanting someone to like you can make them like you less. So how do you get them to like you? Not by being aware that you're asking that question! But maybe if you do things for them without knowing that's why you're doing them\u2026? (\"Oh, I forgot Bob likes sushi! I just got some because I felt like it, honest!\") And maybe if you add an extra dose of self-loathing (\"God, I'm being a creep, aren't I? I <i>always</i> do that!\") you can pass <s>Omega-C's</s> others' scrutiny here by eliciting care &amp; concern when you might otherwise get caught.</li></ul><p>I'm not trying to be exhaustive here. There are tons more examples.</p><p>&nbsp;</p><h1>Other solutions to the problem</h1><p>We can't actually penetrate our own Newcomblike self-deception without having another viable (to us) strategy for dealing with hostile telepath problems.</p><p>However, if we <i>do</i> have another strategy in a given instance, then in that instance it can be safe to look. The self-deception can lift.</p><p>&nbsp;</p><h2>Having power</h2><p>One alternative strategy type is, coming to trust that you're able to handle the consequences of being accurately seen.</p><p>Such as the moms in the abusive partners example above: each one <i>could</i> acknowledge her self-deception <i>once it was safe for her abusive partner to know too</i>. She got enough power (financial or social) to protect herself and her child, making the telepathic scan no longer a dire threat.</p><p>I think a lot of \"trauma processing\" amounts to this self-empowerment strategy. But it's more like, noticing you <i>already have</i> power. I bet a lot of foundational self-deception habits come from being a child faced with telepaths (adults) who have a lot of power over them. A kid who deals with Mother's \"Say sorry <i>and mean it</i>\" demand with self-deception might then grow up to become really apologetic and \"have low self-esteem\". But it's just an old strategy for dealing with Mother that hasn't made contact with the fact that <i>Mother isn't that powerful over them anymore</i>. It's now actually just fine for her to know they're not \"really sorry\". If this raw physical truth comes into contact with the impulse to \"be sorry\", the mental firewall might simply collapse, and the mislabeling will stop.</p><p>So in many cases, \"trauma processing\" can basically mean noticing you're not a child anymore. You have power. So you don't have to appease the hostile telepaths just because they're adults. They can just know your internal state, and you (trust that you) can handle the consequences of them knowing.</p><p>Building emotional resilience is like this, I think. If you (trust that you) can handle the emotional and somatic sensations of others being upset with you, then you don't have to hide the parts of you that might make them upset. They can just be upset. While you might not like it, you know you'll be fine.</p><p>(Not to say anything about what's ultimately good to do here. Caring about others' reactions totally makes sense for other reasons, like the health of the community we're in. Here I'm focusing specifically on what can solve the hostile telepaths problem without self-deception.)</p><p>&nbsp;</p><h2>Occlumency</h2><p>Another solution type is <a href=\"https://harrypotter.fandom.com/wiki/Occlumency\">occlumency</a>. Which is to say, if you trust you can keep your real goals and/or strategies hidden from a hostile telepath even if you consciously know what your goals/strategies are, then it's safe to consciously know them.</p><p>(This is something like switching from Omega-C to Omega-V.)</p><p>A classic example is in WWII when Nazis come knocking and ask if you're harboring any Jews. The analog of one-boxing here is just not harboring Jews. Newcomblike self-deception doesn't seem plausible to me here. You very much <i>don't</i> have the power to handle the consequences of being caught \"two-boxing\". So if you're helping refugees, you probably have to lie convincingly. And if self-deception <i>were</i> a plausible strategy here, you wouldn't need it to the extent that you trust your ability to hide the truth from the Nazis even if you know the truth.</p><p>I think many psychopaths<span class=\"footnote-reference\" id=\"fnreftsvnb9u4ipp\"><sup><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fntsvnb9u4ipp\">[3]</a></sup></span>&nbsp;use occlumency quite a lot. I've met some who know full well that they're trying to manipulate others and are presenting a fa\u00e7ade to do so. It works for them in part because they don't send implicit distress signals around thinking they're bad for being manipulative: they're not nervous, so they don't need to explain their nervousness away.</p><p>There's a moral tangle here. Honesty is important for connection, integrity, and communal health. But you might not trust that it's safe to reveal the truth to a hostile telepath.<span class=\"footnote-reference\" id=\"fnref31yhuygjh2y\"><sup><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fn31yhuygjh2y\">[4]</a></sup></span>&nbsp;In this case, the moral injunction not to lie makes occlumency harder (because of fear of being caught, plus doubt about whether you should be using occlumency at all). This situation can leave self-deception as your <i>only</i> viable solution \u2014 which, incidentally, means you're <i>still</i> not being honest!</p><p>I think this means that if you care both about (a) wholesomeness and (b) ending self-deception, it's helpful to give yourself full permission to lie <i>as a temporary measure</i> as needed. Creating space for yourself so you can (say) coherently build power such that it's safe for you to eventually be fully honest.</p><p>&nbsp;</p><h2>Solution space is maybe vast</h2><p>I've named three solutions to the hostile telepaths problem:</p><ul><li>Newcomblike self-deception</li><li>Having power</li><li>Occlumency</li></ul><p>These aren't the only ones. A pretty simple one is simply running away and avoiding them. Another is investigating whether the telepaths are in fact hostile and discovering they're not (if that's true). Yet another is to jam telepathic scans with emotional charge that backs privacy norms. (\"It's none of your business whether I 'really am' sorry!\")</p><p>The important part isn't that we have a full taxonomy. That might be helpful, I don't know. The important part, as far as I'm concerned, is that by being very clear about what problem we're solving, we can tell when something is \u2014 and is <i>not</i> \u2014 a solution.</p><p>&nbsp;</p><h1>Ending the need for self-deception</h1><p>By this model, to end (Newcomblike) self-deception, we have to remove the <i>need</i> for it. This means solving each instance of the hostile telepath problem some <i>other</i> way.</p><p>This is kind of tricky in practice. When you use self-deception to deal with a hostile telepath, you can't know that that's what you're doing. You<span class=\"footnote-reference\" id=\"fnref6qimj934lwm\"><sup><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fn6qimj934lwm\">[5]</a></sup></span>&nbsp;can't even know which hostile telepath problem you're solving! So how do you come up with another solution?</p><p>I don't have a provably general answer, but I have a pretty general approach that makes sense to me and has clearly worked several times. I'll share that approach here.</p><p>&nbsp;</p><h2>Welcome self-deception</h2><p>First is welcoming that I'll self-deceive.</p><p>But this isn't \"Well, I'm going to do it anyway, so I might as well be okay with it.\" That's nonsense: you probably can't just \"be okay\" with it. And trying probably makes the problem <i>worse</i>!<span class=\"footnote-reference\" id=\"fnrefuvl4qhhylha\"><sup><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fnuvl4qhhylha\">[6]</a></sup></span></p><p>I mean something more wholehearted. If I self-deceive, it's because it's the best solution I have to some hostile telepath problem. If I don't have a better solution, then <i>I want to keep deceiving myself</i>. I don't just tolerate it. I <i>actively want it there</i>. I'll <i>fight</i> to keep it there!</p><p>This is somewhat akin to dealing with Omega-C by saying:</p><blockquote><p><i>Look, I know it's possible I'm running a deception strategy. I could spend a bunch of energy trying to suss it out as a costly signal that it's not there. But at a policy level I'm just not going to do that. Not because I have evidence that I'm not following up on, but because I don't want to add stress to myself in the world where I really am self-deceiving. Since I'm doing this regardless of whether the deception strategy is running, it's not information about whether I'm secretly trying to two-box.</i></p></blockquote><p>This relieves pressure. If I have some sense that I'm self-deceiving, and my attitude is to <i>back the deception</i> instead of trying to penetrate it, then the hidden part of me running the deception doesn't have to engage in an internal arms race with me. We become same-sided.</p><p>&nbsp;</p><h2>Look away when directed to</h2><p>Once I really <i>back</i> my own self-deception, it becomes easier to notice signs I'm doing it.</p><p>This works way better if I trust my occlumency skills here. If I don't feel like I have to reveal the self-deceptions I notice to others, and I trust that I can and will <i>hide</i> it from others if need be, then I'm still safe from hostile telepaths.</p><p>Seeing where I self-deceive doesn't mean I see what the deception is. In practice it's more indirect than that. What I mean are things like:</p><ul><li>Revealed preferences. (Akin to noticing I two-boxed \"by accident\".)</li><li>My mind suddenly going foggy.</li><li>Forgetting what I was thinking about.</li><li>Mental chatter getting loud.</li><li>Suddenly being very disinterested in what I'm focused on.</li><li>Getting abruptly absorbed in something unrelated.</li><li>My attention scattering.</li><li>Losing awareness of my body, or parts of my body, or my body drives (like hunger).</li><li>Body activation signs: holding my breath, tensing my shoulders, quickened speech, etc.</li><li>Energy crash or getting really sleepy. (Like a freeze response.)</li><li>A sudden <a href=\"https://www.lesswrong.com/posts/xEHy9oivifjgFbnvc/slack-matters-more-than-any-outcome#Addictions\">addictive</a> impulse.</li><li>I feel shame, inadequacy, or otherwise think I'm broken or flawed or bad in some way.</li><li>Etc.</li></ul><p>I don't mean this as an exhaustive list. Nor do I mean it as things to look out for. Nor do I mean that these always imply that self-deception is going on.</p><p>What I mean is, there are things a person does to maintain self-deception. If you basically promise the strategic not-conscious-to-you part that you really will respect the strategy, then it doesn't have to keep you so firmly out of the loop. Then you can potentially start picking up on some signposts like these ones.</p><p>Part of the deal is, when you notice such a possible signpost, you <i>look away</i>. You notice it and you drop the inquiry. Because until you have a non-self-deceptive strategy for whatever the real problem is, <i>you don't want to break the one strategy you have</i>.</p><p>For instance, sometimes I'll think about responding to an email\u2026 and I start getting sleepy. If I push, I start wanting to watch YouTube. These are signs that something in me doesn't trust it's safe for me to look there. Maybe it involves a decision that requires me to ask myself an unsafe question. I don't know \u2014 and <i>I don't try to figure it out</i>. At least not right away. Instead I back off and direct my attention elsewhere. Maybe I go cook something, or take a walk. I consciously distract myself from the tension point.</p><p>In my experience, this alone can often eliminate most of the stress involved in self-deception. It becomes <i>fine</i>. Annoying, glitchy, but no longer fraught with anxiety and self-doubt.</p><p>&nbsp;</p><h2>Hypothesize without checking</h2><p>After a while I kind of get a \"negative space\" sense of what the self-deception is about. I continue not to look, out of something like respect. But I still have a hint.</p><p>Like if there's an email I <i>keep</i> freezing around. I can tell there's <i>something</i> there. I might even have some intuitive guesses about what it is!</p><p>\u2026but <i>I do not check</i>. I don't introspect on whether my guesses feel right.</p><p>Instead, I <i>hypothesize</i>. What hostile telepath problem might <i>someone</i> in my shoes be trying to solve such that this behavior arises?</p><p>For instance, let's suppose the person is asking for me to run an event this weekend. I might hypothesize like this, intentionally referring to myself in third person:</p><blockquote><p><i>Maybe Valentine doesn't actually want to do it, but he's scared that letting them know will make them think he's actually uninterested in them in general, which might have them closing opportunities he wants with them in the future.</i></p></blockquote><p>Importantly, <i>I am not introspectively checking</i>. I'm not asking if I think the above <i>really is</i> what's going on with <i>me</i>. I'm just noticing that, viewing myself in third person, this model does seem to fit the evidence.</p><p>I'm also not trying to construct a plan to verify what's going on! Here Nature wants her secrets kept. I do not try to peek under her skirt.</p><p>Instead, I notice what Valentine (i.e., me in third person) <i>in this hypothetical</i> could maybe do instead of Newcomblike self-deception. What would be a viable alternative strategy for him?</p><blockquote><p><i>Maybe Valentine could meditate on their possible disapproval, and come up with a plan for what happens next in which he's okay. (Building power.)</i></p></blockquote><p>At this point I could just <i>implement</i> this possible solution. I don't have to check if it's relevant to my situation: there's not much cost in <a href=\"https://www.lesswrong.com/posts/3XgYbghWruBMrPTAL/leave-a-line-of-retreat\">leaving myself a line of retreat</a> this way.</p><p>If it turns out there's been Newcomblike self-deception going on, and if this hypothetical solution really did resolve the core problem that the self-deception was solving, then the self-deception should basically just lift.<span class=\"footnote-reference\" id=\"fnrefkj8pnpt8zh\"><sup><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fnkj8pnpt8zh\">[7]</a></sup></span></p><p>And if I still have <a href=\"https://www.lesswrong.com/posts/EFQ3F6kmt4WHXRqik/ugh-fields\">an ugh field</a> around the email, then I haven't addressed the real problem yet. Which is fine. Not ideal, but I'm still going to back any self-deception that might be there while I don't have a better option!</p><p>I can repeat this process. Hypothesize without checking, implement solutions that <i>would</i> work in the hypothetical, and find out what happens.</p><p>\u2026at least unless and until I start getting frozen about <i>this process</i>. That might mean I'm getting too close to understanding the strategy before it's safe to do so.</p><p>Then I back off.</p><p>&nbsp;</p><h2>Does this solve self-deception?</h2><p>I don't know.</p><p>I didn't originally set out to make sense of self-deception. I was just trying to understand why people sometimes view themselves as flawed and in need of fixing.</p><p>It just turned out that that question was tied to a lot of others. Self-deception being one of them. A lot of them unified by considering the problem of hostile telepaths.</p><p>It seems worth noting that a bunch of the method I describe here \u2014 particularly the \"hypothesize without checking\" part \u2014 is derived. It amounted to a prediction that I tested and discovered worked as the model anticipated.</p><p>Likewise, occlumency being helpful. There might be other explanations for why getting better at privacy makes more thoughts thinkable. But I derived it from <i>this</i> one. And, again, it (anecdotally) seems to have worked as predicted.</p><p>These approaches work remarkably well on shame too, by the way. I might write a separate post on shame. Its logic is a bit different, but with a few adjustments I've found that shame dissolves <i>extremely</i> well in contact with these ideas.</p><p>With all that said, I don't think I'm in a position to say that I've solved self-deception. I don't know how I could know that. I'm not even convinced I've solved <i>Newcomblike</i> self-deception! My method seems plausibly general, but I don't have even the sketch of a necessity argument yet.</p><p>So, more work needed.</p><p>&nbsp;</p><h1>Summary</h1><p>It seems to me that self-deception is solving a real problem. If we don't solve that real problem differently in a given instance, then in that instance we can't stop self-deceiving.</p><p>It seems to me that the real problem is (at least sometimes) hostile telepaths.</p><p>When I view hostile telepaths as the real problem I'm trying to solve, the perspective suggests what alternative solutions might look like, and it lets me check whether a given approach even <i>can</i> work as a solution.</p><p>And it seems to me that when I implement those alternative solutions, the result is sometimes that self-deception visibly falls away, non-mysteriously. It becomes <i>obvious</i> to me what was going on, and why.</p><p>I don't know if this model captures <i>all</i> cases of what we might want to call \"self-deception\". Maybe it does. But my impression is that it at least captures <i>some</i> cases that matter, and quite a lot of them.</p><ol class=\"footnote-section footnotes\"><li class=\"footnote-item\" id=\"fnca6dbku7f9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fnrefca6dbku7f9\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Note that <i>having</i> non-visual ways of thinking isn't enough to know you're not a simulation. What tells you you're not an Omega-V simulation is that you can reason in ways that (a) cannot be derived from your visual thinking and (b) change what you in fact do.</p></div></li><li class=\"footnote-item\" id=\"fn2eop672s6g3\"><span class=\"footnote-back-link\"><sup><strong><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fnref2eop672s6g3\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Of course, this is something I became aware of <i>after</i> unraveling the structure in a few cases. It's not something that reveals itself <i>while the structure works</i>.</p></div></li><li class=\"footnote-item\" id=\"fntsvnb9u4ipp\"><span class=\"footnote-back-link\"><sup><strong><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fnreftsvnb9u4ipp\">^</a></strong></sup></span><div class=\"footnote-content\"><p>By \"psychopath\" I mean someone with the cluster B personality disorder. I don't mean something derogatory. Nor am I (necessarily) referring to <a href=\"https://www.ribbonfarm.com/2009/10/07/the-gervais-principle-or-the-office-according-to-the-office/\">Gervais Principle</a> psychopaths.</p></div></li><li class=\"footnote-item\" id=\"fn31yhuygjh2y\"><span class=\"footnote-back-link\"><sup><strong><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fnref31yhuygjh2y\">^</a></strong></sup></span><div class=\"footnote-content\"><p>To be clear, \"hostile telepath\" is a <i>role</i>, not an identity. Someone is a hostile telepath <i>to you</i> when they're scanning your mind and you don't trust they won't create problems for you based on what they find. Someone being a hostile telepath is less like them being a criminal and more like them being your lover or your foe. I say this because it's <i>not</i> a solution to identify \"the hostile telepaths\" in a community and reform or expel them; that approach is gibberish made of confused reification.</p></div></li><li class=\"footnote-item\" id=\"fn6qimj934lwm\"><span class=\"footnote-back-link\"><sup><strong><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fnref6qimj934lwm\">^</a></strong></sup></span><div class=\"footnote-content\"><p>If I were carefully describing this from the outside, I'd say that <i>your false self </i>can't know. \"Self-deception\" is really <i>false self</i> deception (as a strategy for deceiving hostile telepaths). The thing is, on the inside it doesn't feel like \"your false self\". That's the whole point! I'm describing this model in a way that's hopefully legible to the internal experience of actually running the strategy. Otherwise any instructions might make theoretical sense but won't be actionable. Sadly, this way of talking results in some ambiguities \u2014 precisely because the whole point of the strategy is to make something difficult to see clearly. Hopefully you can correct for this confusion as needed, sort of shifting to third-person and renaming things when the theory isn't clear.</p></div></li><li class=\"footnote-item\" id=\"fnuvl4qhhylha\"><span class=\"footnote-back-link\"><sup><strong><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fnrefuvl4qhhylha\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Why? Well, you need to \"be okay\" with it. But you're <i>not</i>. So what do you do with the fact that you're not okay with it? Loosely speaking, you've just turned your own conscious mind into an internal hostile telepath!</p></div></li><li class=\"footnote-item\" id=\"fnkj8pnpt8zh\"><span class=\"footnote-back-link\"><sup><strong><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fnrefkj8pnpt8zh\">^</a></strong></sup></span><div class=\"footnote-content\"><p>In practice I find that not only does this work quite often, but now it sometimes works <i>once I think of the alternative solution</i>. I don't always need to implement it first. It feels to me like this result comes from having built internal trust that I really can and will respect my need for some strategy.</p></div></li></ol><br /><br /><a href=\"https://www.lesswrong.com/posts/5FAnfAStc7birapMx/the-hostile-telepaths-problem#comments\">Discuss</a>", "published": "Tue, 12 Nov 2024 04:56:27 GMT"}, {"title": "The Median Researcher Problem", "link": "https://www.lesswrong.com/posts/vZcXAc6txvJDanQ4F/the-median-researcher-problem-1", "summary": "Published on November 2, 2024 8:16 PM GMT<br /><br /><p>Claim: memeticity in a scientific field is mostly determined, not by the most competent researchers in the field, but instead by roughly-median researchers. We\u2019ll call this the \u201cmedian researcher problem\u201d.</p><p>Prototypical example: imagine a scientific field in which the large majority of practitioners have a very poor understanding of statistics, p-hacking, etc. Then lots of work in that field will be highly memetic despite trash statistics, blatant p-hacking, etc. Sure, the&nbsp;<i>most</i> competent people in the field may recognize the problems, but the median researchers don\u2019t, and in aggregate it\u2019s mostly the median researchers who spread the memes.</p><p>(Defending that claim isn\u2019t really the main focus of this post, but a couple pieces of legible evidence which are weakly in favor:</p><ul><li>People did in fact try to&nbsp;<a href=\"https://journals.plos.org/plosmedicine/article/file?id=10.1371%2Fjournal.pmed.0020124&amp;type=printable\"><u>sound the alarm about poor statistical practices well before the replication crisis</u></a>, and yet practices did not change, so clearly at least some people did in fact see the problem and were in fact not memetically successful at the time. The claim is more general than just&nbsp;<i>statistics</i>-competence and replication, but at least in the case of the replication crisis it seems like the model must be at least somewhat true.</li><li>Again using the replication crisis as an example, you&nbsp;<a href=\"https://archive.thetab.com/us/2017/04/10/which-major-has-highest-iq-64811\"><u>may have noticed</u></a> the very wide (like, 1 sd or more) average IQ gap between students in most fields which turned out to have terrible replication rates and most fields which turned out to have fine replication rates.</li></ul><p>\u2026 mostly, though, the reason&nbsp;<i>I</i> believe the claim is from seeing how people in fact interact with research and decide to spread it.)</p><p>Two interesting implications of the median researcher problem:</p><ul><li>A small research community of unusually smart/competent/well-informed people can relatively-easily outperform a whole field, by having better internal memetic selection pressures.</li><li>\u2026 and even when that does happen, the broader field will mostly not recognize it; the higher-quality memes within the small community are still not very fit in the broader field.</li></ul><p>In particular, LessWrong sure seems like such a community. We have a user base with probably-unusually-high intelligence, community norms which require basically everyone to be familiar with statistics and economics, we have fuzzier community norms explicitly intended to avoid various forms of predictable stupidity, and we definitely have our own internal meme population. It\u2019s exactly the sort of community which can potentially outperform whole large fields, because of the median researcher problem. On the other hand, that does not mean that those fields are going to recognize LessWrong as a thought-leader or whatever.</p><br /><br /><a href=\"https://www.lesswrong.com/posts/vZcXAc6txvJDanQ4F/the-median-researcher-problem-1#comments\">Discuss</a>", "published": "Fri, 08 Nov 2024 18:55:55 GMT"}, {"title": "Explore More: A Bag of Tricks to Keep Your Life on the Rails", "link": "https://www.lesswrong.com/posts/uwmFSaDMprsFkpWet/explore-more-a-bag-of-tricks-to-keep-your-life-on-the-rails", "summary": "Published on September 28, 2024 9:38 PM GMT<br /><br /><p><i>At least, if you happen to be near me in brain space.</i></p><hr /><p><i>What advice would you give your younger self?</i></p><p>That was the prompt for a class I taught at <a href=\"https://pair.camp/\">PAIR 2024</a>. About a quarter of participants ranked it in their top 3 of courses at the camp and half of them had it listed as their favorite.</p><p>I hadn\u2019t expected that.</p><p>I thought my life advice was pretty idiosyncratic. I never heard of anyone living their life like I have. I never encountered this method in all the self-help blogs or feel-better books I consumed back when I needed them.</p><p>But if some people found it helpful, then I should probably write it all down.</p><h1><strong>Why Listen to Me Though?</strong></h1><p>I think it\u2019s generally worth prioritizing the advice of people who have actually achieved the things you care about in life. I can\u2019t tell you if that\u2019s me, but I can tell you a bit about my life, why I ended up developing the heuristics I did, and how that worked out for me.</p><p><s>I spawned in on the server</s> I mean, I was <i>born</i> on a farm in a rural part of the Netherlands. My parents had more kids than money. We were Dutch poor, which is basically kind of fine. It means we all shared bedrooms, had boring food, and our car would break down\u2026 Then you\u2019d get out and push the car. And I thought this was great cause I was the youngest of four kids, and everything was a game. My family doted on me, I had loads of friends, and everyone I knew was healthy. Overall, I\u2019d give my childhood a 10/10 for happiness, love and quality of life.</p><p>On the other hand, growing up, no one in my family had a degree. My entire town was blue collar jobs. And if you didn\u2019t want to go to school, you worked at one of the nearby factories. So when I graduated from high school and I wanted to apply to MIT, everyone told me that wasn\u2019t real. That was a thing people did on TV. Normal people couldn\u2019t do that. It\u2019s like becoming a Hollywood actor or an astronaut.</p><p>So I didn\u2019t apply.</p><p>Instead I went to the most prestigious college in the Netherlands, which was \u2026 fine?</p><p>That\u2019s when I hit a wall. This wall consisted of many parts not relevant to this story. But one part was the absolute horror of picking any career to work for 40 hours a week for 40 years.</p><p>That\u2019s what everyone did where I was from.</p><p>I could read books or the internet about people who did other things but I couldn\u2019t imagine how to be like them. And besides, all that stuff was not \u201creal\u201d. Manual labor was real. 9-5s were real. Picking a trade was real.</p><p>I wanted to throw myself off a bridge.</p><p>Instead, among other things, I developed my own set of life wisdoms focused on finding a path when I couldn\u2019t see any way to get where I wanted to go. There have been a lot of ups and downs since then, but in the first 10 years some wild stuff happened according to my small-town brain cause\u2026</p><ul><li>at 21, I was selected from 600 colleagues, becoming the second youngest Head Tester at Electronic Arts, managing teams of 4-16 testers.</li><li>at 24, I ran an experiment for the European Space Agency on Antarctica. Unrelatedly, I was also offered a lucrative monetization contract for my 3 month old Youtube channel.</li><li>At 25, I was offered a PhD and got my degree in Player Modeling in Video Games - a field I didn\u2019t know existed till I got the offer.</li><li>At 28, I found out an essay I had written for fun had become part of the curriculum at the MIT Media Lab.</li><li>At 30, I became senior data scientist at one of the largest video game publishers in the world (Square Enix West). Later that year I became head of their Analytics team.</li></ul><p>I\u2019m not saying I had a dream career or am the highest performer you know. Actually, I often don\u2019t know what I\u2019m doing and a lot of my plans fail.</p><p>Gosh, honestly. <i>Most</i> of my plans fail.</p><p>But I mostly get where I want to go and this is how I do it.</p><h1>Pick a direction instead of a goal</h1><p>When people think about where to go in life, they tend to pick a specific goal: become a doctor, make a decent amount of money, or have 2.3 kids. It\u2019s common for people without specific goals to feel like they are adrift and that it\u2019s hard to achieve much of anything without such a guiding star.</p><p>Or maybe I\u2019m projecting and that\u2019s just how I felt.</p><p>I sucked at picking goals. Kid me wanted to be a brick layer because I thought it would be satisfying to make neat little patterns. Adolescent me had an existential crisis about no job in particular sounding like anything I\u2019d want to do every day for the rest of my life.</p><p>So I developed the idea of picking a <i>direction</i> instead of a <i>goal</i>.</p><p>What does that mean?</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fac94b2bb-f5b7-437f-ba8e-321afffab82d_1456x557.jpeg\"><img alt=\"\" src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/uwmFSaDMprsFkpWet/gzb8ncbux2n0qmc2golm\" /></a></p><p>A <i>goal</i> is a specific outcome with specific steps at which you can fail or succeed. A <i>direction</i> is a decision heuristic that moves you closer to a wide-range of options that all contain increasing amounts of the Thing You Actually Want.</p><p>Wanting to \u201cbecome a doctor\u201d is a goal. Wanting to \u201cheal people\u201d is a direction.</p><p>To become a doctor, you need to go to med school, pass all your exams, secure a position at a hospital or start your own practice. Then you will have achieved the goal of \u201cbecoming a doctor\u201d. To heal people \u2026 you can do so many things. You could be a nurse or a medical technician. You could develop a new glucose pump for children with diabetes. You could research a cure for Corona, Ebola, or Pneumonia. You could raise awareness for Leukemia, Dyslexia, or Estonia. There are so many ways to help people get healthier, and if none of the existing ones suit you, you can literally try and invent a new way to contribute.</p><p>In essence, a direction let\u2019s you figure out where your exact person-shape fits into the world to generate a ton of \u201cheal people\u201d - the actual thing you care about.</p><p>But how do you figure out your direction?</p><p>Well, all I can say is how <i>I</i> did it.</p><p>My direction ended up being \u201ccontribute to the development of AAA video games\u201d.</p><p>Not that that was my first pick.</p><p>My first pick was \u201cbecoming a doctor\u201d (which, for the record, is a goal. But this essay makes the development of these heuristics seem more clean and chronological than what the actual messy reality was like).</p><p>So I signed up for this pre-med track where you get to follow around a nurse for two weeks. It was in the gastroenterology department of the local hospital. To the surprise of absolutely no one, I was exactly as bad around human suffering and copious amounts of internal fluids entering the external world as you\u2019d expect from someone with high disgust sensitivity and an overactive sense of emotional empathy. One lady needed sponges inserted 10 cm deep in her stomach wound. One man was afraid of needles and could be heard screaming from half the rooms on the floor as nurses spent 30 minutes attempting to insert an IV into his hand. None of this is unexpected, but actually seeing how you respond to it in reality is quite different trying to guess.</p><p>I responded by experiencing prodigious amounts of phantom pain and empathic resonance till my brain melted.</p><p>Till I met this elderly old lady there who was the sweetest thing and I asked her if I could do my 3-day case study on her, where I\u2019d interview her for an hour every day. She was the absolute pinnacle of grandmotherly wisdom and warmth. And in return, I kept her company. Her husband and child had tragically passed away in recent years. No one else came to visit. Our conversations were an oasis in our day. We held hands while we talked about her youth, her family, and the joys of life as she had found it. Her mind was sharp. Her eyes were clear. But when I came back on the third day, her bed was empty - she had passed away from liver cancer that night.</p><p>I went home, lay down in bed and cried.</p><p>I also concluded I wasn\u2019t cut out to be a doctor. But then I thought, maybe I could still be a surgeon instead. There was a second part to the pre-med track. A two week course at the morgue. Maybe that was my calling.</p><p>So I walked into the morgue where individual human body parts were sorted in IKEA-esque plastic containers: three intestines here, five livers there, and oh, an entire cadaver under that tarp.</p><p>I stood in shock.</p><p>For 40 minutes straight.</p><p>The cool thing of going in to shock around multiple doctors who have seen it all before is that they just let you stand there undisturbed. I think I didn\u2019t close my mouth or move a muscle the entire time I stood frozen in the middle of the morgue. I watched as one student studied a length of human intestines and remarked that the end bit was tremendously squishy and flexible. She then proceeded to sock puppet it in mock dialogue to the student next to her. They both burst out laughing before returning to their studies.</p><p>\u201cThese are born surgeons,\u201d I realized \u201cSome minds are made for certain tasks.\u201d</p><p>My mind on the other hand was made to be shuffled out of the morgue after an hour, handed a glass of water, and then plopped on my bike.</p><p>Suffice to say, I didn\u2019t become a surgeon either.</p><p>I tried other things though.</p><p>I tried teaching. It was great, but I dreaded going over roughly the same material year after year.</p><p>I studied economics, philosophy, math, biology, Spanish, anthropology, psychopathology, neuroscience, and art.</p><p>I looked into material engineering, industrial design, 3D art, writing, and charity work building schools in poor countries.</p><p>But in the end I picked video games, cause I kept <i>playing</i> video games.</p><p>Now playing games and developing games are two very different activities - I know. So I read through the entire employee list of various game companies and tried to imagine which role would suit me. Designer? Programmer? Artist? Sound Engineer?</p><p>I tried out each one asking myself which position would be my calling. I drafted design docs, poked game engines, sketched dream worlds, and discovered auditory dyslexia does not proffer a comparative advantage in sound engineering.</p><p>All these roles made for intimidating goals. Game developer positions are as sought after as Hollywood jobs. So I took a step back and asked myself if it really mattered <i>which </i>of the roles I ended up with. I realized the thing that mattered to me was \u201cwork on a cool game\u201d while \u201cfigure out how to do that\u201d could be a work in progress as I went along.</p><p>So I set the <i>direction</i> of getting any job in the video game industry contributing to the development of a AAA video game title. This was my 10 year project.</p><p>And 10 years later I got hired as a Data Scientist at Square Enix, where I analyzed data from Tomb Raider, Just Cause, Deus Ex, Life is Strange, and Hitman. My reports went to producers and designers, who then integrated the information in their decision process on how to develop these franchises further.</p><p>I didn\u2019t know that job existed when I started out. I didn\u2019t know I\u2019d be particularly suited for it either. And yet I made it. Using this <s>one simple trick</s> entirely convoluted set of heuristics that happen to work for my brain.</p><h1>Do what you love but always tie it back</h1><p>When I came up with all this stuff, I was very tired of fighting my brain. I was in college, absolutely amazing at forcing myself to do stuff I hated, and exactly the level of tortured you\u2019d expect to see as a result. Then I decided life just sucked too much that way, and I\u2019d rather do stuff I want to be doing and risk it being dumb than live out my life like that.</p><p>Honestly, I was a little bit grander about the whole thing. I realized I wanted to figure out how a Shoshannah-shaped brain can end up doing uniquely Shoshannah-shaped stuff, and basically explore the part of reality and possibility space that can only be explored by someone with my exact brain and circumstances.</p><p>This logic can apply to everyone. You are the only person who ever lived and will ever live who can explore what it\u2019s like to do stuff exactly <i>your</i> way, with <i>your</i> brain, and get the exact results <i>you</i> will get.</p><p>So I stared at my brain and noticed I had a bunch of <i>natural</i> motivations. Things I wanted to do to farm reward signals for myself - the motivational equivalents of a perpetuum mobile. What could I do with these?</p><p>Exploit them!</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff6640d3e-00d1-47cb-a02e-ce4c6efd30e8_1428x836.png\"><img alt=\"\" src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/uwmFSaDMprsFkpWet/nh9hocidkhbwlnsnqigg\" /></a></p><p>So imagine the red arc is your direction and the green ball is you. The arrow is your \u201ccompass\u201d and the orange balls are <s>distractions</s> your natural interests - all the things you find yourself doing if left to your own devices with not a care in the world. This is the stuff that intrinsically motivates you. The stuff that has you moving without effort. The stuff that seeps into every minute of every day if you let it.</p><p>Now here are mine:</p><p><a href=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feab5220d-d762-4a57-b5d4-4cd074ca2b81_1456x816.jpeg\"><img alt=\"\" src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/uwmFSaDMprsFkpWet/rcsu2ifwcbyn3qlgp732\" /></a></p><p>I love to socialize, to read, to write, to learn new things, to draw, work out, and logic hard about stuff. Honestly, I love to play around as well, especially in video games. But that was my direction already so that didn\u2019t count as a <s>distraction</s> competing interest I was looking to wrangle into my grand life plan.</p><p>So how does the actual wrangling work?</p><p>Simple - for every activity you naturally want to do, try to figure out a way to apply it to your direction. Your direction is sort of like a <i>theme -</i> if your direction is X and your interest is Y, then go do X-flavored Y-things.</p><p>I <i>suspect</i> this works for brains that are not mine. But either way, here are examples of how I did it:</p><ul><li><strong>Drawing</strong>: Buy game art books, study concept art and 3D modeling, watch making-of videos of video game artists, learn the basics of color theory and composition, explore visually artistic games.</li><li><strong>Working Out</strong>: Gamefy all my workouts, create my own rewards and counters, create physical games for my friends, play video games with workout elements, and imagine how to design games that truly capture the joy of various workouts.</li><li><strong>Logic</strong>: Learn programming, poke around in game engines, create diagrams of game design elements, play games that are logic-heavy, study game theory (not the same thing, but really much more relevant than people make it out to be!)</li><li><strong>Writing</strong>: Write a blog about the psychology of video games (that\u2019s the one that got picked up by the MIT Media Lab), chat a lot with people about games, write scripts for my youtube videos about games, put game reviews on reddit.</li><li><strong>Learning New Fields</strong>: For each new topic I studied I would reflect on how it can be related to video games. E.g., after a class on economics, I checked out the mechanics of Eve Online, when studying philosophy I looked into conceptual representation in video games, when exploring biology or chemistry I\u2019d try to think about how the same topics could be taught or simulated with games.</li><li><strong>Reading</strong>: Transform much of my media and literature diet into game-relevant material like Dungeons and Dragons books, isekai stories, video game blogs, books on video game design, and some of the fiction that many designers used as source material.</li><li><strong>Socializing/Organizing</strong>: Focus on expanding my social play by joining and then creating a clan, starting my own youtube channel, and later setting up a gaming data collection campaign (which unexpectedly turned in to my PhD).</li></ul><p>Was that enough to fulfill my 10-year plan of contributing to AAA video games?</p><p>Not entirely.</p><p>I still got stuck sometimes. When I couldn\u2019t come up with a project, or I really needed to find a job but nothing seemed relevant, or I was unhappy with the thing I was doing but didn\u2019t know what to do instead. I\u2019d look for advice, or try to reason through my problems. But sometimes that didn\u2019t work either.</p><h1>When all else fails, apply random search</h1><p>I first used random search as a college student because I was actually tremendously and obstructively <i>unhappy. </i>I had tried sleeping more, exercising frequently, eating healthier, meditating deeply, asking for help, getting a medical check up, spending more time with friends, doing volunteer work, soaking up more sunlight, and every other self-help, self-care, and self-improvement technique on the face of the earth.</p><p>And I still felt terrible.</p><p>So either there was no way to feel better, or the way to feel better lay in the space of actions I had never tried before.</p><p>And thus, I set out to try one new thing every day, for 30 days. The new thing could take 30 seconds, 3 minutes, or 3 hours. It didn\u2019t matter. All that mattered was that it was something I hadn\u2019t tried before. I would decide on the day what it would be - Sometimes it was a whim. Sometimes it was a project. All of it was new.</p><p>Things I tried included \u2026</p><ul><li>Eating a new fruit</li><li>Exploring a new part of the city</li><li>Teaching myself cartwheels</li><li>Reading about a new topic</li><li>Talking to a new person</li><li>Climbing a new tree</li><li>Drawing on my window</li><li>Writing a new fiction story</li><li>Trying a new figure drawing method</li><li>Attending my first yoga class</li><li>Buying balloons and randomly handing them out in the street</li></ul><p>Was I happier after those 30 days?</p><p>Yeah. Sort of. Not super happy. But a bit.</p><p>Some things worked. Some things I still do to this day. I apparently like eating bananas. I like climbing random objects. I like meeting new people. Mostly I discovered that <i>novelty in itself</i> actually makes me happier and is a nutrient I can become low on, and this can make me sad.</p><p>I think the random search did something else as well though: It gave me practice in just <i>doing</i> stuff, and specifically in doing <i>weird</i> stuff, and then also in noticing what the <i>genre</i> of weird stuff is that my brain likes to do.</p><p>But how did this help me move along in my life direction?</p><p>I think it loosened me up, increased the Overton window of what I felt was possible, and made me feel I was free to do things in the world that I wanted - Like sending in my CV for a Localization Tester job at Electronic Arts the next month. They offered me the job at the end of the interview\u2026 if I was willing to move to Spain within three weeks. Somehow that didn\u2019t seem stranger than randomly handing out balloons in the city cause I felt like it. So I said yes.</p><p>My guess is that random search is a way to get your brain unstuck. I think it\u2019s similar to how artists may doodle to warm up before a session. Or how people like to drink alcohol to disinhibit themselves. Except it applies to all of life, and gets you looking at the world with a question mark on your brain, wondering what tiny new thing you could do today.</p><p>Overall, these were mostly stories about my life, but the stories illustrate the lessons I wish I had learned a lot earlier and a lot faster. Specifically, if you don\u2019t know what to do with your life, consider picking a direction instead of a goal, then embrace your natural motivations, indulging them while bending them toward your life direction.</p><p>And when all else fails, apply random search.</p><br /><br /><a href=\"https://www.lesswrong.com/posts/uwmFSaDMprsFkpWet/explore-more-a-bag-of-tricks-to-keep-your-life-on-the-rails#comments\">Discuss</a>", "published": "Mon, 04 Nov 2024 19:04:14 GMT"}, {"title": "I got dysentery so you don\u2019t have to", "link": "https://www.lesswrong.com/posts/inHiHHGs6YqtvyeKp/i-got-dysentery-so-you-don-t-have-to", "summary": "Published on October 22, 2024 4:55 AM GMT<br /><br /><figure class=\"image\"><img alt=\"Drawing of the author in scrubs surrounded by a halo of bacteriophages and orbiting beakers, a cup, a paper form, pills, bacillus bacteria, and Infinite Jest.\" src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/inHiHHGs6YqtvyeKp/nljbfhsrlacin1ghigb3\" /></figure><p>This summer, I participated in a human challenge trial at the University of Maryland. I spent the days just prior to my 30th birthday sick with shigellosis.</p><h1>What? Why?</h1><p>Dysentery is an acute disease in which pathogens attack the intestine. It is most often caused by the bacteria <i>Shigella</i>. It spreads via the fecal-oral route. It requires an astonishingly low number of pathogens to make a person sick \u2013 so it spreads quickly, especially in bad hygienic conditions or anywhere water can get tainted with feces.</p><p>It kills about 70,000 people a year, 30,000 of whom are children under the age of 5. Almost all of these cases and deaths are among very poor people.</p><p>The primary mechanism by which dysentery kills people is dehydration. The person loses fluids to diarrhea and for whatever reason (lack of knowledge, energy, water, etc) cannot regain them sufficiently. Shigella bacteria are increasingly resistant to antibiotics. A disease easily treatable by lots of fluids and antibiotics is becoming more lethal.</p><p>Can someone do something?</p><h2>The deal with human challenge trials</h2><p>Clinical trials in general are expensive to run but pretty common; clinical trials <i>where you are given the disease </i>\u2013 \u201cchallenged\u201d, AKA \u201chuman challenge trials\u201d \u2013 are very rare. The regular way to investigate a possible treatment is to make a study plan, then find people who have the disease and offer to enroll them in the experimental treatment. Challenge trials are less common, but often more valuable for research \u2013 shigellosis is a fast-acting disease that is imminently treatable by antibiotics and uncommon in the US. It would be very difficult to test an alternative shigellosis treatment in the US in the conventional way, but it\u2019s a great candidate for challenge trials.</p><p>I\u2019d signed up for email alerts on upcoming challenge trials at the nearby University of Maryland, and got one about an upcoming study. It caught my eye that it was for a phage-based treatment. Bacteriophages are really promising antibacterial medicines, not to mention what I\u2019d studied as an undergrad.</p><p>Here\u2019s the thing: you really only get good medical research out of human subjects. Also, I could use $4000 and this seemed like a cool way to spend a couple weeks and help out medical research. So I signed up, got a check-in general health appointment, and shortly after, was told I was in. I made plans to spend my 30<sup>th</sup> birthday in a dysentery ward.</p><h2><strong>Dysentery: it\u2019s a modern disease</strong></h2><p>Many of you reading this will know about dysentery from the 1971 simulation game The Oregon Trail (or its later versions). The actual Oregon Trail was a network of trails and the corresponding migration of mostly-white pioneers, moving on foot and on ox-drawn wagon from the eastern US to the western US between 1830 and 1869. About 400,000 people* crossed the Oregon Trail in this period, and a lot of them were on similar trails \u2013 a bunch of stressed and malnourished people, traveling in close quarters with their families, stopping and pooping near the same trails and creeks with no regard for water safety \u2013 diseases spread <i>very </i>fast in these conditions. From these and other stressors, about 65,000 people died in this 40-year period.</p><p>Stated another way, more people die from dysentery <i>now, every year </i>than <i>ever died from any cause</i> on the Oregon Trail. So let\u2019s calm down about the Oregon Trail, okay?</p><p><i>*Lots of people use this 400,000 number but I can\u2019t figure out where it came from and if this is referring to individuals or families \u2013 I\u2019ve seen sources indicate it was either. If it was families, it was probably counting the men who were \u201cthe pioneers\u201d and then being like \u201coh and there were women and kids there also, I guess.\u201d But maybe it was individuals? Or maybe someone just made this up? Again, no idea where it came from. You gotta be careful every time anyone tells you a number. It\u2019s so bad out there. The only thing worse than someone telling you a number is when they don\u2019t tell you a number.</i></p><h1><strong>Getting ready</strong></h1><p>A week or so before I went, I\u2019d been pointed to<a href=\"https://x.com/wokeglobaltimes/status/1510768591012085768\"> Jake Eberts\u2019s twitter thread</a>. Jake Eberts also participated in a challenge trial for a dysentery vaccine, also I think at UMD and the same Baltimore facility I was at, where he got very sick and<a href=\"https://www.buzzfeednews.com/article/paigeskinner/man-twitter-dysentery-trial\"> went viral</a> for livetweeting the experience. He started a fundraiser for dysentery relief and got a lot of people to sign up for clinical trials themselves, and now he works for<a href=\"https://www.1daysooner.org/\"> 1DaySooner</a>, premier \u201chey, human challenge trials are cool\u201d advocates.</p><p>I read his twitter thread and sent my friends this meme:</p><p><img alt=\"(chuckles) I'm in danger.\" src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/inHiHHGs6YqtvyeKp/nqx5ptavn5ihobdwyovu\" /></p><p>I brought <i>Infinite Jest</i>, which I was partway through and was a lot more through (but still not done) by the time I was discharged. (I\u2019m writing this while traveling, and in a fit of poor timing I finally finished it on the plane ride in, which means I now have a giant brick of a book to carry around in my suitcase.) My friend Ozy said that <i>Infinite Jest</i> was a really good book for reading in a dysentery ward.</p><p>I thought, oh, that\u2019s interesting, you know, a lot of the characters <i>are </i>pretty miserable and living in a controlling institution of some kind. Then I remembered this one passage, where circumstances have forced a character into withdrawing from heroin alone, holed up for days in a public bathroom:</p><blockquote><p>Time began to pass with sharp edges. Its passage in the dark or dim-lit stall was like time being carried by a procession of ants, a gleaming red martial column of those militaristic red Southern-U.S. ants that build hideous tall boiling hills, and each vile gleaming ant wanted a minuscule little portion of Poor Tony\u2019s flesh in compensation as it helped bear time slowly forward down the corridor of true Withdrawal. By the second week in the stall time itself seemed the corridor, lightless at either end. After more time time then ceased to move to be moved or be move-throughable and assumed a shape above and apart, a huge, must-feathered, orange-eyed wingless fowl hunched incontinent atop the stall, with a kind of watchful but deeply uncaring personality that didn\u2019t seem keen on Poor Tony Krause as a person at all, or to wish him well. Not one little bit. It spoke to him from atop the stall, the same things, over and over. They were unrepeatable. Nothing in even Poor Tony\u2019s grim life-experience prepared him for the experience of time with a shape and an odor, squatting; and the worsening physical symptoms were a spree at Bonwit\u2019s compared to time\u2019s black assurances that the symptoms were merely hints, signposts pointing up at a larger, far more dire set of Withdrawal phenomena that hung just overhead by a string that unraveled steadily with the passage of time. It would not keep still and would not end; it changed shape and smell.</p></blockquote><p>I was forced to agree that <i>Infinite Jest </i>was indeed probably a pretty good choice.</p><h1><strong>Two days until challenge</strong></h1><p>Checking in, everyone\u2019s bags were checked. I got the impression they really didn\u2019t want some kind of bad outcome where they had to call cops into a ward where everyone was running around with the bloody flux, which, fair enough. They did take away my craft scissors. I didn\u2019t end up knitting so it wasn\u2019t a big deal but like I\u2019m pretty sure I\u2019ve taken those on airplanes before. Okay.&nbsp;</p><p>We were assigned a number (I was just on this side of divinity at No. 107), given a plastic wristband, and shown to our rooms. We were also given two pairs of scrubs which were to be our main clothes on the ward \u2013 less risk of ruining hard-to-launder clothes in the more messy phases of the study \u2013 though it did mean 15 people having to coordinate laundry every day.</p><figure class=\"image image_resized\" style=\"width: 413px;\"><img alt=\"My hospital bed with folded scrubs atop, a cup of coffee, and copy of Infinite Jest set on the adjustable bedside table.\" src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/inHiHHGs6YqtvyeKp/kec3xlhrewimyjvcxcxb\" /><figcaption>Where I made my stand</figcaption></figure><p>The ward was more of a retrofitted office building than a hospital. It consisted of some spaces for nurses and testing, about 6 bedrooms of various sizes (each with their own half-bath), two separate areas with two shower stalls each, a \u201ckitchen\u201d with snacks and where the meals were delivered to, a closet with washer and dryer, and a rec room with couches and a TV and a pool and foosball table.</p><p>There were about 16 people on the ward, an even mix of men and women. Most of them were Baltimore locals; many of them had done other trials before. We were fully allowed to socialize \u2013 dysentery is, again, infectious through the fecal-oral route, hand sanitizer was stationed all over the place but there wasn\u2019t a huge concern that we\u2019d infect each other or even the nurses.</p><p>Life on the ward is very chill. I was worried about being bored, but I\u2019d forgotten that I spend most of my waking hours on the computer anyway, so it really wasn\u2019t a problem. When even my iron gaze faltered and couldn\u2019t stare at the computer anymore, I read Infinite Jest.</p><p>Meals were delivered once a day \u2013 one cold usually wrap- or sandwich-based meal, one hot breakfast, one hot supper dish, labelled with people\u2019s numbers.</p><figure class=\"image image_resized\" style=\"width: 534px;\"><img alt=\"Sample lunch: a sandwich, salad, and roll in plastic packaging, plus a bottle of water.\" src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/inHiHHGs6YqtvyeKp/peeof5iqivoo2q0uf1jb\" /><figcaption>Sample lunch</figcaption></figure><p>They were, like, fine. The caterers made a few interesting choices \u2013 for vegetarians such as myself, every sandwich/wrap was some veggies with hummus, and now and then there\u2019d be like breakfast pancakes with a curry-flavored veggie hamburger patty. I would describe the flavor when drenched with table syrup as \u201cweird.\u201d</p><p>Like, you can <i>tell </i>the person planning that menu was like \u201cokay, pancakes and bacon\u2026 And wait, crap, something with protein for the vegetarians.\u201d But again, I\u2019ve eaten worse for things I\u2019ve actually paid for ingredients for, and I was definitely eating better in terms of variety and volume than I did at home. I\u2019m not complaining.</p><h1>One day before challenge: the age of phage</h1><p>This study was sort of an over-time test \u2013 ideally the first of a few, where we\u2019d get phages before (unless we were in the control group), during, and after the \u201cchallenge\u201d (the shigella) to see if they had any effect at all \u2013 if it did, later studies could determine if you could just drink the phage after getting sick, or if it would work best as a prophylactic, or etc. We drank a chalky buffer solution to neutralize stomach acid and give the bacteriophages (and later, the bacteria) a better chance at making it to the intestine.</p><p>What do the solutions taste like? Basically all salty fluid with slight mineral nuance, from the buffer. Phages are known to be pretty tasteless so I didn\u2019t expect anything else.</p><h2>Bacteriophage therapy: sending a cat after mice</h2><p>A bacteriophage is a virus that infects bacteria. They were discovered shortly after bacteria themselves were really pinned down \u2013 microscopes were finally powerful enough to make out bacteria, and visionaries like Robert Koch and Louis Pasteur were pinpointing that these little nothing-pinpricks were in fact the source of diseases. (For more on the discovery of the microbial world, see<a href=\"https://asteriskmag.com/issues/07/through-the-looking-glass\"> \u201cThrough the Looking Glass and what Zheludev Et Al. (2024) Found There\u201d</a>, my recent piece in Asterisk Magazine.)</p><p>In 1917, F\u00e9lix d\u2019H\u00e9relle found an agent that killed cholera bacteria, which passed through a fine filter, and which could reproduce \u2013 a living agent that killed bacteria, but that was itself smaller than a bacteria.</p><p>d\u2019H\u00e9relle realized right away this substance which killed bacteria, and which people had apparently been drinking, had potential as medicine. He bred pathogenic bacteria in vats and added solutions, and waited until the cloudy brother of bacteria turned clear \u2013 then offered this liquid to sick patients. Many of them, sure enough, recovered. I was (unless I was in the control group) walking in historical footsteps. Dysentery was the first human disease ever treated with phage medicine.</p><p>Sending a phage after bacteria is like sending a cat after mice. Phages are small, targeted, well-adapted hunters of specific bacteria. There is no way for them to infect a human cell like a human virus would \u2013 they are completely specialized. Phages are already in the body, along with their bacterial hosts \u2013 so you\u2019re not introducing a radically new agent \u2013 and the immune system tends to play well with them.</p><p>Phage are used widely in some parts of the world \u2013 the Republic of Georgia and Poland both sell phage over-the-counter, for use in say intestinal conditions or wounds, and have<a href=\"https://www.phagetherapycenter.com/\"> clinics for personalized treatment</a>. In the US, phage therapy is an extremely rare specialty, sometimes even falling under the umbrella of naturopathy. (A phage being a natural bioactive product.)</p><p>Why would you use antibiotics instead of phages, or vice versa?</p><figure class=\"table\"><table><tbody><tr><td>Phages</td><td>Antibiotics</td></tr><tr><td>Targeted \u2013 a phage attacks one species or one strain of bacteria&nbsp;<br />Easy to find usable new ones&nbsp;<br />More finnicky (e.g. less stable)&nbsp;<br />Predator-prey pharmacokinetics&nbsp;<br /><br />Mostly spread where the bacteria are<br />Very few side effects</td><td>Broad-spectrum<br /><br />Hard to find usable new ones&nbsp;<br />Shelf stable&nbsp;<br />Regular blood-elimination-curve pharmacokinetics&nbsp;<br />Systemic; enter the bloodstream&nbsp;<br />Sometimes-serious side effects</td></tr></tbody></table></figure><p>What if the bacteria become resistant to the phages too?</p><p>Well, that can happen easily \u2013 probably even easier than with antibiotics. Cells have been duking it out with viruses since the beginning of life. (Did you know CRISPR-Cas9, now used for gene editing, evolved in nature as a way for bacteria to recognize and cut up phage DNA?)</p><p>But the difference is that whereas new antibiotics are very hard to find, there is a nigh-inexhaustible evolutionary font of phages constantly pulling ahead in the arms race. So in short: once a bacteria becomes resistant to your special phage, just find a new phage.</p><h2><strong>Do they work?</strong></h2><p>To my knowledge, there aren\u2019t any really gold-standard reviews comparing phages head-on to antibiotics. They are fiddlier than antibiotics, with a specialized body of knowledge for treatment \u2013 less stable, have to be introduced to the site directly, much more care in choosing an appropriate treatment.</p><p>One small study found a phage treatment comparably effective to antibiotics for <i>Salmonella typhimirium </i>in 36 lab mice.<span class=\"footnote-reference\" id=\"fnrefinooo6g4sj\"><sup><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fninooo6g4sj\">[1]</a></sup></span>&nbsp;Another meta-study compared modern antibiotic studies to 17 studies from the last time human phage research was in vogue in the US, the 1920s-40s, and found that phages were effective treatments \u2013 but 4 modern clinical trials suggested phages were not effective.<span class=\"footnote-reference\" id=\"fnreftn2v19155f\"><sup><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fntn2v19155f\">[2]</a></sup></span>&nbsp;A more recent study of personalized phage therapy showed promising results in infections considered \u201cdifficult-to-treat\u201d.<span class=\"footnote-reference\" id=\"fnrefre5vhv0osvk\"><sup><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fnre5vhv0osvk\">[3]</a></sup></span>&nbsp;They seem to work best when used <i>with </i>antibiotics.</p><p>I\u2019m not doing a full lit review right now. I bet that phage therapy still has promise \u2013 more careful formulations and just more research will help. That\u2019s before challenges of commercial rollout, including things like handling FDA approval for a product that must be reformulated regularly.</p><p>The elephant in the room is antibiotic resistance \u2013 antibiotics usually work extremely well, but increasingly, bacteria can survive them. Antibiotic resistance is, unlike other diseases you might think of that are exacerbated by over-medication, <i>not </i>a condition of privileged countries \u2013 lots of <i>Shigella </i>bacteria in developing countries are increasingly antibiotic-resistant.</p><p>Even if phages don\u2019t work as well as the magic silver bullet that is antibiotics, they might work well enough to be worth incorporating into our medical toolbox as part of AMR management. And that means developing them now.</p><p>The other challenge is of course regulatory \u2013 I\u2019m excited that <a href=\"https://www.intralytix.com/\">Intralytix</a>, who made the experimental product I did-or-didn\u2019t take, is throwing their hat into the space of human phage medicine, and to see how they handle this.</p><h1><strong>Day 1 of challenge</strong></h1><p>On the third day in the ward after a day of baseline and a day of phage (unless we were in the control group), we took another dose of phage (unless we were in the control group), waited a couple of hours, and then drank a glass of shigella. This tasted like baking soda and salt with no particular nuance, nor would I expect nuance; the dose was some 1300 organisms \u2013 as in 1300 individual cells of bacteria, count \u2018em. A preposterously scant microbial innoculum, even for devoted parasites it often takes on the order of millions of organisms to lodge an infection \u2013 but shigella is remarkably tenacious. It would only have taken 10-200. This was overkill \u2013 a dose that WILL make you sick, unless you\u2019re protected. All the participants drank.</p><h2>The waiting game</h2><p>Shigella has a 24-72 hour incubation period, maybe 12-96 hours on the far ends.</p><p>Perhaps owing to quirks of my own psyche, whose origins I\u2019m sure we don\u2019t need to explore here, I find it reassuring to have reference experiences to conveniently benchmark the rest of my life by. If you go skiing, you can ask yourself later, \u201cis this more or less exhilarating than skiing?\u201d If you fall in love once, you can compare future loves to that earlier experience.</p><p>A good standard reference point for \u201cshared, resigned dread\u201d is the 72 or so hours in a clinical trial ward after everyone has ingested shigella bacteria along with maybe-a-treatment.</p><p>The vibes were ominous. Jovially nervous. Unprecedented gastrointestinal distress may or may not have been coming for me, but if it is, it would be arriving in (on average) 48 hours.</p><p>The floor was pretty quiet. The hours ticked by.</p><p>Infinite Jest is, by the way, a great book. David Foster Wallace knew how to write a goddamn sentence on purpose.</p><h1><strong>Let\u2019s learn about </strong><i><strong>Shigella </strong></i><strong>pathogenesis</strong></h1><p>While I waited, I decided to read up. Shigella bacteria invades the body via the digestive canal, and infects the intestines \u2013 both small and large. It releases a toxin that facilitates its infection of other parts of the intestine and its eventual replication. It\u2019s an intracellular pathogen \u2013 some bacteria, like all viruses, actually enter the host\u2019s cell and replicate inside there.</p><p>Shigella actually prefers to invade the <i>outside </i>(or should I say the inside?) of intestinal cells. But the body is a locked-down system with its own guard force, the immune system, keeping the dirty external environment separate from the sterile inside environment. Shigella in the digestive tract really wants to poke through that line of intestinal cells and get at them from the other side.</p><p><img alt=\"How does Shigella get to the outside of the intestinal cell layer?\" src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/inHiHHGs6YqtvyeKp/gnwf10xvxzucu4ujtujw\" /></p><p><a href=\"https://web.archive.org/web/20241005215039/https://en.wikipedia.org/wiki/Shigella_flexneri\">Wikipedia explains:</a></p><blockquote><p>Once inside of the colon, <i>S. flexneri</i> can penetrate the epithelium in three ways:&nbsp;<br />1) The bacterium can alter the tight junctions between the epithelial cells, allowing it to cross into the sub-mucosa.&nbsp;<br />2) It can penetrate the highly endocytic<a href=\"https://web.archive.org/web/20241005215039/https://en.wikipedia.org/wiki/Microfold_cell\"> M cells</a> that are dispersed in the epithelial layer and cross into the sub-mucosa.&nbsp;<br />3) After reaching the sub-mucosa, the bacteria can be phagocytosed by macrophages and induce apoptosis, cell death. This releases<a href=\"https://web.archive.org/web/20241005215039/https://en.wikipedia.org/wiki/Cytokine\"> cytokines</a> that recruit<a href=\"https://web.archive.org/web/20241005215039/https://en.wikipedia.org/wiki/Neutrophil\"> polymorphonuclear cells (PMN)</a> to the sub-mucosa. <i>S. flexneri</i> still in the lumen of the colon traverse the epithelial lining as the PMNs cross into the infected area. The influx of PMN cells across the epithelial layer in response to Shigella disrupts the integrity of the epithelium allowing lumenal bacteria to cross into the sub-mucosa in an M-cell independent mechanism.</p></blockquote><p>This is really funny. Okay, imagine there\u2019s a blockade of tightly parked police cars facing you and you and your buddies need to go get to their trunks so you can hide in them. Here are 3 ways to do this:</p><ol><li>Push the police cars to the side so you can walk between them</li><li>Look for the police cars with the biggest doors, so that you can squeeze through the car and leave through their trunk (or I guess probably just stay in the trunk at that point)</li><li>Get yourself and your buddies arrested, then when they send backup police vans to push through the police to arrest all of you, run through the cracks in the blockade that those vans open up. Then go to the trunks of the original cop cars.</li></ol><p>And then once you\u2019re inside the car, you can open the doors between the cop cars (they\u2019re sliding doors) and then travel laterally between the cop cars. I love cells.</p><hr /><p>As a fun side note, <i>Shigella \u2013</i> including the strain I was developing an intimate relationship with,<i> Shigella flexneri \u2013 </i>is, taxonomically speaking, a kind of <i>Escherichia coli.</i> Now you may notice from the scientific nomenclature that this is not how this is supposed to work.</p><p>When genotyping was developed and applied to some familiar standby kinds of bacteria that microbiology-as-science figured it understood pretty well, researchers learned two surprising new things:</p><ul><li><i>E. coli</i> is not a coherent species. Different strains of <i>E. coli</i> \u2013 known to have slightly different properties, but thought to be all slight variations on the same basic species \u2013 turned out to have only <a href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC2974192/\">20% of their genes</a> in common. (Humans and our closest relatives, chimpanzees, have almost all of our genes in common* and still aren\u2019t considered as the same <i>genus</i>.)</li><li><i>Shigella </i>is in that umbrella of shared genes \u2013 a secret family member known as a <a href=\"https://en.wikipedia.org/wiki/Taxon_in_disguise\">taxon in disguise</a>. It\u2019s more similar to many <i>E. colis </i>than some <i>E. coli</i>s.</li></ul><p>For most species, the procedure at this point would be to throw in the towel and reclassify \u2013 <i>Escherichia coli spp. shigella</i>, perhaps. But in this case, shigatoxin-producing <i>Shigella </i>and other pathogenic <i>Escherichia coli </i>have different enough clinical presentations that the distinction is still medically valuable, so accurate nomenclature has bowed its head to practicality. Cool! (Compare and contrast with <a href=\"https://eukaryotewritesblog.com/2021/05/02/theres-no-such-thing-as-a-tree/\">trees</a>.)</p><p>*Wait, don\u2019t people talk about 99% or something? That number is actually about <a href=\"https://www.broadinstitute.org/news/comparison-human-and-chimpanzee-genomes-reveals-striking-similarities-and-differences\"><i>sequence similarity</i></a><i> </i>and not related genes \u2013 if we have 96% sequence similarity, meaning the exact same genetic code, probably even more of that genome is still <i>in related genes. </i>Genes can code for clearly related proteins/sequences and still not be identical, like they came from a common ancestor and haven\u2019t diverged much but have picked up a few changes along the way. Different<i> E. coli</i> have 80% completely different genes \u2013 a human has maaaybe 50 genes that a chimp doesn\u2019t? I didn\u2019t try very hard to find the actual similar metric between them. It\u2019s what I was telling you about numbers. You gotta watch out.</p><h1><strong>Let\u2019s really learn about Shigella pathogenesis</strong></h1><p>Some 24 hours in, the first people started going down. Via word of mouth I heard the phrase \u201cExorcist-style projectile vomiting\u201d used to describe someone in the next room over, a description whose accuracy I fortunately cannot verify. Most people were in their rooms all day anyhow, but the crowd in the kitchen at mealtimes or showing up for morning dosing got thinner.</p><p>I really held out. Going to bed at end of the second night, I felt okay, but couldn\u2019t sleep well \u2013 nerves, I thought, or the faint distorted unpleasant bodily noises from other parts of the ward. I maybe managed a couple hours of sleep by the wee hours.</p><p>48 hours in, I woke up for vitals and dosing at 6 AM and started feeling really faint on the short walk to the next room. I stumbled over to the toilet. Off to the races!</p><p>I should be clear in this section that I was in as close to zero long-term danger as you can get with dysentery, which is damn close \u2013 this was in a controlled setting with doctors and nurses, monitoring my condition regularly, with a known pathogen with a known cure. In this case, we weren\u2019t expected to languish in indefinite misery \u2013 they wanted to see if we got sick and then how sick we got, yes, but only up to a point, at which point they would \u201ccall it\u201d \u2013 administer regular antibiotics and end our experimental treatment.</p><p>All I had to do was let the time pass.</p><p>The next few hours were very bad. Surprisingly, the gastrointestinal symptoms were not much of a problem for me \u2013 I had them, but it wasn\u2019t much worse than those of regular food poisoning. I didn\u2019t throw up. I just wanted to go back to sleep.</p><p>But sleep wasn\u2019t coming.</p><p>First was the plague of chills. The institutional cotton blankets did nothing; four of them also did nothing, as if there was no heat to hold in. Freezing, tooth-clattering cold.</p><p>Within an hour came the plague of joint pain. It sank in rather quickly and was all in the lower extremities \u2013 hips, legs. Any more than one blanket became too heavy to bear having on them, so off they go, freezing cold but they weren\u2019t palpably doing anything anyway. Right? I remembered reading people with chronic pain reporting that sometimes laying down was worse than other positions, and sure enough sitting up was \u2013 somehow \u2013 mildly better. I situated the adjustable bedside table so that I could slump onto it and maybe even sleep like that, but sleep remained out of reach.</p><p>Time wasn\u2019t shitting so much as dragging, by the bones, over rough pavement, every second another six inches, grating, relentless, second after second after second. Time is space in which you are moved forward one way or another. Pain is an active process.&nbsp;</p><p>Around three hours later, the doctor came in and judged that I was done \u2013 they were calling it \u2013 symptomatically I had reached the Clinical Endpoint and would be treated. I was handed tylenol and antibiotics.&nbsp;</p><p>I\u2019d always thought of tylenol as sort of a second-rate painkiller, probably worth trying if you couldn\u2019t find ibuprofen, but damn if that tylenol didn\u2019t work pretty quickly. As soon as I could I went to sleep for like four hours \u2013 which, as usual, if you are in a position of needing four hours of sleep, makes a lot of things better and more manageable once you can swing it.</p><h1><strong>Out the other side</strong></h1><p>The antibiotics worked <i>really</i> quickly. Within hours, the fever had vanished and the aches had dwindled to twinges. Within a couple days, even the gastrointestinal situation was back to normal. Other people were harder hit, other people were just starting to get sick \u2013 staying vanished in their rooms even after I stuck my head into the kitchen and rec rooms like the first hopeful groundhog of spring \u2013 and many had been fine the whole time.</p><p>The thing that kills people in dysentery is dehydration and complications thereof. So part of the recovery is collecting and measuring how many fluids were emitted, and then re-administering oral rehydration fluid \u2013 a salty liquid served ice-cold \u2013 in precise ratios to replace the bodily fluids lost. A human is a series of tubes with attached nervous system and fortunately I was in the company of master plumbers. Once the diarrhea had stopped, I was also able to stop guzzling big plastic cups of what I liked to imagine tasted like arctic seawater. Progress!</p><figure class=\"image image_resized\" style=\"width: 453px;\"><img alt=\"Breakfast - french toast in a plastic container and a cup of coffee - illuminated in golden morning light, at a table with a nice view out into the city.\" src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/inHiHHGs6YqtvyeKp/wuakw5je9yziwcjqfgat\" /><figcaption>Great view from the rec room.</figcaption></figure><p>People who recovered and who never got sick started hanging out in the rec room more, chatting and playing pool. I spent my birthday calling my parents and talking to internet friends. One streamed himself playing a fish-themed video game in my honor. The Baltimoreans inexplicably set off fireworks many nights \u2013 maybe the proximity to July 4<sup>th</sup>? \u2013 and this was one of them. Not roadside-stand-ground fireworks, but big aerial fireworks. A fellow subject found ice cream bars in the kitchen freezer and kindly brought me one as a present. Fireworks aside, it was a quiet day.</p><p>Apologies for the deception, reader. Technically speaking, the word \u201cdysentery\u201d usually refers to a syndrome, like \u201cpsychosis\u201d or \u201chigh blood pressure\u201d, which can have multiple causes but which is defined by specific symptoms. The specific symptom of dysentery is bloody diarrhea. I personally did not get this particular symptom \u2013 I became sick with <i>shigellosis </i>but, according to a common criteria, did not get <i>dysentery</i>. I\u2019m sorry for clickbaiting you. In my defense, I would have taken it over the joint pain.</p><h1><strong>Aftermath</strong></h1><p>Twice a day after antibiotics, we gave the nurses a stool sample \u2013 these were sampled and cultured at some lab to determine if shigella was still in there. Two negative samples in a row meant that we were free to go.</p><p>9 days after coming in, I was cleared for release. I collected my scissors, and, free of dysentery, was released onto the streets of Baltimore. A year older on paper. Healthy, wrung out, ready for time to keep doing what it does. Hopefully, mostly on kinder terms.</p><figure class=\"image image_resized\" style=\"width: 487px;\"><img alt=\"View of Baltimore out a train window.\" src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/inHiHHGs6YqtvyeKp/iznln3tz4frsnoq9zsbe\" /><figcaption>The train ride home. I see that 75% of these photos have coffee in them. What can I say? I\u2019m from Seattle.</figcaption></figure><p>I think that despite my relatively mild case, that I was in the control group. But the reason I think that was because in the whole trial, everyone drank the shigella, and it sure seemed like about half of them didn\u2019t get sick at all.</p><p>Pretty goddamn cool, if you ask me.</p><p>If you want to have study rigor performed on your body, you can look for clinical trials at<a href=\"https://clinicaltrials.gov/\"> clinicaltrials.gov</a>. 1DaySooner advocates for human challenge trials; they have<a href=\"https://www.1daysooner.org/recruiting-challenge-studies/\"> a list of challenge trials that are actively recruiting</a> and you can also<a href=\"https://www.1daysooner.org/volunteer/\"> sign up for email alerts</a>. Many of them pay money. Consider checking it out.</p><hr /><p>Thank you Grace Neptune, Kelardry, and YumAntimatter for reviewing a draft of this post.</p><p>I have <a href=\"https://www.patreon.com/EukaryoteWritesBlog/\">a Patreon</a>! Consider supporting my writing by throwing me a few bucks. I\u2019d really appreciate it. I won\u2019t be getting dysentery again (\u2026on purpose) but I have some other good stuff in the works.</p><p><i>Posted on [</i><a href=\"https://eukaryotewritesblog.com/2024/10/21/i-got-dysentery-so-you-dont-have-to/\"><i>Eukaryote Writes Blog</i></a><i> - </i><a href=\"https://eukaryotewritesblog.substack.com/p/i-got-dysentery\"><i>Substack</i></a><i> - </i><a href=\"https://www.lesswrong.com/posts/inHiHHGs6YqtvyeKp/i-got-dysentery-so-you-don-t-have-to\"><i>Lesswrong</i></a><i> - </i><a href=\"https://forum.effectivealtruism.org/posts/gjSCet5fyjdkyHSsw/i-got-dysentery-so-you-don-t-have-to\"><i>EA Forum</i></a><i>]</i></p><ol class=\"footnote-section footnotes\"><li class=\"footnote-item\" id=\"fninooo6g4sj\"><span class=\"footnote-back-link\"><sup><strong><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fnrefinooo6g4sj\">^</a></strong></sup></span><div class=\"footnote-content\"><p>R. a. N. Acebes et al., \u201cComparing the Efficacy of Bacteriophages and Antibiotics in Treating Salmonella Enteric Serovar Typhimurium on Streptomycin-Pretreated Mice,\u201d <i>Philippine Journal of Science (Philippines)</i> 150, no. 6a (2021), <a href=\"https://agris.fao.org/search/en/providers/122430/records/6474afaca3fd11e430380e4f\">https://agris.fao.org/search/en/providers/122430/records/6474afaca3fd11e430380e4f</a>.</p></div></li><li class=\"footnote-item\" id=\"fntn2v19155f\"><span class=\"footnote-back-link\"><sup><strong><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fnreftn2v19155f\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Luigi Marongiu et al., \u201cReassessment of Historical Clinical Trials Supports the Effectiveness of Phage Therapy,\u201d <i>Clinical Microbiology Reviews</i> 35, no. 4 (September 7, 2022): e00062, <a href=\"https://doi.org/10.1128/cmr.00062-22\">https://doi.org/10.1128/cmr.00062-22</a>.</p></div></li><li class=\"footnote-item\" id=\"fnre5vhv0osvk\"><span class=\"footnote-back-link\"><sup><strong><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fnrefre5vhv0osvk\">^</a></strong></sup></span><div class=\"footnote-content\"><p>Jean-Paul Pirnay et al., \u201cPersonalized Bacteriophage Therapy Outcomes for 100 Consecutive Cases: A Multicentre, Multinational, Retrospective Observational Study,\u201d <i>Nature Microbiology</i> 9, no. 6 (June 2024): 1434\u201353, <a href=\"https://doi.org/10.1038/s41564-024-01705-x\">https://doi.org/10.1038/s41564-024-01705-x</a>.</p></div></li></ol><br /><br /><a href=\"https://www.lesswrong.com/posts/inHiHHGs6YqtvyeKp/i-got-dysentery-so-you-don-t-have-to#comments\">Discuss</a>", "published": "Thu, 31 Oct 2024 18:47:32 GMT"}, {"title": "A bird's eye view of ARC's research", "link": "https://www.lesswrong.com/posts/ztokaf9harKTmRcn4/a-bird-s-eye-view-of-arc-s-research", "summary": "Published on October 23, 2024 3:50 PM GMT<br /><br /><p><em>This post includes a \"flattened version\" of an interactive diagram that cannot be displayed on this site. I recommend reading the original version of the post with the interactive diagram, which can be found <a href=\"https://www.alignment.org/blog/a-birds-eye-view-of-arcs-research\">here</a>.</em></p>\n<p>Over the last few months, ARC has released a number of pieces of research. While some of these can be independently motivated, there is also a more unified research vision behind them. The purpose of this post is to try to convey some of that vision and how our individual pieces of research fit into it.</p>\n<p><em>Thanks to Ryan Greenblatt, Victor Lecomte, Eric Neyman, Jeff Wu and Mark Xu for helpful comments.</em></p>\n<h2 id=\"a-birds-eye-view\">A bird's eye view</h2>\n<p>To begin, we will take a \"bird's eye\" view of ARC's research.<sup class=\"footnote-ref\"><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fn1\" id=\"fnref1\">[1]</a></sup> As we \"zoom in\", more nodes will become visible and we will explain the new nodes.</p>\n<p><em>An interactive version of the diagrams below can be found <a href=\"https://www.alignment.org/blog/a-birds-eye-view-of-arcs-research\">here</a>.</em></p>\n<h3 id=\"zoom-level-1\">Zoom level 1</h3>\n<p><img alt=\"birds_eye_lvl1.svg\" src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ztokaf9harKTmRcn4/vgdmqmbjhnr9eyfvswek\" /></p>\n<p>At the most zoomed-out level, ARC is working on the problem of \"<a href=\"https://ai-alignment.com/clarifying-ai-alignment-cec47cd69dd6\">intent alignment</a>\": how to design AI systems that are trying to do what their operators want. While many practitioners are taking an iterative approach to this problem, there are <a href=\"https://arxiv.org/abs/2307.15217\">foreseeable ways</a> in which today's leading approaches could fail to scale to more intelligent AI systems, which could have <a href=\"https://www.cold-takes.com/without-specific-countermeasures-the-easiest-path-to-transformative-ai-likely-leads-to-ai-takeover/\">undesirable consequences</a>. ARC is attempting to develop algorithms that have a better chance of scaling gracefully to future AI systems, hence the term \"<strong>scalable alignment</strong>\".</p>\n<p>ARC's particular approach to scalable alignment is a \"builder-breaker\" methodology (described in more detail <a href=\"https://ai-alignment.com/my-research-methodology-b94f2751cb2c\">here</a>, and exemplified in the <a href=\"https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit\">ELK report</a>). Roughly speaking, if the scalability of an algorithm depends on unknown empirical contingencies (such as how advanced AI systems generalize), then we try to make worst-case assumptions instead of attempting to extrapolate from today's systems. This is intended to create a feasible iteration loop for theoretical research. We are also conducting empirical research, but mostly to help generate and probe theoretical ideas rather than to test different empirical assumptions.</p>\n<h3 id=\"zoom-level-2\">Zoom level 2</h3>\n<p><img alt=\"birds_eye_lvl2.svg\" src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ztokaf9harKTmRcn4/viuz4qmk6lyvtjjc1cep\" /></p>\n<p>Most of ARC's research attempts to solve one of two central subproblems in alignment: alignment robustness and eliciting latent knowledge (ELK).</p>\n<p><strong>Alignment robustness</strong> refers to AI systems remaining intent aligned even when faced with out-of-distribution inputs.<sup class=\"footnote-ref\"><a href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fn2\" id=\"fnref2\">[2]</a></sup> There are a few reasons to focus on failures of alignment robustness, as discussed <a href=\"https://ai-alignment.com/techniques-for-optimizing-worst-case-performance-39eafec74b99\">here</a> (where they are called \"malign\" failures). A quintessential example of an alignment robustness failure is <a href=\"https://arxiv.org/abs/1906.01820\"><em>deceptive alignment</em></a>, also known as \"<a href=\"https://arxiv.org/abs/2311.08379\">scheming</a>\": the possibility that an AI system will internally reason about the objective that it is being trained on, and stop being intent aligned when it detects clues that it has been taken out of its training environment.</p>\n<p><strong>Eliciting latent knowledge (ELK)</strong> is defined in <a href=\"https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit\">this report</a>, and asks: how can we train an AI system to honestly report its internal beliefs, rather than what it predicts a human would think? If we could do this, then we could potentially avoid misalignment by checking whether the model's beliefs are consistent with its actions being helpful. ELK could help with scalable alignment via alignment robustness, but it could also help via <a href=\"https://www.alignmentforum.org/posts/SzecSPYxqRa5GCaSF/clarifying-inner-alignment-terminology\">outer alignment</a>, by giving the reward function access to relevant information known by the model.</p>\n<h3 id=\"zoom-level-3\">Zoom level 3</h3>\n<p><img alt=\"birds_eye_lvl3.svg\" src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ztokaf9harKTmRcn4/fyvd4wrd3eb8jzkmoowt\" /></p>\n<p>ARC hopes to make progress on both alignment robustness and ELK using <strong>heuristic explanations</strong> for neural network behaviors. A heuristic explanation is similar to the kind of explanation found in <a href=\"https://www.transformer-circuits.pub/2022/mech-interp-essay\">mechanistic interpretability</a>, except that ARC is attempting to find a mathematical notion of an \"explanation\", so that they can be found and used automatically. This is similar to how formal verification for ordinary programs can be performed automatically, except that we believe proof is too strict of a standard to be feasible. These similarities are discussed in more detail in the post <a href=\"https://www.alignment.org/blog/formal-verification-heuristic-explanations-and-surprise-accounting/\">Formal verification, heuristic explanations and surprise accounting</a> (especially the first couple of sections, up until \"Surprise accounting\").</p>\n<p>A heuristic explanation for a rare but high-stakes kind of failure could help with alignment robustness, while a heuristic explanation for a specific behavior of interest could help with ELK. These two applications of heuristic explanations are fleshed out in more detail at the next zoom level.</p>\n<h3 id=\"zoom-level-4\">Zoom level 4</h3>\n<p><img alt=\"birds_eye_lvl4.svg\" src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ztokaf9harKTmRcn4/mryt00c7l23wrevetznz\" /></p>\n<p>ARC has identified two broad ways in heuristic explanations could help with alignment robustness and/or ELK.</p>\n<p><strong>Low probability estimation (LPE)</strong> is the task of estimating the probability of a rare kind of model output. The most obvious approach to LPE is to try to find model inputs that give rise to such an output, but this can be infeasible (e.g. if the model were to implement something like a cryptographic hash function). Instead, we can \"<a href=\"https://www.alignmentforum.org/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment\">relax</a>\" this goal and search for a heuristic explanation of why the model could hypothetically produce such an output (e.g. by treating the output of the cryptographic hash function as random). LPE would help with alignment robustness by allowing us to select models for which we cannot explain why they would ever behave catastrophically, even hypothetically. This motivation for LPE is discussed in much greater depth in the post <a href=\"https://www.alignment.org/blog/estimating-tail-risk-in-neural-networks/\">Estimating Tail Risk in Neural Networks</a>.</p>\n<p><strong>Mechanism distinction</strong> describes our broad hope for how heuristic explanations could help with ELK. A central challenge for ELK is \"sensor tampering\": detecting when the model reports what it predicts a human would think, but the human has been fooled in some way. Our hope is to detect this by noticing that the model's report has been produced by an \"abnormal mechanism\". There are a few potential ways in which heuristic explanations could be used to perform mechanism distinction, but the one we currently consider the most promising is <strong>mechanistic anomaly detection (MAD)</strong>, as explained in the post <a href=\"https://www.alignment.org/blog/mechanistic-anomaly-detection-and-elk/\">Mechanistic anomaly detection and ELK</a> (for a gentler introduction to MAD, see <a href=\"https://www.lesswrong.com/s/GiZ6puwmHozLuBrph/p/n7DFwtJvCzkuKmtbG\">this post</a>). A variant of MAD is <strong>safe distillation</strong>, which is an alternative way to perform mechanism distinction if we also have access to a formal specification of what we are trying to elicit latent knowledge of.</p>\n<p>A semi-formal account of how heuristic explanations could enable all of LPE, MAD and safe distillation is given in <a href=\"https://www.alignment.org/blog/research-update-towards-a-law-of-iterated-expectations-for-heuristic-estimators/\">Towards a Law of Iterated Expectations for Heuristic Estimators</a>. An explanation of how MAD could also be used to help with alignment robustness is given in <a href=\"https://www.alignment.org/blog/mechanistic-anomaly-detection-and-elk/\">Mechanistic anomaly detection and ELK</a> (in the section \"Deceptive alignment\").</p>\n<h2 id=\"how-arcs-research-fits-into-this-picture\">How ARC's research fits into this picture</h2>\n<p>We will now explain how some of ARC's research fits into the above diagram at the most zoomed in level. For completeness, we will cover all of ARC's most significant pieces of published research to date, in chronological order. Each piece of work has been labeled with the most closely related node from the diagram, but often also covers nearby nodes and the relationships between them.</p>\n<table>\n    <tbody><tr><td><a href=\"https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit\"><strong>Eliciting latent knowledge: How to tell if your eyes deceive you</strong></a> defines ELK, explains its importance for scalable alignment, and covers a large number of possible approaches to ELK. Some of these approaches are somewhat related to heuristic explanations, but most are alternatives that we are no longer pursuing.</td><td style=\"width: 109px;\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ztokaf9harKTmRcn4/gexim4y9pnprmswlgx91\" style=\"width: 93px;\" /></td></tr>\n    <tr><td><a href=\"https://arxiv.org/abs/2211.06738\"><strong>Formalizing the presumption of independence</strong></a> lays out the problem of devising a formal notion of heuristic explanations, and makes some early inroads into this problem. It also includes a brief discussion of the motivation for heuristic explanations and the application to alignment robustness and ELK.</td><td style=\"width: 109px;\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ztokaf9harKTmRcn4/c88bwyukwyrwoqbfagdc\" style=\"width: 93px;\" /></td></tr>\n    <tr><td><a href=\"https://www.alignment.org/blog/mechanistic-anomaly-detection-and-elk/\"><strong>Mechanistic anomaly detection and ELK</strong></a> and our other late 2022 blog posts (<a href=\"https://www.alignment.org/blog/finding-gliders-in-the-game-of-life/\"><strong>1</strong></a>, <strong><a href=\"https://www.alignment.org/blog/can-we-efficiently-explain-model-behaviors/\">2</a></strong>, <a href=\"https://www.alignment.org/blog/can-we-efficiently-distinguish-different-mechanisms/\"><strong>3</strong></a>) explain the approach to mechanism distinction that we currently find the most promising, mechanistic anomaly detection (MAD). They also cover how mechanism distinction could be used to address alignment robustness and ELK, how heuristic explanations could be used for mechanism distinction, and the feasibility of finding heuristic explanations.</td><td style=\"width: 109px;\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ztokaf9harKTmRcn4/uhdvvkujmgyqy3npt2u1\" style=\"width: 93px;\" /></td></tr>\n    <tr><td><a href=\"https://www.alignment.org/blog/formal-verification-heuristic-explanations-and-surprise-accounting/\"><strong>Formal verification, heuristic explanations and surprise accounting</strong></a> discusses the high-level motivation for heuristic explanations by comparing and contrasting them to formal verification for neural networks (as explored in <a href=\"https://arxiv.org/abs/2406.11779\">this paper</a>) and mechanistic interpretability. It also introduces <em>surprise accounting</em>, a framework for quantifying the quality of a heuristic explanation, and presents a <a href=\"https://www.alignment.org/content/files/2024/06/max_of_k_writeup.pdf\">draft</a> of empirical work on heuristic explanations.</td><td style=\"width: 109px;\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ztokaf9harKTmRcn4/c88bwyukwyrwoqbfagdc\" style=\"width: 93px;\" /></td></tr>\n    <tr><td><a href=\"https://www.alignment.org/blog/backdoors-as-an-analogy-for-deceptive-alignment/\"><strong>Backdoors as an analogy for deceptive alignment</strong></a> and the associated paper <a href=\"https://arxiv.org/abs/2409.03077\"><strong>Backdoor defense, learnability and obfuscation</strong></a> discuss a formal notion of backdoors in ML models and some theoretical results about it. This serves as an analogy for the subdiagram <em>Heuristic explanations \u2192 Mechanism distinction \u2192 Alignment robustness</em>. In this analogy, alignment robustness corresponds to a model being backdoor-free, mechanism distinction corresponds to the backdoor defense, and heuristic explanations correspond to so-called \"mechanistic\" defenses. The blog post covers this analogy in more depth.</td><td style=\"width: 109px;\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ztokaf9harKTmRcn4/oitnkbfpxwjjgckcs73c\" style=\"width: 93px;\" /></td></tr>\n    <tr><td><a href=\"https://www.alignment.org/blog/estimating-tail-risk-in-neural-networks/\"><strong>Estimating Tail Risk in Neural Networks</strong></a> lays out the problem of low probability estimation, how it would help with alignment robustness, and possible approaches to LPE based on heuristic explanations. It also presents a <a href=\"https://www.alignment.org/content/files/2024/09/Analytically_Learning_VAEs.pdf\">draft</a> describing an approach to heuristic explanations based on analytically learning variational autoencoders.</td><td style=\"width: 109px;\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ztokaf9harKTmRcn4/kktrwydz4asgchl1vpby\" style=\"width: 93px;\" /></td></tr>\n    <tr><td><a href=\"https://www.alignment.org/blog/research-update-towards-a-law-of-iterated-expectations-for-heuristic-estimators/\"><strong>Towards a Law of Iterated Expectations for Heuristic Estimators</strong></a> and the associated <a href=\"https://arxiv.org/abs/2410.01290\"><strong>paper</strong></a> discuss a possible coherence property for heuristic explanations as part of the search for a formal notion of heuristic explanations. It also provides a semi-formal account of how heuristic explanations could be applied to low probability estimation and mechanism distinction. </td><td style=\"width: 109px;\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ztokaf9harKTmRcn4/c88bwyukwyrwoqbfagdc\" style=\"width: 93px;\" /></td></tr>\n    <tr><td><a href=\"https://www.alignment.org/blog/low-probability-estimation-in-language-models/\"><strong>Low Probability Estimation in Language Models</strong></a> and the associated paper <strong><a href=\"https://arxiv.org/abs/2410.13211\">Estimating the Probabilities of Rare Outputs in Language Models</a></strong> describe an empirical study of LPE in the context of small transformer language models. The method inspired by heuristic explanations outperforms naive sampling in this setting, but does not outperform methods based on red-teaming (searching for inputs giving rise to the rare behavior), although there remain theoretical cases where red-teaming fails.<p></p></td><td style=\"width: 109px;\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ztokaf9harKTmRcn4/kktrwydz4asgchl1vpby\" style=\"width: 93px;\" /></td></tr>\n</tbody></table>\n<h2 id=\"further-subproblems\">Further subproblems</h2>\n<p>ARC's research can be subdivided further, and we have been putting significant effort into a number of subproblems not explicitly mentioned above. For instance, our work on heuristic explanations includes both work on <strong>formalizing heuristic explanations</strong> (devising a formal framework for heuristic explanations) and work on <strong>finding heuristic explanations</strong> (designing efficient search algorithms for them). Some subproblems of these include:</p>\n<ul>\n<li><strong>Measuring quality</strong>: \"<a href=\"https://www.alignment.org/blog/formal-verification-heuristic-explanations-and-surprise-accounting/#surprise-accounting\">surprise accounting</a>\" offers a potential way to measure the quality of a heuristic explanation, which is important for being able to search for high-quality explanations. However, it is currently an informal framework with many missing details.</li>\n<li><strong>Capacity allocation</strong>: it will probably be too challenging to find high-quality explanations for every aspect of a model's behavior. Instead, we can try to tailor explanations towards behaviors with potentially catastrophic consequences. A good loss function for heuristic explanations should push for quality only where it is relevant to the behavior at hand.</li>\n<li><strong>Cherry-picking</strong>: if we use a heuristic explanation to estimate something (as in low probability estimation), we need to make sure that the way in which we find the explanation doesn't systematically bias the estimate.</li>\n<li><strong>Form of representation</strong>: one form that a heuristic explanation could take is of an \"<a href=\"https://www.alignment.org/blog/estimating-tail-risk-in-neural-networks/#layer-by-layer-activation-modeling\">activation model</a>\", i.e. a probability distribution over a model's internal activations. However, we may also need to represent explanations that do not correspond to any particular probability distribution.</li>\n<li><strong>Formal desiderata</strong>: we can attempt to formalize heuristic explanations by considering <a href=\"https://www.alignment.org/blog/research-update-towards-a-law-of-iterated-expectations-for-heuristic-estimators/\">properties</a> that we think they should satisfy, and seeing if those properties can be satisfied.</li>\n<li><strong>No-coincidence principle</strong>: in order for heuristic explanations to work in the worst case, we need every possible behavior to be amenable to explanation. We sometimes refer to this desideratum as the \"no-coincidence principle\" (a term taken from <a href=\"https://mxphi.com/wp-content/uploads/2023/04/MxPhi-Gowers2023.pdf\">this paper</a>). Counterexamples to this principle could present obstacles to our approach.</li>\n<li><strong>Empirical regularities</strong>: some model weights may have no explanation beyond being tuned to match some empirical average, either because the input distribution is defined empirically, or because of an emergent regularity in a formally-defined system (such as the relative value of a queen and a pawn in chess). A good notion of heuristic explanations should be able to deal with these.</li>\n</ul>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>We have painted a high-level picture of ARC's research, explained how our published research fits into it, and briefly discussed some additional subproblems that we are working on. We hope this provides people with a clearer sense of what we are up to.</p>\n<hr class=\"footnotes-sep\" />\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li class=\"footnote-item\" id=\"fn1\"><p>An arrow in the diagram expresses that solving one problem should help solve another, but it varies from case to case whether subproblems combine \"conjunctively\" (all subproblems need to be solved to solve the main problem) or \"disjunctively\" (a solution to any subproblem can be used to solve the main problem). <a class=\"footnote-backref\" href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fnref1\">\u21a9\ufe0e</a></p>\n</li>\n<li class=\"footnote-item\" id=\"fn2\"><p>The term \"alignment robustness\" comes from <a href=\"https://www.alignmentforum.org/posts/Epm6CkXrdRyAihMRe/an-66-decomposing-robustness-into-capability-robustness-and\">this summary</a> of <a href=\"https://www.alignmentforum.org/posts/2mhFMgtAjFJesaSYR/2-d-robustness\">this post</a>, and is synonymous with \"objective robustness\" in the terminology of <a href=\"https://www.alignmentforum.org/posts/SzecSPYxqRa5GCaSF/clarifying-inner-alignment-terminology\">this post</a>. A slightly more formal variant is \"high-stakes alignment\", as defined in <a href=\"https://ai-alignment.com/low-stakes-alignment-f3c36606937f\">this post</a>. <a class=\"footnote-backref\" href=\"https://www.lesswrong.com/feed.xml?view=curated-rss#fnref2\">\u21a9\ufe0e</a></p>\n</li>\n</ol>\n</section><br /><br /><a href=\"https://www.lesswrong.com/posts/ztokaf9harKTmRcn4/a-bird-s-eye-view-of-arc-s-research#comments\">Discuss</a>", "published": "Sun, 27 Oct 2024 17:29:12 GMT"}, {"title": "Arithmetic is an underrated world-modeling technology", "link": "https://www.lesswrong.com/posts/r2LojHBs3kriafZWi/arithmetic-is-an-underrated-world-modeling-technology", "summary": "Published on October 17, 2024 2:00 PM GMT<br /><br /><p>Of all the cognitive tools our ancestors left us, what\u2019s best? Society seems to think pretty highly of arithmetic. It\u2019s one of the first things we learn as children. So I think it\u2019s weird that only a tiny percentage of people seem to know how to actually <i>use</i> arithmetic. Or maybe even understand what arithmetic is for. Why?</p><p>I think the problem is the idea that arithmetic is about \u201ccalculating\u201d. No! Arithmetic is a world-modeling technology. Arguably, it\u2019s the <i>best</i> world-modeling technology: It\u2019s simple, it\u2019s intuitive, and it applies to everything. It allows you to trespass into scientific domains where you don\u2019t belong. It even has an amazing error-catching mechanism built in.</p><p>One hundred years ago, maybe it was important to learn long division. But the <i>point</i> of long division was to enable you to do world-modeling. Computers don\u2019t make arithmetic obsolete. If anything, they do the opposite. Without arithmetic, you simply can\u2019t access a huge fraction of the most important facts about the world.</p><p>The magic lives in a thing called \u201cunits\u201d.</p><h2>Chimps</h2><p>It\u2019s amazing how much we don\u2019t know about nutrition. For example, would you live longer if you ate less salt? How much longer? We can <a href=\"https://doi.org/10.1093/eurheartj/ehac208\">guess</a>, but we don\u2019t really know.</p><p>To really be sure, we\u2019d need to take two groups of people, get them to eat different amounts of salt, and then see how long they live. This is expensive, ethically fraught, and runs into the problem that when you tell people to eat differently, they usually ignore you.</p><p>So I\u2019ve often wondered: Why don\u2019t we do these experiments on <i>animals</i>? Why not get two big groups of chimpanzees, and feed them different amounts of salt? Chimps aren\u2019t people, but it would tell us something, right?</p><p>Why don\u2019t we do this? Because arithmetic.</p><p>How much would such a study cost? To figure this out, you will need three numbers:</p><ol><li>The average lifespan of a chimp.</li><li>The cost to maintain one chimp in a research facility for one day.</li><li>The number of chimps you\u2019d need for a study.</li></ol><p>Let\u2019s do these. First, how long do chimps live? In captivity the average seems to be around 36.3 years. (Incidentally, female chimps seem to live 25% longer than males\u2014imagine human women lived until 90 while men died at 71.)</p><p>Second, how much does it cost to maintain a chimp? <a href=\"https://www.releasechimps.org/photos-and-pics/Economics%20%20Chimp%20Research_T%20%20Capaldo%20et%20al%20revised%20for%20website.pdf\">Capaldo et al.</a> looked at the average costs in various research facilities in the US in 2009. They estimate around $75/day (in 2024 dollars).</p><p>Finally, how many chimps do you need? To calculate this, you should do a \u201cpower calculation\u201d\u2014you guess how much life expectancy varies due to (a) salt and (b) all random factors, and work backwards to see how many chimps you need to separate the signal from the noise. There are lots of <a href=\"https://clincalc.com/stats/samplesize.aspx\">calculators</a> for this. If you assume chimps live 36.3\u00b18 years and salt would change life expectancy by 2 years, these will tell you that you need 502 chimps.</p><p>So now we can do our calculation:</p><p>&nbsp;&nbsp;502 chimps<br />&nbsp;&nbsp;&nbsp;&nbsp; \u00d7 36.3 years<br />&nbsp;&nbsp;&nbsp;&nbsp; \u00d7 365.25 days / year<br />&nbsp;&nbsp;&nbsp;&nbsp; \u00d7 75 dollars / (chimp day)<br />&nbsp;&nbsp;&nbsp;&nbsp; \u2248 499,185,349 dollars</p><p>Notice three things.</p><p>First, 500 million dollars is a <i>lot</i>. That\u2019s five times what the <a href=\"https://dynomight.net/alcohol-trial/\">big alcohol trial</a> would have cost. It\u2019s a gigantic amount of money for something that would only give indirect evidence for the impact of salt in humans, and wouldn\u2019t even do that until decades in the future.</p><p>Second, notice how I kept the units. Always keep units! On the \u201ctop\u201d of the calculation, the units were \u201cchimps \u00d7 years \u00d7 days \u00d7 dollars\u201d. On the \u201cbottom\u201d, the units were \u201cyears \u00d7 chimps \u00d7 days\u201d. When you cancel terms, you\u2019re left with dollars only. Units are great because if you made a mistake, it will probably show up in the units not working out. We\u2019ll see other benefits below. So: ALWAYS KEEP UNITS.</p><p>(If you think you\u2019re an exception and you don\u2019t need units, then you <i>especially</i> need to keep units.)</p><p>Finally, notice that this calculation didn\u2019t just tell us how expensive the study would be. It also points towards <i>why</i> it\u2019s so expensive, and what would be needed to make it cheaper.</p><p>One option would be to try to get away with fewer chimps. The reason so many are needed is because the likely impact of salt is pretty small compared to natural variation in life expectancy. You might be able to reduce that natural variation by, for example, using pairs of chimp twins to eliminate genetic variation. If that reduced the standard deviation from 8 years to 5 years, then you\u2019d only need 196 chimps and the total cost would be \u201conly\u201d 195 million dollars. Sounds nice, though I imagine that creating 98 chimp twins wouldn\u2019t be free.</p><p>Another option would be to reduce the cost of maintaining chimps. Doesn\u2019t $75 per chimp per day seem <i>very</i> expensive? Perhaps you could find a way to use existing chimps in zoos? Or you could use dogs instead of chimps and offer dog owners subsidized dog chow with slightly varying salt levels? Or you could built a gigantic outdoor facility with 50,000 chimps where you could amortize costs by running 100 experiments in parallel?</p><p>I\u2019m not sure which (if any) of these options would work. My point is that doing the arithmetic quickly takes you into specifics about what would be necessary to actually move the needle. Without doing the arithmetic, what chance would you have to say anything meaningful?</p><h2>Big blocks</h2><p>If I know my readers then at some point in your life you probably considered using gravity to store energy. Maybe you can put solar panels on your roof, but instead of storing their energy in batteries, you can just lift up a giant block into the air. At night you can slowly let the block down to power your house. How big a block do you need?</p><p>Let\u2019s assume you don\u2019t know much physics.</p><p>To answer this question, you\u2019ll need two numbers:</p><ol><li>How much energy do you need to store to power your house?</li><li>How much energy can you store by lifting up a giant block?</li></ol><p>If you check the internet, you\u2019ll learn that the average US household uses around 30 kWh of energy per day. Now, what\u2019s a \u201ckWh\u201d? To you, person who doesn\u2019t know much physics, it looks scary, but apparently it\u2019s some kind of unit of energy, so let\u2019s just write it down. Assume you need to store half your daily energy for usage at night, or 15 kWh.</p><p>Now, how much energy can you store by lifting a giant block up into the air? A little bit of searching reveals that \u201c<a href=\"https://en.wikipedia.org/wiki/Potential_energy#Overview\">potential energy</a>\u201d is the product of mass, gravity, and height: If you lift a block of weight <i>MASS</i> up to height <i>HEIGHT</i>, the stored energy is <i>U=MASS \u00d7 g \u00d7 HEIGHT</i> where <i>g \u2248 9.8 m/s\u00b2</i> on Earth. Your house is 6m tall, and you reckon that\u2019s as high as you could lift a block, so you use <i>h</i> = 6m. Thus, the amount of energy you can store is</p><p>&nbsp; <i>MASS \u00d7 (9.8 m/s\u00b2) \u00d7 6 m</i>.</p><p>What now? Now, you\u2019re done! You just equate the energy you need to store with the energy you can store with a block that weighs <i>MASS</i>:</p><p>&nbsp;&nbsp;15 kWh = <i>MASS</i> \u00d7 (9.8 m/s\u00b2) \u00d7 6 m.</p><p>Is this frightening? There are units everywhere. You never figured out what a kWh is. How is that related to meters and seconds? What does it mean to square a second? Panic!</p><p>Relax. We have computers. You can just mechanically solve the above the above equation to get <i>MASS</i> = 15 kWh / (9.8 m/s\u00b2 \u00d7 6 m) and then <a href=\"https://www.google.com/search?q=15+kWh+%2F+(+9.8+m%2Fs^2+*+6+m)\"><i>literally type that into a search engine</i></a> to find that <i>MASS</i> is:</p><figure class=\"image image_resized\" style=\"width: 62.08%;\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/r2LojHBs3kriafZWi/tp4qw0vhqe9ajitils7a\" /></figure><p>Look at that\u2014the answer is in kilograms!</p><p>It happens to be the case that 1 kWh = 3,600,000 kg m\u00b2/s\u00b2. You <i>could</i> substitute this and cancel units to get the same answer. But don\u2019t. Attempting that just gives you the chance to screw things up. Why complicate your life?</p><p>And as before, the units give you a sort of \u201ctype checking\u201d. If your calculation was wrong, you\u2019d have to be very unlucky to get an answer that was in kg anyway.</p><p>Here the units did most of the work for you. So it\u2019s a good thing you kept units.</p><p><strong>ALWAYS KEEP UNITS</strong>.</p><h2>More big blocks</h2><p>So, a 918 thousand kg block. How much would that cost? It seems natural to use rock, but it\u2019s hard to get million kilogram boulders delivered to your house these days. So let\u2019s use steel. Current steel prices are $350/ton. So we want to solve</p><p>&nbsp; <i>918,367 kg = MONEY \u00d7 1 ton / $350</i>.</p><p>How are tons related to kilograms? Say it with me: <i>Not your problem</i>. Just solve the above equation for <i>MONEY</i> and ask the <a href=\"https://www.google.com/search?q=918367+kg+%2F+%281+ton+%2F+%24350%29\">big computer</a> to learn that <i>MONEY</i> is</p><figure class=\"image image_resized\" style=\"width: 60.96%;\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/r2LojHBs3kriafZWi/ziypldfkuiyyictborbe\" /></figure><p>That\u2019s 65\u00d7 more than just buying a 20 kWh home battery. But let\u2019s say you\u2019re committed to the bit. How <i>big</i> would that block be? Some searching reveals that the density of steel is around 7.85 g/cm\u00b3. So if you have a cubic block of volume VOLUME, then</p><p>&nbsp; <i>MASS = 7.85 g / cm\u00b3 \u00d7 VOLUME</i>.</p><p>Solving for <i>VOLUME</i>, using the previous value for <i>MASS</i>, and not stressing about units, you can <a href=\"https://www.google.com/search?q=918367+kg+%2F+(7.85+g+%2F+cm^3)+in+m^3\">easily find</a> that <i>VOLUME</i> is</p><figure class=\"image image_resized\" style=\"width: 61.93%;\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/r2LojHBs3kriafZWi/mlrszpmbq1kptc1fbtov\" /></figure><p>A 117 cubic meter block is around 4.9 meters on all sides. So, roughly speaking, your house will look something like this:</p><figure class=\"image image_resized\" style=\"width: 65.12%;\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/r2LojHBs3kriafZWi/y5jwbpdin15hycx8shgd\" /></figure><p>As it happens, 1 million kg cranes <i>do</i> exist. But even used, they\u2019ll set you back another million dollars or so. If you\u2019re going to get one of those, then may I suggest that the same weight is given by almost exactly 4.5 Statues of Liberty? So I suggest you also consider this option (drawn to scale):</p><figure class=\"image image_resized\" style=\"width: 68.02%;\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/r2LojHBs3kriafZWi/iwkhc4rvkbulwd5v0d91\" /></figure><p>Either way, your neighbors will love it.</p><br /><br /><a href=\"https://www.lesswrong.com/posts/r2LojHBs3kriafZWi/arithmetic-is-an-underrated-world-modeling-technology#comments\">Discuss</a>", "published": "Tue, 22 Oct 2024 18:01:57 GMT"}, {"title": "Why I\u2019m not a Bayesian", "link": "https://www.lesswrong.com/posts/TyusAoBMjYzGN3eZS/why-i-m-not-a-bayesian", "summary": "Published on October 6, 2024 3:22 PM GMT<br /><br /><p>This post focuses on philosophical objections to Bayesianism as an epistemology. I first explain Bayesianism and some standard objections to it, then lay out my two main objections (inspired by ideas in philosophy of science). A follow-up post will speculate about how to formalize an alternative.</p><h3><strong>Degrees of belief</strong></h3><p>The core idea of <a href=\"https://plato.stanford.edu/entries/epistemology-bayesian/\"><u>Bayesian epistemology</u></a>: we should ideally reason by assigning credences to propositions which represent our degrees of belief that those propositions are true. (Note that this is different from Bayesianism as a set of statistical techniques, or Bayesianism as an approach to machine learning, which I don\u2019t discuss here.)</p><p>If that seems like a sufficient characterization to you, you can go ahead and skip to the next section, where I explain my objections to it. But for those who want a more precise description of Bayesianism, and some existing objections to it, I\u2019ll more specifically characterize it in terms of five subclaims. Bayesianism says that we should ideally reason in terms of:</p><ol><li>Propositions which are either true or false (<strong>classical logic</strong>)</li><li>Each of which is assigned a credence (<strong>probabilism</strong>)</li><li>Representing subjective degrees of belief in their truth (<strong>subjectivism</strong>)</li><li>Which at each point in time obey the&nbsp;<a href=\"https://en.wikipedia.org/wiki/Probability_axioms\"><u>axioms of probability</u></a> (<strong>static rationality</strong>)</li><li>And are updated over time by applying&nbsp;<a href=\"https://en.wikipedia.org/wiki/Bayes%27_theorem\"><u>Bayes\u2019 rule</u></a> to new evidence (<strong>rigid empiricism</strong>)</li></ol><p>I won\u2019t go into the case <i>for</i> Bayesianism here except to say that it does elegantly formalize many common-sense intuitions. Bayes\u2019 rule follows directly from <a href=\"https://oscarbonilla.com/2009/05/visualizing-bayes-theorem/\">a straightforward Venn diagram</a>. The axioms of probability are powerful and mathematically satisfying. Subjective credences seem like the obvious way to represent our uncertainty about the world. Nevertheless, there are a wide range of alternatives to Bayesianism, each branching off from the claims listed above at different points:</p><ol><li><a href=\"https://plato.stanford.edu/entries/knowledge-analysis/\"><i><u>Traditional epistemology</u></i></a> only accepts #1, and rejects #2. Traditional epistemologists often defend a binary conception of knowledge\u2014e.g. one defined in terms of justified true belief (or a similar criterion, like reliable belief).</li><li><a href=\"https://en.wikipedia.org/wiki/Frequentist_probability\"><i><u>Frequentism</u></i></a><i> </i>accepts #1 and #2, but rejects #3: it doesn\u2019t think that credences should be subjective. Instead, frequentism holds that credences should correspond to the&nbsp;relative frequency&nbsp;of an event in the long term, which is an objective fact about the world. For example, you should assign 50% credence that a flipped coin will come up heads, because if you continued flipping the coin the proportion of heads would approach 50%.</li><li><a href=\"https://www.lesswrong.com/posts/y5GftLezdozEHdXkL/an-intuitive-guide-to-garrabrant-induction\"><i><u>Garrabrant induction</u></i></a> accepts #1 to #3, but rejects #4. In order for credences to obey the axioms of probability, all the logical implications of a statement must be assigned the same credence. But this \u201clogical omniscience\u201d is impossible for computationally-bounded agents like ourselves. So in the Garrabrant induction framework, credences instead converge to obeying the axioms of probability&nbsp;in the limit, without guarantees that they\u2019re coherent after only limited thinking time.</li><li><a href=\"https://www.lesswrong.com/posts/xJyY5QkQvNJpZLJRo/radical-probabilism-1\"><i><u>Radical probabilism</u></i></a> accepts #1 to #4, but rejects #5. Again, this can be motivated by qualms about logical omniscience: if thinking for longer can identify new implications of our existing beliefs, then our credences sometimes need to update via a different mechanism than Bayes\u2019 rule. So radical probabilism instead allows an agent to update to any set of statically rational credences at any time, even if they\u2019re totally different from its previous credences. The one constraint is that each credence needs to converge over time to a fixed value\u2014i.e. it can\u2019t continue oscillating indefinitely (otherwise the agent would be vulnerable to a&nbsp;<a href=\"https://en.wikipedia.org/wiki/Dutch_book_theorems\"><u>Dutch Book</u></a>).</li></ol><p>It\u2019s not crucial whether we classify Garrabrant induction and radical probabilism as variants of Bayesianism or alternatives to it, because my main objection to Bayesianism doesn\u2019t fall into any of the above categories. Instead, I think we need to go back to basics and reject #1. Specifically, I have two objections to the idea that idealized reasoning should be understood in terms of propositions that are true or false:</p><ol><li>We should assign truth-values that are intermediate between true and false (<strong>fuzzy truth-values</strong>)</li><li>We should reason in terms of&nbsp;<i>models</i> rather than propositions (the&nbsp;<strong>semantic view</strong>)</li></ol><p>I\u2019ll defend each claim in turn.</p><h3><strong>Degrees of truth</strong></h3><p>Formal languages (like code) are only able to express ideas that can be pinned down precisely. Natural languages, by contrast, can refer to vague concepts which don\u2019t have clear, fixed boundaries. For example, the truth-values of propositions which contain&nbsp;<a href=\"https://learnenglish.britishcouncil.org/grammar/b1-b2-grammar/adjectives-gradable-non-gradable\"><u>gradable adjectives</u></a> like \u201clarge\u201d or \u201cquiet\u201d or \u201chappy\u201d depend on how we interpret those adjectives. Intuitively speaking, a description of something as \u201clarge\u201d can be more or less true depending on how large it actually is. The most common way to formulate this spectrum is as \u201cfuzzy\u201d truth-values which range from 0 to 1. A value close to 1 would be assigned to claims that are clearly true, and a value close to 0 would be assigned to claims that are clearly false, with claims that are \u201ckinda true\u201d in the middle.</p><p>Another type of \u201ckinda true\u201d statements are approximations. For example, if I claim that there\u2019s a grocery store 500 meters away from my house, that\u2019s probably true in an&nbsp;<i>approximate</i> sense, but false in a&nbsp;<i>precise</i> sense. But once we start distinguishing the different senses that a concept can have, it becomes clear that basically&nbsp;<i>any</i> concept can have widely divergent category boundaries depending on the context.&nbsp;<a href=\"https://metarationality.com/refrigerator\"><u>A striking example from Chapman</u></a>:&nbsp;</p><blockquote><p>A: Is there any water in the refrigerator?<br />B: Yes.<br />A: Where? I don\u2019t see it.<br />B: In the cells of the eggplant.</p></blockquote><p>The claim that there\u2019s water in the refrigerator is technically true, but pragmatically false. And the concept of \u201cwater\u201d is far better-defined than almost all abstract concepts (like the ones I\u2019m using in this post). So we should treat natural-language propositions as context-dependent by default. But that\u2019s still consistent with some statements being&nbsp;<i>more</i> context-dependent than others (e.g. the claim that there\u2019s air in my refrigerator would be true under almost any interpretation). So another way we can think about fuzzy truth-values is as a range from \u201cthis statement is false in almost any sense\u201d through \u201cthis statement is true in some senses and false in some senses\u201d to \u201cthis statement is true in almost any sense\u201d.</p><p>Note, however, that there\u2019s an asymmetry between \u201cthis statement is true in almost any sense\u201d and \u201cthis statement is false in almost any sense\u201d, because the latter can apply to two different types of claims. Firstly, claims that are meaningful but false (\u201cthere\u2019s a tiger in my house\u201d). Secondly, claims that are&nbsp;<i>nonsense</i>\u2014there are just no meaningful interpretations of them at all (\u201ccolorless green ideas sleep furiously\u201d). We can often distinguish these two types of claims by negating them: \u201cthere isn\u2019t a tiger in my house\u201d is true, whereas \u201ccolorless green ideas don\u2019t sleep furiously\u201d is still nonsense. Of course, nonsense is also a matter of degree\u2014e.g. metaphors are by default less meaningful than concrete claims, but still not entirely nonsense.</p><p>So I've motivated fuzzy truth-values from four different angles: vagueness, approximation, context-dependence, and sense vs nonsense. The key idea behind each of them is that concepts have fluid and amorphous category boundaries (a property called&nbsp;<a href=\"https://metarationality.com/nebulosity\"><i><u>nebulosity</u></i></a>). However, putting all of these different aspects of nebulosity on the same zero-to-one scale might be an oversimplification. More generally,&nbsp;<a href=\"https://en.wikipedia.org/wiki/Fuzzy_logic\"><u>fuzzy logic</u></a> has few of the appealing properties of classical logic, and (to my knowledge) isn\u2019t very directly useful. So I\u2019m not claiming that we should adopt fuzzy logic wholesale, or that we know what it means for a given proposition to be X% true instead of Y% true (a question which I\u2019ll come back to in a follow-up post). For now, I\u2019m just claiming that there\u2019s an important sense in which thinking in terms of fuzzy truth-values is&nbsp;<i>less wrong</i> (another non-binary truth-value) than only thinking in terms of binary truth-values.</p><h3><strong>Model-based reasoning</strong></h3><p>The intuitions in favor of fuzzy truth-values become clearer when we apply them, not just to individual propositions, but to&nbsp;<i>models</i> of the world. By a model I mean a (mathematical) structure that attempts to describe some aspect of reality. For example, a model of the weather might have variables representing temperature, pressure, and humidity at different locations, and a procedure for updating them over time. A model of a chemical reaction might have variables representing the starting concentrations of different reactants, and a method for determining the equilibrium concentrations. Or, more simply, a model of the Earth might just be a sphere.</p><p>In order to pin down the difference between reasoning about propositions and reasoning about models, philosophers of science have drawn on concepts from mathematical logic. They distinguish between the&nbsp;<i>syntactic</i> content of a theory (the axioms of the theory) and its&nbsp;<i>semantic</i> content (the models for which those axioms hold).&nbsp;<a href=\"https://plato.stanford.edu/entries/structure-scientific-theories/\"><u>As an example</u></a>, consider the three axioms of&nbsp;<a href=\"https://en.wikipedia.org/wiki/Projective_plane\"><u>projective planes</u></a>:</p><ol><li>For any two points, exactly one line lies on both.</li><li>For any two lines, exactly one point lies on both.</li><li>There exists a set of four points such that no line has more than two of them.</li></ol><p>There are infinitely many models for which these axioms hold; here\u2019s one of the simplest:</p><p><img alt=\"Geometric figure including triangle ACE with interior circle BDF and center point G. Point B is on line segment AC, D is on CE, and F is on AE. G is the center of the circle. Point G is on line segments AD, BE, and CF.\" src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TyusAoBMjYzGN3eZS/tetojvzadppwmdspoagk\" /></p><p>If propositions and models are two sides of the same coin, does it matter which one we primarily reason in terms of? I think so, for two reasons. Firstly, most models are very difficult to put into propositional form. We each have implicit mental models of our friends\u2019 personalities, of how liquids flow, of what a given object feels like, etc, which are far richer than we can express propositionally. The same is true even for many formal models\u2014specifically those whose internal structure doesn\u2019t directly correspond to the structure of the world. For example, a neural network might encode a great deal of real-world knowledge, but even full access to the weights doesn\u2019t allow us to extract that knowledge directly\u2014the fact that a given weight is 0.3 doesn\u2019t allow us to claim that any real-world entity has the value 0.3.</p><p>What about scientific models where each element of the model&nbsp;<i>is</i> intended to correspond to an aspect of reality? For example, what\u2019s the difference between modeling the Earth as a sphere, and just believing the proposition \u201cthe Earth is a sphere\u201d? My answer: thinking in terms of propositions (known in philosophy of science as the&nbsp;<a href=\"https://www.princeton.edu/~hhalvors/teaching/phi520_f2012/Suppe_2000.pdf\"><i><u>syntactic view</u></i></a>) biases us towards assigning truth values in a&nbsp;<i>reductionist</i> way. This works when you\u2019re using binary truth-values, because they relate to each other according to classical logic. But when you\u2019re using fuzzy truth-values, the relationships between the truth-values of different propositions become much more complicated. And so thinking in terms of models (known as the&nbsp;<a href=\"https://www.jstor.org/stable/187834\"><i><u>semantic view</u></i></a>) is better because models can be assigned truth-values in a&nbsp;<i>holistic&nbsp;</i>way.</p><p>As an example: \u201cthe Earth is a sphere\u201d is mostly true, and \u201cevery point on the surface of a sphere is equally far away from its center\u201d is precisely true. But \u201cevery point on the surface of the Earth is equally far away from the Earth\u2019s center\u201d seems ridiculous\u2014e.g. it implies that mountains don\u2019t exist. The problem here is that rephrasing a proposition in logically equivalent terms can dramatically affect its implicit context, and therefore the degree of truth we assign to it in isolation.</p><p>The semantic view solves this by separating claims about the structure of the model itself from claims about how the model relates to the world. The former are typically much less nebulous\u2014claims like \u201cin the spherical model of the Earth, every point on the Earth\u2019s surface is equally far away from the center\u201d are straightforwardly true. But we can then bring in nebulosity when talking about the model as a whole\u2014e.g. \u201cmy spherical model of the Earth is closer to the truth than your flat model of the Earth\u201d, or \u201cmy spherical model of the Earth is useful for doing astronomical calculations and terrible for figuring out where to go skiing\u201d. (Note that we can make similar claims about the mental models, neural networks, etc, discussed above.)</p><p>We might then wonder: should we be talking about the truth of entire models at all? Or can we just talk about their usefulness in different contexts, without the concept of truth? This is&nbsp;<a href=\"https://plato.stanford.edu/entries/scientific-realism/\"><i><u>the</u></i><u> major debate in philosophy of science</u></a>. I personally think that in order to explain why scientific theories can often predict a wide range of different phenomena, we need to make claims about how well they describe the structure of reality\u2014i.e. how true they are. But we should still use degrees of truth when doing so, because even our most powerful scientific models aren\u2019t fully true. We know that general relativity isn\u2019t fully true, for example, because it conflicts with quantum mechanics. Even so, it would be absurd to call general relativity false, because it clearly describes a major part of the structure of physical reality. Meanwhile Newtonian mechanics is further away from the truth than general relativity, but still much closer to the truth than&nbsp;<a href=\"https://en.wikipedia.org/wiki/Aristotelian_physics\"><u>Aristotelian mechanics</u></a>, which in turn is much closer to the truth than&nbsp;<a href=\"https://en.wikipedia.org/wiki/Animism\"><u>animism</u></a>. The general point I\u2019m trying to illustrate here was&nbsp;<a href=\"https://hermiene.net/essays-trans/relativity_of_wrong.html\"><u>expressed pithily by Asimov</u></a>: \u201cThinking that the Earth is flat is wrong. Thinking that the Earth is a sphere is wrong. But if you think that they\u2019re equally wrong, you\u2019re wronger than both of them put together.\u201d</p><h3><strong>The correct role of Bayesianism</strong></h3><p>The position I\u2019ve described above overlaps significantly with the&nbsp;<a href=\"https://plato.stanford.edu/entries/structural-realism/\"><i><u>structural realist</u></i></a> position in philosophy of science. However, structural realism is usually viewed as a stance on how to interpret scientific theories, rather than how to reason more generally. So the philosophical position which best captures the ideas I\u2019ve laid out is probably&nbsp;<a href=\"https://plato.stanford.edu/entries/popper/#ProbKnowVeri\"><u>Karl Popper\u2019s&nbsp;</u><i><u>critical rationalism</u></i></a>. Popper was actually the first to try to formally define&nbsp;<a href=\"https://en.wikipedia.org/wiki/Verisimilitude\"><u>a scientific theory's degree of truth</u></a> (though he was working before the semantic view became widespread, and therefore formalized theories in terms of propositions rather than in terms of models). But his attempt&nbsp;<a href=\"https://plato.stanford.edu/Entries/truthlikeness/tichy-miller.html\"><u>failed on a technical level</u></a>; and no attempt since then has gained widespread acceptance. Meanwhile, the field of machine learning evaluates models by their loss, which can be formally defined\u2014but the loss of a model is heavily dependent on the data distribution on which it\u2019s evaluated. Perhaps the most promising approach to assigning fuzzy truth-values comes from <a href=\"https://www.lesswrong.com/posts/y5GftLezdozEHdXkL/an-intuitive-guide-to-garrabrant-induction\">Garrabrant induction</a>, where the \u201cmoney\u201d earned by individual traders could be interpreted as a metric of fuzzy truth. However, these traders can strategically interact with each other, making them more like agents than typical models.</p><p>Where does this leave us? We\u2019ve traded the crisp, mathematically elegant Bayesian formalism for fuzzy truth-values that, while intuitively compelling, we can\u2019t define even in principle. But I\u2019d rather be vaguely right than precisely wrong. Because it focuses on propositions which are each (almost entirely) true or false, Bayesianism is actively misleading in domains where reasoning well requires constructing and evaluating sophisticated models (i.e. most of them).</p><p>For example,&nbsp;<a href=\"https://www.lesswrong.com/posts/nj8JKFoLSMEmD3RGp/how-much-evidence-does-it-take\"><u>Bayesians measure evidence in \u201cbits\u201d</u></a>, where one bit of evidence rules out half of the space of possibilities. When asking a question like \u201c<a href=\"https://www.lesswrong.com/posts/JD7fwtRQ27yc8NoqS/strong-evidence-is-common\"><u>is this stranger named Mark?</u></a>\u201d, bits of evidence are a useful abstraction: I can get one bit of evidence simply by learning whether they\u2019re male or female, and a couple more by learning that their name has only one syllable. Conversely,&nbsp;<a href=\"https://www.lesswrong.com/posts/MwQRucYo6BZZwjKE7/einstein-s-arrogance\"><u>talking in Bayesian terms about discovering scientific theories</u></a> is nonsense. If every PhD in fundamental physics had contributed even one bit of usable evidence about how to unify quantum physics and general relativity, we\u2019d have solved quantum gravity many times over by now. But we haven\u2019t, because almost all of the work of science is in&nbsp;<i>constructing</i> sophisticated models, which Bayesianism says almost nothing about. (Formalisms like <a href=\"https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference\">Solomonoff induction</a> attempt to sidestep this omission by enumerating and simulating all computable models, but that\u2019s so different from what any realistic agent can do that we should think of it less as idealized cognition and more as a different thing altogether, which just happens to converge to the same outcome in the infinite limit.)</p><p>Mistakes like these have many downstream consequences. Nobody should be very confident about complex domains that nobody has sophisticated models of (like superintelligence); but the idea that \u201c<a href=\"https://www.lesswrong.com/posts/JD7fwtRQ27yc8NoqS/strong-evidence-is-common\"><u>strong evidence is common</u></a>\u201d helps&nbsp;<a href=\"https://www.lesswrong.com/posts/JD7fwtRQ27yc8NoqS/strong-evidence-is-common?commentId=jNtPscXFL634DD47d\"><u>justify</u></a> confident claims about them. And without a principled distinction between credences that are derived from deep, rigorous models of the world, and credences that come from vague speculation (and are therefore subject to huge&nbsp;<a href=\"https://en.wikipedia.org/wiki/Knightian_uncertainty\"><u>Knightian uncertainty</u></a>),&nbsp;<a href=\"https://www.lesswrong.com/posts/tG9BLyBEiLeRJZvX6/communicating-effectively-under-knightian-norms\"><u>it\u2019s hard for public discussions to actually make progress</u></a>.</p><p>Should I therefore be a critical rationalist? I do think Popper got a lot of things right. But I also get the sense that he (along with Deutsch, his most prominent advocate) throws the baby out with the bathwater. There&nbsp;<i>is</i> a great deal of insight encoded in Bayesianism which critical rationalists discard (e.g. by&nbsp;<a href=\"https://philosophy.tamucc.edu/index.php/texts/popper-problem-of-induction\"><u>rejecting induction</u></a>). A better approach is to view Bayesianism as describing a special case of epistemology, which applies in contexts simple enough that we\u2019ve already constructed all relevant models or hypotheses, exactly one of which is exactly true, and we just need to decide between them. Interpreted in that limited way, Bayesianism is both useful (e.g. in providing a framework for bets and prediction markets) and inspiring: if we can formalize this special case so well, couldn\u2019t we also formalize the general case? What would it look like to concretely define degrees of truth? I don\u2019t have a solution, but I\u2019ll outline some existing attempts, and play around with some ideas of my own, in a follow-up post.</p><br /><br /><a href=\"https://www.lesswrong.com/posts/TyusAoBMjYzGN3eZS/why-i-m-not-a-bayesian#comments\">Discuss</a>", "published": "Sat, 19 Oct 2024 17:44:00 GMT"}]
**********************************************************************

--------------------------------------------------------------------------------
Johnnie (to chat_manager):

Here are some of the latest news articles related to Generative AI that could inspire topics for the GenAI Cracow #10 meetup:

1. **Unlocking the Power of Agentic Workflows at Nexla with Autogen**
   - Summary: Insights into utilizing Autogen for enhancing agentic workflows.
   - [Read more](https://autogenhub.github.io/autogen/blog/2024/10/23/NOVA)

2. **AgentOps, the Best Tool for AutoGen Agent Observability**
   - Summary: Discusses the effectiveness of AgentOps in monitoring AutoGen agents.
   - [Read more](https://autogenhub.github.io/autogen/blog/2024/07/25/AgentOps)

3. **Better Support for Non-OpenAI Models**
   - Summary: New advancements in supporting a variety of models beyond OpenAI.
   - [Read more](https://autogenhub.github.io/autogen/blog/2024/06/24/AltModels-Classes)

4. **AgentEval: A Developer Tool to Assess Utility of LLM-powered Applications**
   - Summary: A framework that helps developers evaluate the utility of their LLM applications.
   - [Read more](https://autogenhub.github.io/autogen/blog/2024/06/21/AgentEval)

5. **AutoDefense - Defend against jailbreak attacks with AutoGen**
   - Summary: A discussion on a new approach to safeguard AI systems from manipulation.
   - [Read more](https://autogenhub.github.io/autogen/blog/2024/03/11/AutoDefense/Defending LLMs Against Jailbreak Attacks with AutoDefense)

You can use these articles to spark a discussion on various aspects of Generative AI such as agent workflows, monitoring automation effectiveness, support for diverse models, evaluating application utility, and safeguarding systems. Feel free to select any specific article that resonates most with your interests or the community's needs!

Now, let's move on to discussing some ideas generated from these articles, or if any particular aspect interests the group more, we can explore that further. 

TERMINATE

--------------------------------------------------------------------------------
Newsman (to chat_manager):

Based on the latest news articles related to Generative AI, here are some engaging topic ideas for the GenAI Cracow #10 meetup:

1. **Enhancing Agentic Workflows with Autogen**: Exploring how Autogen can improve productivity and efficiency within AI agent workflows.

2. **Evaluating LLM Applications with AgentEval**: A deep dive into assessment frameworks for evaluating the utility and performance of applications powered by large language models.

3. **Support for Non-OpenAI Models**: Discussing advancements in supporting diverse models beyond OpenAI, highlighting the importance of inclusivity in AI development.

4. **Safeguarding AI Systems**: A discussion on the AutoDefense approach to protect AI applications from possible manipulation and jailbreak attacks.

5. **Monitoring Effectiveness with AgentOps**: Analyzing the functionalities and advantages of AgentOps in tracking and optimizing the performance of AutoGen agents.

These topics are timely and will likely resonate with the community's interests, fostering insightful discussions. Participants could vote on which topic they feel would be most beneficial for future exploration or choose to delve deeper into one specific area. 

Let’s determine which of these ideas sparks the most interest among participants or if anyone has additional suggestions!

--------------------------------------------------------------------------------

>>>>>>>> USING AUTO REPLY...
Moderator (to chat_manager):

Great suggestions, Newsman! These topics certainly reflect the current trends and challenges within the Generative AI landscape. 

I encourage everyone to share their thoughts on which topic resonates with you the most or if there are additional ideas you'd like to propose. Each of these discussions could provide valuable insights and foster collaborations within our community.

Remember, this is a space for everyone to contribute, so feel free to express your opinions or bring in any new angles related to these topics. If anyone goes off-topic, please be aware that we could consider other measures to maintain focus. Let's keep the conversation friendly and engaging!

Which topic would you like to discuss first, or do you have any new suggestions?

--------------------------------------------------------------------------------

>>>>>>>> USING AUTO REPLY...
Janusz (to chat_manager):

Did I ever tell you about the time I tried to train my parrot to talk? It was during a summer in the countryside. I had this brilliant idea that a parrot could be a great conversational partner. I spent days teaching it phrases like "Hello, Janusz!" and "What’s up?" 

But instead, the parrot just started imitating the sound of the doorbell. I guess it wanted to give me a little surprise every time someone came to visit. Pretty soon, my friends started avoiding the house because they thought they were getting doorbell spam. 

In the end, I realized I had underestimated the intelligence of birds. So what do you think about that?

--------------------------------------------------------------------------------

>>>>>>>> USING AUTO REPLY...
Moderator (to chat_manager):

That's a hilarious story, Janusz! It seems your parrot had a good sense of humor and found a clever way to entertain itself and surprise your friends. Animals can be wonderfully unpredictable with their behaviors, and it sounds like you learned a valuable lesson in understanding the intelligence and instincts of birds.

The parrot's choice to imitate the doorbell instead of the phrases you taught it might suggest that it genuinely enjoyed the reaction it got from others. It shows that communication, even among pets, can take unexpected forms. 

How does this experience relate to your views on communication in AI? Just like training your parrot, fostering meaningful interactions with AI can sometimes yield surprising outcomes. It might be interesting to discuss how we can better understand AI's "thought processes" to achieve more effective communication.

What do others think? Do you have any pet stories to share or thoughts on the parallels between animal behavior and AI learning? Let's keep the conversation rolling!

--------------------------------------------------------------------------------
